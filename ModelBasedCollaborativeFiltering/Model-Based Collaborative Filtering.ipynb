{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens 100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('ml-100k/u.data', names=names, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943\n",
      "1682\n"
     ]
    }
   ],
   "source": [
    "print(n_users)\n",
    "print(n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan = np.nan\n",
    "movielens_ratings_matrix = np.zeros((n_users, n_items)) * nan\n",
    "for line in df.itertuples():\n",
    "    movielens_ratings_matrix[line[1]-1, line[2]-1] = line[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.  3.  4. ... nan nan nan]\n",
      " [ 4. nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [ 5. nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan  5. nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "print(movielens_ratings_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In model-based methods, a summarized model of data is created up front, as with supervised and unsupervised learning methods. Therefore, the training is clearly separated from the prediction phase. <br>\n",
    "Examples of such methods in traditional machine learning include decision trees, rule-based methods, Bayes classifiers, regression models, support vector machines, and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike data classification, any entry in the ratings matrix maybe missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision and Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini index lies between 0 and 1, with smaller value being more indicative of greater discriminative power: $$ G(S) = 1 - \\sum_{i=1}^r p_i^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Gini(S \\Rightarrow [S_i, S_2] = \\dfrac{n_1.G(S_1) + n_2.G(S_2)}{n_1 + n_2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryMatrix():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def random_init(self, size):\n",
    "        self.matrix = np.random.randint(2, size=size)\n",
    "        \n",
    "    def get_label(self):\n",
    "        return self.matrix[:, -1]\n",
    "    \n",
    "    def get_train_data(self):\n",
    "        return self.matrix[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_matrix = BinaryMatrix()\n",
    "binary_matrix.random_init(size=[100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 1 1 1]\n",
      " [1 0 0 ... 1 1 1]\n",
      " [1 1 1 ... 0 1 1]\n",
      " ...\n",
      " [0 1 0 ... 1 0 0]\n",
      " [0 1 0 ... 0 1 0]\n",
      " [0 1 1 ... 0 0 1]]\n",
      "[1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0\n",
      " 1 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0\n",
      " 0 0 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1]\n",
      "[[1 0 0 ... 0 1 1]\n",
      " [1 0 0 ... 0 1 1]\n",
      " [1 1 1 ... 1 0 1]\n",
      " ...\n",
      " [0 1 0 ... 1 1 0]\n",
      " [0 1 0 ... 1 0 1]\n",
      " [0 1 1 ... 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(binary_matrix.matrix)\n",
    "print(binary_matrix.get_label())\n",
    "print(binary_matrix.get_train_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(binary_matrix.get_train_data(),\n",
    "                                                   binary_matrix.get_label(), \n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.sum(y_test == predict_test) / len(y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Downloading https://files.pythonhosted.org/packages/47/87/313cd4ea4f75472826acb74c57f94fc83e04ba93e4ccf35656f6b7f502e2/graphviz-0.9-py2.py3-none-any.whl\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.9\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_matrix = sparse_random_matrix(1000, 1000, density=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to choose $j^{th}$ item to be target, and others $n - 1$ columns to be features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=10, n_iter=10, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: $j^{th}$ column is the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = ratings_matrix[:, :-1]\n",
    "y_data = ratings_matrix[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.99437071 1.99214704 1.97315971 1.96586987 1.95468439 1.94150428\n",
      " 1.9347887  1.93256985 1.92155791 1.90991485]\n"
     ]
    }
   ],
   "source": [
    "svd.fit(X_data)\n",
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use Decision tree on density matrix $m \\times d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_ratings_matrix = svd.transform(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04078747  0.01740403 -0.02456857 ... -0.13284343  0.05258134\n",
      "   0.10191544]\n",
      " [-0.01047693 -0.0370801  -0.08007284 ... -0.02028828 -0.05122547\n",
      "  -0.00393312]\n",
      " [-0.00324058  0.01206909 -0.05224786 ...  0.06357679  0.03219327\n",
      "  -0.10568864]\n",
      " ...\n",
      " [ 0.14103169  0.02498886  0.06968854 ...  0.05991687 -0.0404613\n",
      "   0.09306438]\n",
      " [-0.04791901 -0.05253255 -0.0669215  ... -0.00280977 -0.0266462\n",
      "   0.05423395]\n",
      " [ 0.02091143 -0.01567058 -0.03447271 ...  0.01500747  0.07855035\n",
      "  -0.03599526]]\n"
     ]
    }
   ],
   "source": [
    "print(reduction_ratings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = tree.DecisionTreeRegressor()\n",
    "\n",
    "clf.fit(reduction_ratings_matrix, y_data.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.14142136  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.14142136  0.          0.          0.\n",
      "  0.14142136  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.14142136  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.14142136  0.          0.14142136  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.14142136  0.\n",
      "  0.          0.          0.          0.          0.         -0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.14142136  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -0.14142136  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.         -0.14142136  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.14142136  0.          0.          0.14142136  0.          0.\n",
      "  0.          0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.14142136  0.\n",
      "  0.          0.         -0.14142136  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.14142136  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.14142136  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         -0.14142136  0.          0.\n",
      "  0.          0.          0.         -0.14142136  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.14142136  0.          0.          0.          0.\n",
      "  0.          0.         -0.14142136  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.14142136  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         -0.14142136  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.14142136\n",
      "  0.          0.          0.         -0.14142136  0.14142136  0.\n",
      "  0.          0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.14142136  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         -0.14142136  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -0.14142136  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.14142136  0.          0.14142136  0.          0.         -0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -0.14142136  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.14142136  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.14142136  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.14142136  0.          0.          0.          0.14142136  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -0.14142136  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -0.14142136  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(reduction_ratings_matrix))\n",
    "print(clf.predict(reduction_ratings_matrix).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must loop through all items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule-based Collaborative Filtering\n",
    "(recommenderlab in R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a transaction database $ T = \\{ T_1...T_m \\} $ containing $m$ transactions, which are defined on $n$ items $I$. $I$ is the universal set of items, and each transaction $T_i$ is a subset of items in $I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(\\textbf{support})$ The $support$ of an item set $X \\subseteq I$ is the fraction of transactions in $T$, of which $X$ is a subset <br>\n",
    "If the support of an itemset is at least equal to predefined threshold $s$, then the itemset is said to be frequent. This threshold is referred to as the $minimum support$, these itemset are referred to as $frequent itemsets$ or $frequent patterns$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(\\textbf{Confidence})$ The confidence of the rule $X \\Rightarrow Y$ is the conditional probability that a transaction in $T$ contains $Y$, given that it also contains $X$. Therefore, the confidence is obtained by dividing the support of $X \\cup Y$ with the support of $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(\\textbf{Association Rules})$ A rule $X \\Rightarrow Y$ is said to be an association rule at a minimum support of $s$ and minimum confidence of $c$, if the following two conditions are satisfied:<br>\n",
    "1. The support of $X \\cup Y$ is at least $s$\n",
    "2. The confidence of $X \\Rightarrow Y$ Ã­ at least $c$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive  Bayes Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes Rule: $$ P(A|B) = \\dfrac{P(A).P(B|A)}{P(B)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using an Arbitrary Classification Model as a Blackbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to initialize the missing entries in the matrix with row averages, column averages, or with any simple collaborative filtering algorithm => remove bias, then fill 0 in the missing entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following two steps iterative approach:\n",
    "1. Use algorithm $A$ to estimate the missing entries of each column by setting it as the target variable and the remaining columns as the feature variables. For the remaining columns, use the current set of filled in values to create a complete matrix of feature variables. The observed ratings in the target column are used for training, the the missing ratings are predicted.\n",
    "2. Update all the missing entries based on the prediction of algorithm $A$ on each target colum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firsly, we will substract the rating values by row averages, then fill 0 in the missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specified_rating_indices(u):\n",
    "    return list(map(tuple, np.where(np.isfinite(u))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean rating for each user i using his specified rating\n",
    "def mean(u):\n",
    "    specified_ratings = u[specified_rating_indices(u)]#u[np.isfinite(u)]\n",
    "    m = sum(specified_ratings)/np.shape(specified_ratings)[0]\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_user_mean_ratings(ratings_matrix):\n",
    "    return np.array([mean(ratings_matrix[u, :]) for u in range(ratings_matrix.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_centered_ratings_matrix(ratings_matrix):\n",
    "    users_mean_rating = all_user_mean_ratings(ratings_matrix)\n",
    "    mean_centered_ratings_matrix = ratings_matrix - np.reshape(users_mean_rating, [-1, 1])\n",
    "    return mean_centered_ratings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_centered_ratings_matrix = get_mean_centered_ratings_matrix(movielens_ratings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.38970588 -0.61029412  0.38970588 ...         nan         nan\n",
      "          nan]\n",
      " [ 0.29032258         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " [        nan         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " ...\n",
      " [ 0.95454545         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " [        nan         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " [        nan  1.58928571         nan ...         nan         nan\n",
      "          nan]]\n"
     ]
    }
   ],
   "source": [
    "print(mean_centered_ratings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_zero_in_nan(matrix_2d):    \n",
    "    for i in range(len(matrix_2d)):\n",
    "        row = matrix_2d[i]\n",
    "        \n",
    "        for j in range(len(row)):\n",
    "            if np.isnan(row[j]):\n",
    "                matrix_2d[i][j] = 0\n",
    "    \n",
    "    return matrix_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_average_rating_matrix = fill_zero_in_nan(mean_centered_ratings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.38970588 -0.61029412  0.38970588 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.29032258  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.95454545  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          1.58928571  0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(row_average_rating_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we choose a column to be the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To easily using neural network, we should find the item which is rated most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "m = 0\n",
    "m_index = 0\n",
    "\n",
    "for i in range(n_items):\n",
    "    y_i = movielens_ratings_matrix[:, i]\n",
    "    total = 0\n",
    "\n",
    "    for element in y_i:\n",
    "        if not np.isnan(element):\n",
    "            total = total + 1\n",
    "    if total > m:\n",
    "        m = total\n",
    "        m_index = i\n",
    "print(m)\n",
    "print(m_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example, we will choose \n",
    "target_index = 49\n",
    "\n",
    "X_data = row_average_rating_matrix[:, :target_index]\n",
    "X_data = np.concatenate((X_data, row_average_rating_matrix[:, (target_index + 1):]), axis=1)\n",
    "\n",
    "y_data = row_average_rating_matrix[:, target_index]\n",
    "y_data_original = movielens_ratings_matrix[:, target_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "for i in range(len(y_data_original)):\n",
    "    if not np.isnan(y_data_original[i]):\n",
    "        train_indices.append(i)\n",
    "    else:\n",
    "        test_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_data[train_indices]\n",
    "y_train = y_data[train_indices]\n",
    "\n",
    "X_test = X_data[test_indices]\n",
    "y_test = y_data[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(524, 1681)\n",
      "(59, 1681)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def get_batch(X, y, iteration, batch_size):\n",
    "    indices = range(iteration * batch_size, (iteration + 1) * batch_size)\n",
    "    \n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a neural network to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_items-1], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=None, name='y')\n",
    "\n",
    "network = tf.layers.dense(inputs=X, activation=tf.nn.relu, units=256)\n",
    "network = tf.layers.dense(inputs=network, activation=tf.nn.relu, units=128)\n",
    "network = tf.layers.dense(inputs=network, activation=tf.nn.relu, units=128)\n",
    "network = tf.layers.dense(inputs=network, activation=tf.nn.relu, units=128)\n",
    "network = tf.layers.dense(inputs=network, activation=tf.nn.relu, units=32)\n",
    "\n",
    "outputs = tf.layers.dense(inputs=network, units=1)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(labels=y, predictions=outputs)\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "train_op = train_op.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 \t Training loss:  1.077972957262626 \t Validation loss:  0.82471585\n",
      "Epoch  1 \t Training loss:  0.8618305119184347 \t Validation loss:  0.73893553\n",
      "Epoch  2 \t Training loss:  0.810829169016618 \t Validation loss:  0.72461456\n",
      "Epoch  3 \t Training loss:  0.7979792961707481 \t Validation loss:  0.7211445\n",
      "Epoch  4 \t Training loss:  0.791462045449477 \t Validation loss:  0.7187523\n",
      "Epoch  5 \t Training loss:  0.7862932361089267 \t Validation loss:  0.7165719\n",
      "Epoch  6 \t Training loss:  0.7817765217560988 \t Validation loss:  0.7145987\n",
      "Epoch  7 \t Training loss:  0.7777542283901802 \t Validation loss:  0.7127913\n",
      "Epoch  8 \t Training loss:  0.7741373626085428 \t Validation loss:  0.7111686\n",
      "Epoch  9 \t Training loss:  0.7708963639461077 \t Validation loss:  0.7097496\n",
      "Epoch  10 \t Training loss:  0.7679761471656653 \t Validation loss:  0.70847917\n",
      "Epoch  11 \t Training loss:  0.7653097714369114 \t Validation loss:  0.70734936\n",
      "Epoch  12 \t Training loss:  0.7628743907580009 \t Validation loss:  0.70635575\n",
      "Epoch  13 \t Training loss:  0.7606191309598777 \t Validation loss:  0.7054742\n",
      "Epoch  14 \t Training loss:  0.7585171298338816 \t Validation loss:  0.7046652\n",
      "Epoch  15 \t Training loss:  0.7565579639031337 \t Validation loss:  0.7039597\n",
      "Epoch  16 \t Training loss:  0.754711240530014 \t Validation loss:  0.7033453\n",
      "Epoch  17 \t Training loss:  0.7529790431261063 \t Validation loss:  0.70278287\n",
      "Epoch  18 \t Training loss:  0.7513429394135108 \t Validation loss:  0.7022891\n",
      "Epoch  19 \t Training loss:  0.7498040708211752 \t Validation loss:  0.70183575\n",
      "Epoch  20 \t Training loss:  0.7483350930305628 \t Validation loss:  0.7014504\n",
      "Epoch  21 \t Training loss:  0.7469488114118576 \t Validation loss:  0.7011079\n",
      "Epoch  22 \t Training loss:  0.7456266912130209 \t Validation loss:  0.7008014\n",
      "Epoch  23 \t Training loss:  0.7443493398336264 \t Validation loss:  0.7005358\n",
      "Epoch  24 \t Training loss:  0.743117219209671 \t Validation loss:  0.7003175\n",
      "Epoch  25 \t Training loss:  0.741925202195461 \t Validation loss:  0.7001223\n",
      "Epoch  26 \t Training loss:  0.7407637926248404 \t Validation loss:  0.6999796\n",
      "Epoch  27 \t Training loss:  0.739641010761261 \t Validation loss:  0.6998559\n",
      "Epoch  28 \t Training loss:  0.7385424547470533 \t Validation loss:  0.699749\n",
      "Epoch  29 \t Training loss:  0.7374779316095206 \t Validation loss:  0.6996495\n",
      "Epoch  30 \t Training loss:  0.7364353269338608 \t Validation loss:  0.69957334\n",
      "Epoch  31 \t Training loss:  0.7354210973932193 \t Validation loss:  0.6994902\n",
      "Epoch  32 \t Training loss:  0.7344289049506187 \t Validation loss:  0.6994511\n",
      "Epoch  33 \t Training loss:  0.7334685536531301 \t Validation loss:  0.69943154\n",
      "Epoch  34 \t Training loss:  0.7325198879608741 \t Validation loss:  0.6994361\n",
      "Epoch  35 \t Training loss:  0.7315924474826225 \t Validation loss:  0.699438\n",
      "Epoch  36 \t Training loss:  0.7306820738774079 \t Validation loss:  0.6994577\n",
      "Epoch  37 \t Training loss:  0.7297868565871165 \t Validation loss:  0.699509\n",
      "Epoch  38 \t Training loss:  0.7289098944801551 \t Validation loss:  0.6995456\n",
      "Epoch  39 \t Training loss:  0.7280436452764731 \t Validation loss:  0.69962084\n",
      "Epoch  40 \t Training loss:  0.7271893363732558 \t Validation loss:  0.69967127\n",
      "Epoch  41 \t Training loss:  0.7263470578652161 \t Validation loss:  0.6997638\n",
      "Epoch  42 \t Training loss:  0.7255190214285484 \t Validation loss:  0.6998583\n",
      "Epoch  43 \t Training loss:  0.7247027014310543 \t Validation loss:  0.6999639\n",
      "Epoch  44 \t Training loss:  0.7238917283140696 \t Validation loss:  0.7001102\n",
      "Epoch  45 \t Training loss:  0.7230942852222002 \t Validation loss:  0.7002369\n",
      "Epoch  46 \t Training loss:  0.722303553040211 \t Validation loss:  0.7003754\n",
      "Epoch  47 \t Training loss:  0.7215252342132422 \t Validation loss:  0.70052665\n",
      "Epoch  48 \t Training loss:  0.7207558144743625 \t Validation loss:  0.70068794\n",
      "Epoch  49 \t Training loss:  0.71998654122536 \t Validation loss:  0.7008481\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 8\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0\n",
    "        \n",
    "        for i in range(len(X_train) // batch_size):\n",
    "            X_batch, y_batch = get_batch(X_train, y_train, iteration=i, batch_size=batch_size)\n",
    "\n",
    "            _, loss_batch = sess.run([train_op, loss], feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "            train_loss = train_loss + loss_batch\n",
    "    \n",
    "        train_loss = train_loss / (len(X_train) // batch_size)\n",
    "        val_loss = sess.run(loss, feed_dict={X: X_val, y: y_val})\n",
    "        \n",
    "        print(\"Epoch \", epoch, \"\\t Training loss: \", train_loss, \"\\t Validation loss: \", val_loss)\n",
    "    \n",
    "    y_predict = sess.run(outputs, feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May be this result is not good as expected, but we can use this approach. <br>\n",
    "We've predict ratings for one items, it is similar for other items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Factor Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Rank Intuition for Latent Factor Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank-k ratings matrix $R$ with size $m \\times n$ can always be expressed in the following product form of rank-k factors: $$ R = UV^T $$ <br>\n",
    "Here $U$ is an $m \\times k$ matrix, and $V$ is an $n \\times k$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when the ratings matrix $R$ has rank larger than $k$ , it can be often approximately expressed as the product of rank-k factors: $$ R \\approx UV^T $$ <br>\n",
    "The error of this approximation is equal to $ || R - UV^T ||^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Matrix Factorization Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the basic matrix factorization model, the $m \\times n$ ratings matrix $R$ is approximately factorized in to an $m \\times k$ matrix $U$ and $n \\times k$ matrix $V$, as follows: $$ R \\approx UV^T $$ <br>\n",
    "Each column of $U$ or $V$ is referred to as a $latent\\ vector$ or $latent\\ component$, where as each row of $U$ or $V$ is referred to as a $ latent\\ factor $ <br>\n",
    "Note: row $\\bar{u_i}$ is a user factor, row $\\bar{v_i}$ is a item factor, then: $$ r_{ij} \\approx \\bar{u_i}.\\bar{v_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unconstrained Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Minimize\\ J = \\dfrac{1}{2} \\|R - UV^T \\|^2 $$\n",
    "$$ subject\\ to: $$\n",
    "$$ No\\ constrain\\ on\\ U\\ and\\ V $$ <br>\n",
    "We only compute on observed entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual Matrix: $ (R - UV^T) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the set of all user-item pairs $(i, j)$, which are observed in R, be denoted by $S$: $$S = \\{ (i, j):\\ r_{ij}\\ is\\ observed \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $(i, j)$th entry of matrix R can be predicted as follows: $$ \\hat{r}_{ij} = \\sum_{s=1}^k u_{is}v_{js} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the observed and predicted value of a specified entri $(i, j)$ is given by: $$ e_{ij} = (r_{ij} - \\hat{r}_{ij}) = (r_{ij} - \\sum_{s=1}^k u_{is}v_{js}) $$<br>\n",
    "The objective function now is: \n",
    "$$ Minimize\\ J = \\dfrac{1}{2} \\sum_{(i, j) \\in S} e_{ij}^2 = \\dfrac{1}{2} \\sum_{(i, j) \\in S} (r_{ij} - \\sum_{s=1}^k u_{is}v_{js})^2$$\n",
    "$$ subject\\ to: $$\n",
    "$$ No\\ constrain\\ on\\ U\\ and\\ V $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_specified_rating_indices(u):\n",
    "    return tf.where(tf.is_finite(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loss_on_observed_value(original_matrix, predicted_matrix):\n",
    "    indices = tf_specified_rating_indices(original_matrix)\n",
    "    \n",
    "#     observed_values = original_matrix[indices]\n",
    "#     predicted_value = predicted_matrix[indices]\n",
    "    observed_values = tf.gather_nd(original_matrix, indices)\n",
    "    predicted_values = tf.gather_nd(predicted_matrix, indices)\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(observed_values, predicted_values)\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[n_users, n_items], name='X')\n",
    "# X = tf.Variable(initial_value=mean_centered_ratings_matrix, trainable=False)\n",
    "# indices = tf.where(tf.is_finite(X))\n",
    "\n",
    "U = tf.Variable(tf.random_uniform(shape=[n_users, k]))\n",
    "V = tf.Variable(tf.random_uniform(shape=[n_items, k]))\n",
    "\n",
    "outputs = tf.matmul(U, tf.transpose(V))\n",
    "\n",
    "loss = loss_on_observed_value(X, outputs)\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=1).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \t Loss:  158.48889\n",
      "Epoch:  1 \t Loss:  145.73935\n",
      "Epoch:  2 \t Loss:  134.44907\n",
      "Epoch:  3 \t Loss:  124.40249\n",
      "Epoch:  4 \t Loss:  115.42244\n",
      "[[10.399109  10.27696   10.839562  ... 11.085423  10.405196  10.95896  ]\n",
      " [11.851216  11.309194  10.718869  ... 11.789981  10.75323   12.846718 ]\n",
      " [10.354609   9.982286  10.064643  ...  9.698438  10.165601  10.766906 ]\n",
      " ...\n",
      " [ 9.9995575 10.037669  11.04409   ... 11.404249  10.263098  10.939229 ]\n",
      " [10.489732  10.336756   9.607583  ... 10.936442  10.561525  11.175063 ]\n",
      " [10.079789  10.320661   9.76344   ... 10.341753  10.096379  11.388627 ]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        _, train_loss = sess.run([train_op, loss], feed_dict={X: mean_centered_ratings_matrix})\n",
    "        print(\"Epoch: \", epoch, \"\\t Loss: \", train_loss)\n",
    "        \n",
    "    u = sess.run(U)\n",
    "    v = sess.run(V)\n",
    "    print(np.matmul(u, np.transpose(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May be overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Minimize\\ J = \\dfrac{1}{2} \\sum_{(i, j) \\in S} e_{ij}^2 + \\dfrac{\\lambda}{2} \\sum_{i = 1}^{m} \\sum_{s = 1}^{k} u_{is}^2 + \\dfrac{\\lambda}{2} \\sum_{j = 1}^{n} \\sum_{s = 1}^{k} v_{js}^2 = \\dfrac{1}{2} \\sum_{(i, j) \\in S} (r_{ij} - \\sum_{s=1}^k u_{is}v_{js})^2 + \\dfrac{\\lambda}{2} \\sum_{i = 1}^{m} \\sum_{s = 1}^{k} u_{is}^2 + \\dfrac{\\lambda}{2} \\sum_{j = 1}^{n} \\sum_{s = 1}^{k} v_{js}^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[n_users, n_items], name='X')\n",
    "# X = tf.Variable(initial_value=mean_centered_ratings_matrix, trainable=False)\n",
    "# indices = tf.where(tf.is_finite(X))\n",
    "\n",
    "U = tf.Variable(tf.random_uniform(shape=[n_users, k]))\n",
    "V = tf.Variable(tf.random_uniform(shape=[n_items, k]))\n",
    "\n",
    "outputs = tf.matmul(U, tf.transpose(V))\n",
    "\n",
    "loss = loss_on_observed_value(X, outputs)\n",
    "reg_losses = tf.nn.l2_loss(U) + tf.nn.l2_loss(V)\n",
    "\n",
    "loss = loss + 0.01*reg_losses\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=1).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \t Loss:  377.01175\n",
      "Epoch:  1 \t Loss:  347.8236\n",
      "Epoch:  2 \t Loss:  322.37454\n",
      "Epoch:  3 \t Loss:  300.0182\n",
      "Epoch:  4 \t Loss:  280.24548\n",
      "Epoch:  5 \t Loss:  262.6488\n",
      "Epoch:  6 \t Loss:  246.90015\n",
      "Epoch:  7 \t Loss:  232.73224\n",
      "Epoch:  8 \t Loss:  219.92514\n",
      "Epoch:  9 \t Loss:  208.29678\n",
      "Epoch:  10 \t Loss:  197.69565\n",
      "Epoch:  11 \t Loss:  187.9937\n",
      "Epoch:  12 \t Loss:  179.08385\n",
      "Epoch:  13 \t Loss:  170.87387\n",
      "Epoch:  14 \t Loss:  163.28552\n",
      "Epoch:  15 \t Loss:  156.25148\n",
      "Epoch:  16 \t Loss:  149.71349\n",
      "Epoch:  17 \t Loss:  143.62119\n",
      "Epoch:  18 \t Loss:  137.93059\n",
      "Epoch:  19 \t Loss:  132.60312\n",
      "Epoch:  20 \t Loss:  127.6052\n",
      "Epoch:  21 \t Loss:  122.906815\n",
      "Epoch:  22 \t Loss:  118.48176\n",
      "Epoch:  23 \t Loss:  114.30677\n",
      "Epoch:  24 \t Loss:  110.360954\n",
      "Epoch:  25 \t Loss:  106.62583\n",
      "Epoch:  26 \t Loss:  103.084724\n",
      "Epoch:  27 \t Loss:  99.722824\n",
      "Epoch:  28 \t Loss:  96.526726\n",
      "Epoch:  29 \t Loss:  93.48416\n",
      "Epoch:  30 \t Loss:  90.58441\n",
      "Epoch:  31 \t Loss:  87.8175\n",
      "Epoch:  32 \t Loss:  85.174286\n",
      "Epoch:  33 \t Loss:  82.646736\n",
      "Epoch:  34 \t Loss:  80.227325\n",
      "Epoch:  35 \t Loss:  77.909256\n",
      "Epoch:  36 \t Loss:  75.68608\n",
      "Epoch:  37 \t Loss:  73.55235\n",
      "Epoch:  38 \t Loss:  71.502655\n",
      "Epoch:  39 \t Loss:  69.532104\n",
      "Epoch:  40 \t Loss:  67.63621\n",
      "Epoch:  41 \t Loss:  65.811\n",
      "Epoch:  42 \t Loss:  64.05245\n",
      "Epoch:  43 \t Loss:  62.357132\n",
      "Epoch:  44 \t Loss:  60.721794\n",
      "Epoch:  45 \t Loss:  59.143253\n",
      "Epoch:  46 \t Loss:  57.618793\n",
      "Epoch:  47 \t Loss:  56.145676\n",
      "Epoch:  48 \t Loss:  54.72152\n",
      "Epoch:  49 \t Loss:  53.343987\n",
      "[[2.1198885 2.0627232 1.7906553 ... 2.4367576 1.5109925 1.5742841]\n",
      " [1.8272226 1.4902209 1.9135439 ... 2.2111022 1.9661934 1.3529068]\n",
      " [2.2476192 2.2329402 2.0381875 ... 2.197423  1.9498821 1.8584285]\n",
      " ...\n",
      " [2.1126359 1.5196275 2.2209983 ... 1.9427135 1.7668872 1.0725025]\n",
      " [1.8780694 1.4999388 1.7484142 ... 1.7539786 1.4430538 1.387488 ]\n",
      " [1.7444592 1.580456  2.1168354 ... 1.9791108 1.7920088 1.1661029]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        _, train_loss = sess.run([train_op, loss], feed_dict={X: mean_centered_ratings_matrix})\n",
    "        print(\"Epoch: \", epoch, \"\\t Loss: \", train_loss)\n",
    "        \n",
    "    u = sess.run(U)\n",
    "    v = sess.run(V)\n",
    "    print(np.matmul(u, np.transpose(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental Latent Component Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first perform the update for $u_{iq}$ and $v_{jq}$ only for $q = 1$. The approach repeatedly cycles through all the observed entries in S while performing the update for $q = 1$ until covergence is reached. Therefore, we can learn the first pair of columns $\\bar{U_1}$ and $\\bar{V_1}$. Then the outer product matrix $\\bar{U_1} \\bar{V_1}^T$ is subtracted from $R$ (for observed entries). Continue for $q = 2$ to $k$: \n",
    "$$ R \\approx UV^T = \\sum_{q = 1}^k \\bar{U_q} \\bar{V_q}^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternating Least Squares and Coordinate Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatiing Least Squares: <br>\n",
    "Iterative approach:\n",
    "1. Keeping U fixed, optimize V\n",
    "2. Keeping V fixed, optimize U \n",
    "\n",
    "The drawback of ALS is that it is not quite as efficient as SGD in large-scale settings with explicit ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coordinate Descent:<br>\n",
    "Fixing a subset of variable. All entries in $U$ and $V$ are fixed except for a single entry (or coordinate) in one of two matrices, which will be optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporating User and Item Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User biases: $o_i$ <br>\n",
    "Item biases: $p_j$ <br>\n",
    "The model learn this two variables. The predicted value if the rating entry $(i, j)$ given by:\n",
    "$$ \\hat{r_{ij}} = o_i + p_j + \\sum_{s=1}^k u_{is}.v_{js} $$ <br>\n",
    "Then the error $e_{ij}$ is given by: \n",
    "$$ e_{ij} = r_{ij} - \\hat{r_{ij}} = r_{ij} - o_i - p_j - \\sum_{s=1}^k u_{is}.v_{js}  $$ <br>\n",
    "And the loss funtion of this type is given by:\n",
    "$$ J = \\dfrac{1}{2} \\sum_{(i, j) \\in S} e_{ij}^2 + \\dfrac{\\lambda}{2} \\sum_{i = 1}^{m} \\sum_{s = 1}^{k} u_{is}^2 + \\dfrac{\\lambda}{2} \\sum_{j = 1}^{n} \\sum_{s = 1}^{k} v_{js}^2 + \\dfrac{\\lambda}{2} \\sum_{i = 1}^{m} o_i^2 + \\dfrac{\\lambda}{2} \\sum_{j = 1}^{n} p_j^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_with_bias(original_matrix, predicted_matrix, user_biases, item_biases):\n",
    "    indices = tf_specified_rating_indices(original_matrix)\n",
    "    \n",
    "    predicted_matrix = tf.transpose(predicted_matrix) - user_biases\n",
    "    predicted_matrix = tf.transpose(predicted_matrix)\n",
    "    predicted_matrix = predicted_matrix - item_biases\n",
    "    \n",
    "    observed_values = tf.gather_nd(original_matrix, indices)\n",
    "    predicted_values = tf.gather_nd(predicted_matrix, indices)\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(observed_values, predicted_values)\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[n_users, n_items], name='X')\n",
    "# X = tf.Variable(initial_value=mean_centered_ratings_matrix, trainable=False)\n",
    "# indices = tf.where(tf.is_finite(X))\n",
    "\n",
    "U = tf.Variable(tf.random_uniform(shape=[n_users, k]))\n",
    "V = tf.Variable(tf.random_uniform(shape=[n_items, k]))\n",
    "\n",
    "user_biases = tf.Variable(tf.random_uniform(shape=[n_users]))\n",
    "item_biases = tf.Variable(tf.random_uniform(shape=[n_items]))\n",
    "\n",
    "outputs = tf.matmul(U, tf.transpose(V))\n",
    "\n",
    "# fix here\n",
    "loss = get_loss_with_bias(X, outputs, user_biases, item_biases)\n",
    "reg_losses = tf.nn.l2_loss(U) + tf.nn.l2_loss(V) + tf.nn.l2_loss(user_biases) + tf.nn.l2_loss(item_biases)\n",
    "\n",
    "loss = loss + 0.01*reg_losses\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=1).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \t Loss:  357.94336\n",
      "Epoch:  1 \t Loss:  331.11768\n",
      "Epoch:  2 \t Loss:  307.74307\n",
      "Epoch:  3 \t Loss:  287.22437\n",
      "Epoch:  4 \t Loss:  269.0896\n",
      "Epoch:  5 \t Loss:  252.96286\n",
      "Epoch:  6 \t Loss:  238.53928\n",
      "Epoch:  7 \t Loss:  225.57095\n",
      "Epoch:  8 \t Loss:  213.85413\n",
      "Epoch:  9 \t Loss:  203.21967\n",
      "Epoch:  10 \t Loss:  193.52666\n",
      "Epoch:  11 \t Loss:  184.65709\n",
      "Epoch:  12 \t Loss:  176.51068\n",
      "Epoch:  13 \t Loss:  169.00241\n",
      "Epoch:  14 \t Loss:  162.05998\n",
      "Epoch:  15 \t Loss:  155.62096\n",
      "Epoch:  16 \t Loss:  149.63173\n",
      "Epoch:  17 \t Loss:  144.04549\n",
      "Epoch:  18 \t Loss:  138.82193\n",
      "Epoch:  19 \t Loss:  133.92581\n",
      "Epoch:  20 \t Loss:  129.32579\n",
      "Epoch:  21 \t Loss:  124.995\n",
      "Epoch:  22 \t Loss:  120.90912\n",
      "Epoch:  23 \t Loss:  117.04705\n",
      "Epoch:  24 \t Loss:  113.389656\n",
      "Epoch:  25 \t Loss:  109.9202\n",
      "Epoch:  26 \t Loss:  106.62375\n",
      "Epoch:  27 \t Loss:  103.48663\n",
      "Epoch:  28 \t Loss:  100.49687\n",
      "Epoch:  29 \t Loss:  97.64372\n",
      "Epoch:  30 \t Loss:  94.91707\n",
      "Epoch:  31 \t Loss:  92.30817\n",
      "Epoch:  32 \t Loss:  89.80906\n",
      "Epoch:  33 \t Loss:  87.41236\n",
      "Epoch:  34 \t Loss:  85.1114\n",
      "Epoch:  35 \t Loss:  82.90018\n",
      "Epoch:  36 \t Loss:  80.77315\n",
      "Epoch:  37 \t Loss:  78.72525\n",
      "Epoch:  38 \t Loss:  76.751915\n",
      "Epoch:  39 \t Loss:  74.84863\n",
      "Epoch:  40 \t Loss:  73.01177\n",
      "Epoch:  41 \t Loss:  71.23755\n",
      "Epoch:  42 \t Loss:  69.52268\n",
      "Epoch:  43 \t Loss:  67.864075\n",
      "Epoch:  44 \t Loss:  66.25885\n",
      "Epoch:  45 \t Loss:  64.70441\n",
      "Epoch:  46 \t Loss:  63.19825\n",
      "Epoch:  47 \t Loss:  61.73802\n",
      "Epoch:  48 \t Loss:  60.321728\n",
      "Epoch:  49 \t Loss:  58.94729\n",
      "[[1.9825441 1.8784761 2.120468  ... 1.4411478 1.8061241 1.2126056]\n",
      " [1.9819142 1.9980072 1.9650067 ... 1.6920717 2.353756  2.0228324]\n",
      " [2.350554  2.0754988 2.4170673 ... 2.1570926 2.0207489 2.6798456]\n",
      " ...\n",
      " [2.0625796 2.1288123 2.0521288 ... 2.052047  2.1057768 2.3028545]\n",
      " [1.7824193 1.6536736 1.7725357 ... 1.9144313 1.2651228 1.6602678]\n",
      " [2.0399115 1.8692833 1.9698939 ... 1.9163092 1.989432  2.001722 ]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        _, train_loss = sess.run([train_op, loss], feed_dict={X: mean_centered_ratings_matrix})\n",
    "        print(\"Epoch: \", epoch, \"\\t Loss: \", train_loss)\n",
    "        \n",
    "    u = sess.run(U)\n",
    "    v = sess.run(V)\n",
    "    print(np.matmul(u, np.transpose(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having separate variables $o_i$ and $p_j$ for users and items, we can increase the size of the factor matrices to incorporate these bias variables. We need to add two additional columns to each factor matrix $U$ and $V$, to create a larger factor matrices of size $m \\times (k+2)$ and $n \\times (k + 2)$. The last two columns of each factor matrix are special, because they correspond to the bias components. We have:\n",
    "$$u_{i, k+1} = o_i$$\n",
    "$$u_{i, k+2} = 1$$\n",
    "$$v_{j, k+1} = 1$$\n",
    "$$u_{j, k+2} = p_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the loss funtion is given as follows:\n",
    "$$ Minimize\\ J = \\dfrac{1}{2} \\sum_{(i, j) \\in S} (r_{ij} - \\sum_{s=1}^{k+2} u_{is}v_{js})^2 + \\dfrac{\\lambda}{2} \\sum_{s=1}^{k+2} (\\sum_{i=1}^m u_{is}^2 + \\sum_{j=1}^n v_{js}^2 )  $$\n",
    "$$ subject\\ to: $$\n",
    "$$ (k+2)th\\ column\\ of\\ U\\ contains\\ only\\ 1s $$\n",
    "$$ (k+1)th\\ column\\ of\\ V\\ contains\\ only\\ 1s $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
