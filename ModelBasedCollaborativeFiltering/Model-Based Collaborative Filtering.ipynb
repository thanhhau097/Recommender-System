{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens 100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('ml-100k/u.data', names=names, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943\n",
      "1682\n"
     ]
    }
   ],
   "source": [
    "print(n_users)\n",
    "print(n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan = np.nan\n",
    "movielens_ratings_matrix = np.zeros((n_users, n_items)) * nan\n",
    "for line in df.itertuples():\n",
    "    movielens_ratings_matrix[line[1]-1, line[2]-1] = line[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.  3.  4. ... nan nan nan]\n",
      " [ 4. nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [ 5. nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan  5. nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "print(movielens_ratings_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In model-based methods, a summarized model of data is created up front, as with supervised and unsupervised learning methods. Therefore, the training is clearly separated from the prediction phase. <br>\n",
    "Examples of such methods in traditional machine learning include decision trees, rule-based methods, Bayes classifiers, regression models, support vector machines, and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike data classification, any entry in the ratings matrix maybe missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision and Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini index lies between 0 and 1, with smaller value being more indicative of greater discriminative power: $$ G(S) = 1 - \\sum_{i=1}^r p_i^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Gini(S \\Rightarrow [S_i, S_2] = \\dfrac{n_1.G(S_1) + n_2.G(S_2)}{n_1 + n_2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryMatrix():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def random_init(self, size):\n",
    "        self.matrix = np.random.randint(2, size=size)\n",
    "        \n",
    "    def get_label(self):\n",
    "        return self.matrix[:, -1]\n",
    "    \n",
    "    def get_train_data(self):\n",
    "        return self.matrix[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_matrix = BinaryMatrix()\n",
    "binary_matrix.random_init(size=[100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 1 ... 0 0 1]\n",
      " ...\n",
      " [0 1 0 ... 0 1 0]\n",
      " [1 0 1 ... 0 1 1]\n",
      " [1 1 0 ... 0 1 1]]\n",
      "[0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1\n",
      " 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1\n",
      " 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 1]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 1 0 1]\n",
      " [1 0 1 ... 1 0 1]\n",
      " [1 1 0 ... 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(binary_matrix.matrix)\n",
    "print(binary_matrix.get_label())\n",
    "print(binary_matrix.get_train_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(binary_matrix.get_train_data(),\n",
    "                                                   binary_matrix.get_label(), \n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.sum(y_test == predict_test) / len(y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /anaconda3/lib/python3.6/site-packages (0.9)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_matrix = sparse_random_matrix(1000, 1000, density=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to choose $j^{th}$ item to be target, and others $n - 1$ columns to be features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=10, n_iter=10, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: $j^{th}$ column is the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = ratings_matrix[:, :-1]\n",
    "y_data = ratings_matrix[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.99437071 1.99214704 1.97315971 1.96586987 1.95468439 1.94150428\n",
      " 1.9347887  1.93256985 1.92155791 1.90991485]\n"
     ]
    }
   ],
   "source": [
    "svd.fit(X_data)\n",
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use Decision tree on density matrix $m \\times d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_ratings_matrix = svd.transform(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04078747  0.01740403 -0.02456857 ... -0.13284343  0.05258134\n",
      "   0.10191544]\n",
      " [-0.01047693 -0.0370801  -0.08007284 ... -0.02028828 -0.05122547\n",
      "  -0.00393312]\n",
      " [-0.00324058  0.01206909 -0.05224786 ...  0.06357679  0.03219327\n",
      "  -0.10568864]\n",
      " ...\n",
      " [ 0.14103169  0.02498886  0.06968854 ...  0.05991687 -0.0404613\n",
      "   0.09306438]\n",
      " [-0.04791901 -0.05253255 -0.0669215  ... -0.00280977 -0.0266462\n",
      "   0.05423395]\n",
      " [ 0.02091143 -0.01567058 -0.03447271 ...  0.01500747  0.07855035\n",
      "  -0.03599526]]\n"
     ]
    }
   ],
   "source": [
    "print(reduction_ratings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = tree.DecisionTreeRegressor()\n",
    "\n",
    "clf.fit(reduction_ratings_matrix, y_data.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.14142136  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.14142136  0.          0.          0.\n",
      "  0.14142136  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.14142136  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.14142136  0.          0.14142136  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.14142136  0.\n",
      "  0.          0.          0.          0.          0.         -0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.14142136  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -0.14142136  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.         -0.14142136  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.14142136  0.          0.          0.14142136  0.          0.\n",
      "  0.          0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.14142136  0.\n",
      "  0.          0.         -0.14142136  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.14142136  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.14142136  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         -0.14142136  0.          0.\n",
      "  0.          0.          0.         -0.14142136  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.14142136  0.          0.          0.          0.\n",
      "  0.          0.         -0.14142136  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.14142136  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         -0.14142136  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.14142136\n",
      "  0.          0.          0.         -0.14142136  0.14142136  0.\n",
      "  0.          0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.14142136  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         -0.14142136  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.14142136  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -0.14142136  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.14142136  0.          0.14142136  0.          0.         -0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -0.14142136  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.14142136  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.14142136  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.14142136  0.          0.          0.          0.14142136  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -0.14142136  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         -0.14142136\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -0.14142136  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(reduction_ratings_matrix))\n",
    "print(clf.predict(reduction_ratings_matrix).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must loop through all items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule-based Collaborative Filtering\n",
    "(recommenderlab in R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a transaction database $ T = \\{ T_1...T_m \\} $ containing $m$ transactions, which are defined on $n$ items $I$. $I$ is the universal set of items, and each transaction $T_i$ is a subset of items in $I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(\\textbf{support})$ The $support$ of an item set $X \\subseteq I$ is the fraction of transactions in $T$, of which $X$ is a subset <br>\n",
    "If the support of an itemset is at least equal to predefined threshold $s$, then the itemset is said to be frequent. This threshold is referred to as the $minimum support$, these itemset are referred to as $frequent itemsets$ or $frequent patterns$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(\\textbf{Confidence})$ The confidence of the rule $X \\Rightarrow Y$ is the conditional probability that a transaction in $T$ contains $Y$, given that it also contains $X$. Therefore, the confidence is obtained by dividing the support of $X \\cup Y$ with the support of $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(\\textbf{Association Rules})$ A rule $X \\Rightarrow Y$ is said to be an association rule at a minimum support of $s$ and minimum confidence of $c$, if the following two conditions are satisfied:<br>\n",
    "1. The support of $X \\cup Y$ is at least $s$\n",
    "2. The confidence of $X \\Rightarrow Y$ Ã­ at least $c$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive  Bayes Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes Rule: $$ P(A|B) = \\dfrac{P(A).P(B|A)}{P(B)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using an Arbitrary Classification Model as a Blackbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to initialize the missing entries in the matrix with row averages, column averages, or with any simple collaborative filtering algorithm => remove bias, then fill 0 in the missing entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following two steps iterative approach:\n",
    "1. Use algorithm $A$ to estimate the missing entries of each column by setting it as the target variable and the remaining columns as the feature variables. For the remaining columns, use the current set of filled in values to create a complete matrix of feature variables. The observed ratings in the target column are used for training, the the missing ratings are predicted.\n",
    "2. Update all the missing entries based on the prediction of algorithm $A$ on each target colum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firsly, we will substract the rating values by row averages, then fill 0 in the missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specified_rating_indices(u):\n",
    "    return list(map(tuple, np.where(np.isfinite(u))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean rating for each user i using his specified rating\n",
    "def mean(u):\n",
    "    specified_ratings = u[specified_rating_indices(u)]#u[np.isfinite(u)]\n",
    "    m = sum(specified_ratings)/np.shape(specified_ratings)[0]\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_user_mean_ratings(ratings_matrix):\n",
    "    return np.array([mean(ratings_matrix[u, :]) for u in range(ratings_matrix.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_centered_ratings_matrix(ratings_matrix):\n",
    "    users_mean_rating = all_user_mean_ratings(ratings_matrix)\n",
    "    mean_centered_ratings_matrix = ratings_matrix - np.reshape(users_mean_rating, [-1, 1])\n",
    "    return mean_centered_ratings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_centered_ratings_matrix = get_mean_centered_ratings_matrix(movielens_ratings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.38970588 -0.61029412  0.38970588 ...         nan         nan\n",
      "          nan]\n",
      " [ 0.29032258         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " [        nan         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " ...\n",
      " [ 0.95454545         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " [        nan         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " [        nan  1.58928571         nan ...         nan         nan\n",
      "          nan]]\n"
     ]
    }
   ],
   "source": [
    "print(mean_centered_ratings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_zero_in_nan(matrix_2d):  \n",
    "    result_matrix = matrix_2d.copy()\n",
    "    for i in range(len(result_matrix)):\n",
    "        row = result_matrix[i]\n",
    "        \n",
    "        for j in range(len(row)):\n",
    "            if np.isnan(row[j]):\n",
    "                result_matrix[i][j] = 0\n",
    "    \n",
    "    return result_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_average_rating_matrix = fill_zero_in_nan(mean_centered_ratings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.38970588 -0.61029412  0.38970588 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.29032258  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.95454545  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          1.58928571  0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(row_average_rating_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.38970588 -0.61029412  0.38970588 ...         nan         nan\n",
      "          nan]\n",
      " [ 0.29032258         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " [        nan         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " ...\n",
      " [ 0.95454545         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " [        nan         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " [        nan  1.58928571         nan ...         nan         nan\n",
      "          nan]]\n"
     ]
    }
   ],
   "source": [
    "print(mean_centered_ratings_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we choose a column to be the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To easily using neural network, we should find the item which is rated most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "m = 0\n",
    "m_index = 0\n",
    "\n",
    "for i in range(n_items):\n",
    "    y_i = movielens_ratings_matrix[:, i]\n",
    "    total = 0\n",
    "\n",
    "    for element in y_i:\n",
    "        if not np.isnan(element):\n",
    "            total = total + 1\n",
    "    if total > m:\n",
    "        m = total\n",
    "        m_index = i\n",
    "print(m)\n",
    "print(m_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example, we will choose \n",
    "target_index = 49\n",
    "\n",
    "X_data = row_average_rating_matrix[:, :target_index]\n",
    "X_data = np.concatenate((X_data, row_average_rating_matrix[:, (target_index + 1):]), axis=1)\n",
    "\n",
    "y_data = row_average_rating_matrix[:, target_index]\n",
    "y_data_original = movielens_ratings_matrix[:, target_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "for i in range(len(y_data_original)):\n",
    "    if not np.isnan(y_data_original[i]):\n",
    "        train_indices.append(i)\n",
    "    else:\n",
    "        test_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_data[train_indices]\n",
    "y_train = y_data[train_indices]\n",
    "\n",
    "X_test = X_data[test_indices]\n",
    "y_test = y_data[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(524, 1681)\n",
      "(59, 1681)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def get_batch(X, y, iteration, batch_size):\n",
    "    indices = range(iteration * batch_size, (iteration + 1) * batch_size)\n",
    "    \n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a neural network to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_items-1], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=None, name='y')\n",
    "\n",
    "network = tf.layers.dense(inputs=X, activation=tf.nn.relu, units=256)\n",
    "network = tf.layers.dense(inputs=network, activation=tf.nn.relu, units=128)\n",
    "network = tf.layers.dense(inputs=network, activation=tf.nn.relu, units=128)\n",
    "network = tf.layers.dense(inputs=network, activation=tf.nn.relu, units=128)\n",
    "network = tf.layers.dense(inputs=network, activation=tf.nn.relu, units=32)\n",
    "\n",
    "outputs = tf.layers.dense(inputs=network, units=1)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(labels=y, predictions=outputs)\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "train_op = train_op.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 \t Training loss:  1.077972957262626 \t Validation loss:  0.82471585\n",
      "Epoch  1 \t Training loss:  0.8618305119184347 \t Validation loss:  0.73893553\n",
      "Epoch  2 \t Training loss:  0.810829169016618 \t Validation loss:  0.72461456\n",
      "Epoch  3 \t Training loss:  0.7979792961707481 \t Validation loss:  0.7211445\n",
      "Epoch  4 \t Training loss:  0.791462045449477 \t Validation loss:  0.7187523\n",
      "Epoch  5 \t Training loss:  0.7862932361089267 \t Validation loss:  0.7165719\n",
      "Epoch  6 \t Training loss:  0.7817765217560988 \t Validation loss:  0.7145987\n",
      "Epoch  7 \t Training loss:  0.7777542283901802 \t Validation loss:  0.7127913\n",
      "Epoch  8 \t Training loss:  0.7741373626085428 \t Validation loss:  0.7111686\n",
      "Epoch  9 \t Training loss:  0.7708963639461077 \t Validation loss:  0.7097496\n",
      "Epoch  10 \t Training loss:  0.7679761471656653 \t Validation loss:  0.70847917\n",
      "Epoch  11 \t Training loss:  0.7653097714369114 \t Validation loss:  0.70734936\n",
      "Epoch  12 \t Training loss:  0.7628743907580009 \t Validation loss:  0.70635575\n",
      "Epoch  13 \t Training loss:  0.7606191309598777 \t Validation loss:  0.7054742\n",
      "Epoch  14 \t Training loss:  0.7585171298338816 \t Validation loss:  0.7046652\n",
      "Epoch  15 \t Training loss:  0.7565579639031337 \t Validation loss:  0.7039597\n",
      "Epoch  16 \t Training loss:  0.754711240530014 \t Validation loss:  0.7033453\n",
      "Epoch  17 \t Training loss:  0.7529790431261063 \t Validation loss:  0.70278287\n",
      "Epoch  18 \t Training loss:  0.7513429394135108 \t Validation loss:  0.7022891\n",
      "Epoch  19 \t Training loss:  0.7498040708211752 \t Validation loss:  0.70183575\n",
      "Epoch  20 \t Training loss:  0.7483350930305628 \t Validation loss:  0.7014504\n",
      "Epoch  21 \t Training loss:  0.7469488114118576 \t Validation loss:  0.7011079\n",
      "Epoch  22 \t Training loss:  0.7456266912130209 \t Validation loss:  0.7008014\n",
      "Epoch  23 \t Training loss:  0.7443493398336264 \t Validation loss:  0.7005358\n",
      "Epoch  24 \t Training loss:  0.743117219209671 \t Validation loss:  0.7003175\n",
      "Epoch  25 \t Training loss:  0.741925202195461 \t Validation loss:  0.7001223\n",
      "Epoch  26 \t Training loss:  0.7407637926248404 \t Validation loss:  0.6999796\n",
      "Epoch  27 \t Training loss:  0.739641010761261 \t Validation loss:  0.6998559\n",
      "Epoch  28 \t Training loss:  0.7385424547470533 \t Validation loss:  0.699749\n",
      "Epoch  29 \t Training loss:  0.7374779316095206 \t Validation loss:  0.6996495\n",
      "Epoch  30 \t Training loss:  0.7364353269338608 \t Validation loss:  0.69957334\n",
      "Epoch  31 \t Training loss:  0.7354210973932193 \t Validation loss:  0.6994902\n",
      "Epoch  32 \t Training loss:  0.7344289049506187 \t Validation loss:  0.6994511\n",
      "Epoch  33 \t Training loss:  0.7334685536531301 \t Validation loss:  0.69943154\n",
      "Epoch  34 \t Training loss:  0.7325198879608741 \t Validation loss:  0.6994361\n",
      "Epoch  35 \t Training loss:  0.7315924474826225 \t Validation loss:  0.699438\n",
      "Epoch  36 \t Training loss:  0.7306820738774079 \t Validation loss:  0.6994577\n",
      "Epoch  37 \t Training loss:  0.7297868565871165 \t Validation loss:  0.699509\n",
      "Epoch  38 \t Training loss:  0.7289098944801551 \t Validation loss:  0.6995456\n",
      "Epoch  39 \t Training loss:  0.7280436452764731 \t Validation loss:  0.69962084\n",
      "Epoch  40 \t Training loss:  0.7271893363732558 \t Validation loss:  0.69967127\n",
      "Epoch  41 \t Training loss:  0.7263470578652161 \t Validation loss:  0.6997638\n",
      "Epoch  42 \t Training loss:  0.7255190214285484 \t Validation loss:  0.6998583\n",
      "Epoch  43 \t Training loss:  0.7247027014310543 \t Validation loss:  0.6999639\n",
      "Epoch  44 \t Training loss:  0.7238917283140696 \t Validation loss:  0.7001102\n",
      "Epoch  45 \t Training loss:  0.7230942852222002 \t Validation loss:  0.7002369\n",
      "Epoch  46 \t Training loss:  0.722303553040211 \t Validation loss:  0.7003754\n",
      "Epoch  47 \t Training loss:  0.7215252342132422 \t Validation loss:  0.70052665\n",
      "Epoch  48 \t Training loss:  0.7207558144743625 \t Validation loss:  0.70068794\n",
      "Epoch  49 \t Training loss:  0.71998654122536 \t Validation loss:  0.7008481\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 8\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0\n",
    "        \n",
    "        for i in range(len(X_train) // batch_size):\n",
    "            X_batch, y_batch = get_batch(X_train, y_train, iteration=i, batch_size=batch_size)\n",
    "\n",
    "            _, loss_batch = sess.run([train_op, loss], feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "            train_loss = train_loss + loss_batch\n",
    "    \n",
    "        train_loss = train_loss / (len(X_train) // batch_size)\n",
    "        val_loss = sess.run(loss, feed_dict={X: X_val, y: y_val})\n",
    "        \n",
    "        print(\"Epoch \", epoch, \"\\t Training loss: \", train_loss, \"\\t Validation loss: \", val_loss)\n",
    "    \n",
    "    y_predict = sess.run(outputs, feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May be this result is not good as expected, but we can use this approach. <br>\n",
    "We've predict ratings for one items, it is similar for other items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Factor Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Rank Intuition for Latent Factor Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank-k ratings matrix $R$ with size $m \\times n$ can always be expressed in the following product form of rank-k factors: $$ R = UV^T $$ <br>\n",
    "Here $U$ is an $m \\times k$ matrix, and $V$ is an $n \\times k$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when the ratings matrix $R$ has rank larger than $k$ , it can be often approximately expressed as the product of rank-k factors: $$ R \\approx UV^T $$ <br>\n",
    "The error of this approximation is equal to $ || R - UV^T ||^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Matrix Factorization Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the basic matrix factorization model, the $m \\times n$ ratings matrix $R$ is approximately factorized in to an $m \\times k$ matrix $U$ and $n \\times k$ matrix $V$, as follows: $$ R \\approx UV^T $$ <br>\n",
    "Each column of $U$ or $V$ is referred to as a $latent\\ vector$ or $latent\\ component$, where as each row of $U$ or $V$ is referred to as a $ latent\\ factor $ <br>\n",
    "Note: row $\\bar{u_i}$ is a user factor, row $\\bar{v_i}$ is a item factor, then: $$ r_{ij} \\approx \\bar{u_i}.\\bar{v_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unconstrained Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Minimize\\ J = \\dfrac{1}{2} \\|R - UV^T \\|^2 $$\n",
    "$$ subject\\ to: $$\n",
    "$$ No\\ constrain\\ on\\ U\\ and\\ V $$ <br>\n",
    "We only compute on observed entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual Matrix: $ (R - UV^T) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the set of all user-item pairs $(i, j)$, which are observed in R, be denoted by $S$: $$S = \\{ (i, j):\\ r_{ij}\\ is\\ observed \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $(i, j)$th entry of matrix R can be predicted as follows: $$ \\hat{r}_{ij} = \\sum_{s=1}^k u_{is}v_{js} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the observed and predicted value of a specified entri $(i, j)$ is given by: $$ e_{ij} = (r_{ij} - \\hat{r}_{ij}) = (r_{ij} - \\sum_{s=1}^k u_{is}v_{js}) $$<br>\n",
    "The objective function now is: \n",
    "$$ Minimize\\ J = \\dfrac{1}{2} \\sum_{(i, j) \\in S} e_{ij}^2 = \\dfrac{1}{2} \\sum_{(i, j) \\in S} (r_{ij} - \\sum_{s=1}^k u_{is}v_{js})^2$$\n",
    "$$ subject\\ to: $$\n",
    "$$ No\\ constrain\\ on\\ U\\ and\\ V $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_specified_rating_indices(u):\n",
    "    return tf.where(tf.is_finite(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loss_on_observed_value(original_matrix, predicted_matrix):\n",
    "    indices = tf_specified_rating_indices(original_matrix)\n",
    "    \n",
    "#     observed_values = original_matrix[indices]\n",
    "#     predicted_value = predicted_matrix[indices]\n",
    "    observed_values = tf.gather_nd(original_matrix, indices)\n",
    "    predicted_values = tf.gather_nd(predicted_matrix, indices)\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(observed_values, predicted_values)\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[n_users, n_items], name='X')\n",
    "# X = tf.Variable(initial_value=mean_centered_ratings_matrix, trainable=False)\n",
    "# indices = tf.where(tf.is_finite(X))\n",
    "\n",
    "U = tf.Variable(tf.random_uniform(shape=[n_users, k]))\n",
    "V = tf.Variable(tf.random_uniform(shape=[n_items, k]))\n",
    "\n",
    "outputs = tf.matmul(U, tf.transpose(V))\n",
    "\n",
    "loss = loss_on_observed_value(X, outputs)\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=1).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \t Loss:  159.51732\n",
      "Epoch:  1 \t Loss:  132.48048\n",
      "Epoch:  2 \t Loss:  112.48997\n",
      "Epoch:  3 \t Loss:  97.2084\n",
      "Epoch:  4 \t Loss:  85.21045\n",
      "Epoch:  5 \t Loss:  75.58179\n",
      "Epoch:  6 \t Loss:  67.71176\n",
      "Epoch:  7 \t Loss:  61.1784\n",
      "Epoch:  8 \t Loss:  55.68175\n",
      "Epoch:  9 \t Loss:  51.00334\n",
      "Epoch:  10 \t Loss:  46.9809\n",
      "Epoch:  11 \t Loss:  43.491154\n",
      "Epoch:  12 \t Loss:  40.439346\n",
      "Epoch:  13 \t Loss:  37.751415\n",
      "Epoch:  14 \t Loss:  35.36869\n",
      "Epoch:  15 \t Loss:  33.24424\n",
      "Epoch:  16 \t Loss:  31.33992\n",
      "Epoch:  17 \t Loss:  29.624762\n",
      "Epoch:  18 \t Loss:  28.07314\n",
      "Epoch:  19 \t Loss:  26.663685\n",
      "Epoch:  20 \t Loss:  25.37863\n",
      "Epoch:  21 \t Loss:  24.20281\n",
      "Epoch:  22 \t Loss:  23.123533\n",
      "Epoch:  23 \t Loss:  22.129915\n",
      "Epoch:  24 \t Loss:  21.212542\n",
      "Epoch:  25 \t Loss:  20.36336\n",
      "Epoch:  26 \t Loss:  19.5754\n",
      "Epoch:  27 \t Loss:  18.842518\n",
      "Epoch:  28 \t Loss:  18.159353\n",
      "Epoch:  29 \t Loss:  17.52133\n",
      "Epoch:  30 \t Loss:  16.92424\n",
      "Epoch:  31 \t Loss:  16.364473\n",
      "Epoch:  32 \t Loss:  15.838733\n",
      "Epoch:  33 \t Loss:  15.344217\n",
      "Epoch:  34 \t Loss:  14.878292\n",
      "Epoch:  35 \t Loss:  14.43865\n",
      "Epoch:  36 \t Loss:  14.02325\n",
      "Epoch:  37 \t Loss:  13.630213\n",
      "Epoch:  38 \t Loss:  13.257862\n",
      "Epoch:  39 \t Loss:  12.904674\n",
      "Epoch:  40 \t Loss:  12.5692625\n",
      "Epoch:  41 \t Loss:  12.25039\n",
      "Epoch:  42 \t Loss:  11.94691\n",
      "Epoch:  43 \t Loss:  11.657763\n",
      "Epoch:  44 \t Loss:  11.382017\n",
      "Epoch:  45 \t Loss:  11.118808\n",
      "Epoch:  46 \t Loss:  10.867308\n",
      "Epoch:  47 \t Loss:  10.626815\n",
      "Epoch:  48 \t Loss:  10.396631\n",
      "Epoch:  49 \t Loss:  10.176139\n",
      "Epoch:  50 \t Loss:  9.964761\n",
      "Epoch:  51 \t Loss:  9.761969\n",
      "Epoch:  52 \t Loss:  9.567272\n",
      "Epoch:  53 \t Loss:  9.380235\n",
      "Epoch:  54 \t Loss:  9.20039\n",
      "Epoch:  55 \t Loss:  9.027388\n",
      "Epoch:  56 \t Loss:  8.8608265\n",
      "Epoch:  57 \t Loss:  8.700396\n",
      "Epoch:  58 \t Loss:  8.545757\n",
      "Epoch:  59 \t Loss:  8.396623\n",
      "Epoch:  60 \t Loss:  8.252723\n",
      "Epoch:  61 \t Loss:  8.113772\n",
      "Epoch:  62 \t Loss:  7.979559\n",
      "Epoch:  63 \t Loss:  7.8498454\n",
      "Epoch:  64 \t Loss:  7.7244005\n",
      "Epoch:  65 \t Loss:  7.6030526\n",
      "Epoch:  66 \t Loss:  7.4855886\n",
      "Epoch:  67 \t Loss:  7.3718395\n",
      "Epoch:  68 \t Loss:  7.261644\n",
      "Epoch:  69 \t Loss:  7.1548424\n",
      "Epoch:  70 \t Loss:  7.051276\n",
      "Epoch:  71 \t Loss:  6.95082\n",
      "Epoch:  72 \t Loss:  6.8533325\n",
      "Epoch:  73 \t Loss:  6.758696\n",
      "Epoch:  74 \t Loss:  6.6667833\n",
      "Epoch:  75 \t Loss:  6.577476\n",
      "Epoch:  76 \t Loss:  6.4906964\n",
      "Epoch:  77 \t Loss:  6.406316\n",
      "Epoch:  78 \t Loss:  6.3242435\n",
      "Epoch:  79 \t Loss:  6.244406\n",
      "Epoch:  80 \t Loss:  6.1667\n",
      "Epoch:  81 \t Loss:  6.09105\n",
      "Epoch:  82 \t Loss:  6.0173774\n",
      "Epoch:  83 \t Loss:  5.945612\n",
      "Epoch:  84 \t Loss:  5.8756785\n",
      "Epoch:  85 \t Loss:  5.807515\n",
      "Epoch:  86 \t Loss:  5.741051\n",
      "Epoch:  87 \t Loss:  5.6762233\n",
      "Epoch:  88 \t Loss:  5.6129913\n",
      "Epoch:  89 \t Loss:  5.55128\n",
      "Epoch:  90 \t Loss:  5.491041\n",
      "Epoch:  91 \t Loss:  5.432238\n",
      "Epoch:  92 \t Loss:  5.3747935\n",
      "Epoch:  93 \t Loss:  5.3186975\n",
      "Epoch:  94 \t Loss:  5.263885\n",
      "Epoch:  95 \t Loss:  5.210314\n",
      "Epoch:  96 \t Loss:  5.1579475\n",
      "Epoch:  97 \t Loss:  5.106745\n",
      "Epoch:  98 \t Loss:  5.0566673\n",
      "Epoch:  99 \t Loss:  5.007682\n",
      "Epoch:  100 \t Loss:  4.9597607\n",
      "Epoch:  101 \t Loss:  4.9128613\n",
      "Epoch:  102 \t Loss:  4.866948\n",
      "Epoch:  103 \t Loss:  4.822001\n",
      "Epoch:  104 \t Loss:  4.777994\n",
      "Epoch:  105 \t Loss:  4.7348866\n",
      "Epoch:  106 \t Loss:  4.6926627\n",
      "Epoch:  107 \t Loss:  4.651289\n",
      "Epoch:  108 \t Loss:  4.6107445\n",
      "Epoch:  109 \t Loss:  4.5710063\n",
      "Epoch:  110 \t Loss:  4.53205\n",
      "Epoch:  111 \t Loss:  4.49385\n",
      "Epoch:  112 \t Loss:  4.456384\n",
      "Epoch:  113 \t Loss:  4.419644\n",
      "Epoch:  114 \t Loss:  4.383599\n",
      "Epoch:  115 \t Loss:  4.3482366\n",
      "Epoch:  116 \t Loss:  4.3135242\n",
      "Epoch:  117 \t Loss:  4.279464\n",
      "Epoch:  118 \t Loss:  4.246023\n",
      "Epoch:  119 \t Loss:  4.213196\n",
      "Epoch:  120 \t Loss:  4.180955\n",
      "Epoch:  121 \t Loss:  4.1492977\n",
      "Epoch:  122 \t Loss:  4.1181965\n",
      "Epoch:  123 \t Loss:  4.0876412\n",
      "Epoch:  124 \t Loss:  4.057624\n",
      "Epoch:  125 \t Loss:  4.0281315\n",
      "Epoch:  126 \t Loss:  3.99914\n",
      "Epoch:  127 \t Loss:  3.970649\n",
      "Epoch:  128 \t Loss:  3.942632\n",
      "Epoch:  129 \t Loss:  3.9150922\n",
      "Epoch:  130 \t Loss:  3.8880053\n",
      "Epoch:  131 \t Loss:  3.8613694\n",
      "Epoch:  132 \t Loss:  3.8351681\n",
      "Epoch:  133 \t Loss:  3.8093975\n",
      "Epoch:  134 \t Loss:  3.784046\n",
      "Epoch:  135 \t Loss:  3.7590916\n",
      "Epoch:  136 \t Loss:  3.7345438\n",
      "Epoch:  137 \t Loss:  3.7103803\n",
      "Epoch:  138 \t Loss:  3.6865952\n",
      "Epoch:  139 \t Loss:  3.6631832\n",
      "Epoch:  140 \t Loss:  3.6401343\n",
      "Epoch:  141 \t Loss:  3.617432\n",
      "Epoch:  142 \t Loss:  3.5950837\n",
      "Epoch:  143 \t Loss:  3.57307\n",
      "Epoch:  144 \t Loss:  3.5513885\n",
      "Epoch:  145 \t Loss:  3.5300307\n",
      "Epoch:  146 \t Loss:  3.508991\n",
      "Epoch:  147 \t Loss:  3.4882622\n",
      "Epoch:  148 \t Loss:  3.4678288\n",
      "Epoch:  149 \t Loss:  3.447699\n",
      "Epoch:  150 \t Loss:  3.4278572\n",
      "Epoch:  151 \t Loss:  3.408299\n",
      "Epoch:  152 \t Loss:  3.3890178\n",
      "Epoch:  153 \t Loss:  3.370015\n",
      "Epoch:  154 \t Loss:  3.3512738\n",
      "Epoch:  155 \t Loss:  3.3328013\n",
      "Epoch:  156 \t Loss:  3.3145776\n",
      "Epoch:  157 \t Loss:  3.2966063\n",
      "Epoch:  158 \t Loss:  3.2788837\n",
      "Epoch:  159 \t Loss:  3.2614\n",
      "Epoch:  160 \t Loss:  3.2441545\n",
      "Epoch:  161 \t Loss:  3.227138\n",
      "Epoch:  162 \t Loss:  3.2103503\n",
      "Epoch:  163 \t Loss:  3.19378\n",
      "Epoch:  164 \t Loss:  3.1774375\n",
      "Epoch:  165 \t Loss:  3.1612995\n",
      "Epoch:  166 \t Loss:  3.145375\n",
      "Epoch:  167 \t Loss:  3.1296585\n",
      "Epoch:  168 \t Loss:  3.1141398\n",
      "Epoch:  169 \t Loss:  3.0988197\n",
      "Epoch:  170 \t Loss:  3.0837011\n",
      "Epoch:  171 \t Loss:  3.0687687\n",
      "Epoch:  172 \t Loss:  3.05402\n",
      "Epoch:  173 \t Loss:  3.0394597\n",
      "Epoch:  174 \t Loss:  3.025076\n",
      "Epoch:  175 \t Loss:  3.010872\n",
      "Epoch:  176 \t Loss:  2.9968402\n",
      "Epoch:  177 \t Loss:  2.9829774\n",
      "Epoch:  178 \t Loss:  2.9692948\n",
      "Epoch:  179 \t Loss:  2.9557624\n",
      "Epoch:  180 \t Loss:  2.9423988\n",
      "Epoch:  181 \t Loss:  2.9291894\n",
      "Epoch:  182 \t Loss:  2.9161434\n",
      "Epoch:  183 \t Loss:  2.9032457\n",
      "Epoch:  184 \t Loss:  2.8904994\n",
      "Epoch:  185 \t Loss:  2.8779044\n",
      "Epoch:  186 \t Loss:  2.8654506\n",
      "Epoch:  187 \t Loss:  2.8531456\n",
      "Epoch:  188 \t Loss:  2.8409789\n",
      "Epoch:  189 \t Loss:  2.8289464\n",
      "Epoch:  190 \t Loss:  2.8170543\n",
      "Epoch:  191 \t Loss:  2.8052979\n",
      "Epoch:  192 \t Loss:  2.7936687\n",
      "Epoch:  193 \t Loss:  2.78217\n",
      "Epoch:  194 \t Loss:  2.7707996\n",
      "Epoch:  195 \t Loss:  2.7595541\n",
      "Epoch:  196 \t Loss:  2.7484312\n",
      "Epoch:  197 \t Loss:  2.7374294\n",
      "Epoch:  198 \t Loss:  2.7265496\n",
      "Epoch:  199 \t Loss:  2.7157824\n",
      "Epoch:  200 \t Loss:  2.7051337\n",
      "Epoch:  201 \t Loss:  2.6945984\n",
      "Epoch:  202 \t Loss:  2.684174\n",
      "Epoch:  203 \t Loss:  2.6738586\n",
      "Epoch:  204 \t Loss:  2.6636555\n",
      "Epoch:  205 \t Loss:  2.6535547\n",
      "Epoch:  206 \t Loss:  2.6435614\n",
      "Epoch:  207 \t Loss:  2.63367\n",
      "Epoch:  208 \t Loss:  2.6238782\n",
      "Epoch:  209 \t Loss:  2.6141913\n",
      "Epoch:  210 \t Loss:  2.6046\n",
      "Epoch:  211 \t Loss:  2.595111\n",
      "Epoch:  212 \t Loss:  2.5857105\n",
      "Epoch:  213 \t Loss:  2.576409\n",
      "Epoch:  214 \t Loss:  2.5671995\n",
      "Epoch:  215 \t Loss:  2.5580816\n",
      "Epoch:  216 \t Loss:  2.5490513\n",
      "Epoch:  217 \t Loss:  2.5401108\n",
      "Epoch:  218 \t Loss:  2.531262\n",
      "Epoch:  219 \t Loss:  2.522495\n",
      "Epoch:  220 \t Loss:  2.513817\n",
      "Epoch:  221 \t Loss:  2.5052214\n",
      "Epoch:  222 \t Loss:  2.4967039\n",
      "Epoch:  223 \t Loss:  2.488271\n",
      "Epoch:  224 \t Loss:  2.479919\n",
      "Epoch:  225 \t Loss:  2.4716463\n",
      "Epoch:  226 \t Loss:  2.463449\n",
      "Epoch:  227 \t Loss:  2.4553344\n",
      "Epoch:  228 \t Loss:  2.4472878\n",
      "Epoch:  229 \t Loss:  2.4393203\n",
      "Epoch:  230 \t Loss:  2.4314268\n",
      "Epoch:  231 \t Loss:  2.4236052\n",
      "Epoch:  232 \t Loss:  2.4158556\n",
      "Epoch:  233 \t Loss:  2.4081783\n",
      "Epoch:  234 \t Loss:  2.400566\n",
      "Epoch:  235 \t Loss:  2.3930259\n",
      "Epoch:  236 \t Loss:  2.385551\n",
      "Epoch:  237 \t Loss:  2.3781476\n",
      "Epoch:  238 \t Loss:  2.370809\n",
      "Epoch:  239 \t Loss:  2.3635328\n",
      "Epoch:  240 \t Loss:  2.3563235\n",
      "Epoch:  241 \t Loss:  2.3491762\n",
      "Epoch:  242 \t Loss:  2.3420932\n",
      "Epoch:  243 \t Loss:  2.3350663\n",
      "Epoch:  244 \t Loss:  2.3281085\n",
      "Epoch:  245 \t Loss:  2.3212068\n",
      "Epoch:  246 \t Loss:  2.3143656\n",
      "Epoch:  247 \t Loss:  2.3075838\n",
      "Epoch:  248 \t Loss:  2.3008597\n",
      "Epoch:  249 \t Loss:  2.29419\n",
      "Epoch:  250 \t Loss:  2.2875774\n",
      "Epoch:  251 \t Loss:  2.281022\n",
      "Epoch:  252 \t Loss:  2.27452\n",
      "Epoch:  253 \t Loss:  2.268074\n",
      "Epoch:  254 \t Loss:  2.2616808\n",
      "Epoch:  255 \t Loss:  2.2553415\n",
      "Epoch:  256 \t Loss:  2.2490523\n",
      "Epoch:  257 \t Loss:  2.2428157\n",
      "Epoch:  258 \t Loss:  2.2366316\n",
      "Epoch:  259 \t Loss:  2.2304964\n",
      "Epoch:  260 \t Loss:  2.2244103\n",
      "Epoch:  261 \t Loss:  2.2183764\n",
      "Epoch:  262 \t Loss:  2.212389\n",
      "Epoch:  263 \t Loss:  2.206452\n",
      "Epoch:  264 \t Loss:  2.2005599\n",
      "Epoch:  265 \t Loss:  2.1947138\n",
      "Epoch:  266 \t Loss:  2.1889138\n",
      "Epoch:  267 \t Loss:  2.183163\n",
      "Epoch:  268 \t Loss:  2.177453\n",
      "Epoch:  269 \t Loss:  2.1717873\n",
      "Epoch:  270 \t Loss:  2.1661682\n",
      "Epoch:  271 \t Loss:  2.160595\n",
      "Epoch:  272 \t Loss:  2.1550624\n",
      "Epoch:  273 \t Loss:  2.1495736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  274 \t Loss:  2.1441236\n",
      "Epoch:  275 \t Loss:  2.1387193\n",
      "Epoch:  276 \t Loss:  2.1333525\n",
      "Epoch:  277 \t Loss:  2.1280262\n",
      "Epoch:  278 \t Loss:  2.122744\n",
      "Epoch:  279 \t Loss:  2.1175\n",
      "Epoch:  280 \t Loss:  2.1122932\n",
      "Epoch:  281 \t Loss:  2.107126\n",
      "Epoch:  282 \t Loss:  2.1019971\n",
      "Epoch:  283 \t Loss:  2.0969088\n",
      "Epoch:  284 \t Loss:  2.091858\n",
      "Epoch:  285 \t Loss:  2.0868394\n",
      "Epoch:  286 \t Loss:  2.081859\n",
      "Epoch:  287 \t Loss:  2.0769193\n",
      "Epoch:  288 \t Loss:  2.0720117\n",
      "Epoch:  289 \t Loss:  2.0671406\n",
      "Epoch:  290 \t Loss:  2.0623057\n",
      "Epoch:  291 \t Loss:  2.0575035\n",
      "Epoch:  292 \t Loss:  2.0527384\n",
      "Epoch:  293 \t Loss:  2.048005\n",
      "Epoch:  294 \t Loss:  2.043304\n",
      "Epoch:  295 \t Loss:  2.0386367\n",
      "Epoch:  296 \t Loss:  2.034005\n",
      "Epoch:  297 \t Loss:  2.0294008\n",
      "Epoch:  298 \t Loss:  2.0248356\n",
      "Epoch:  299 \t Loss:  2.0202982\n",
      "Epoch:  300 \t Loss:  2.0157926\n",
      "Epoch:  301 \t Loss:  2.0113175\n",
      "Epoch:  302 \t Loss:  2.0068743\n",
      "Epoch:  303 \t Loss:  2.0024612\n",
      "Epoch:  304 \t Loss:  1.9980786\n",
      "Epoch:  305 \t Loss:  1.9937257\n",
      "Epoch:  306 \t Loss:  1.9894022\n",
      "Epoch:  307 \t Loss:  1.9851087\n",
      "Epoch:  308 \t Loss:  1.9808414\n",
      "Epoch:  309 \t Loss:  1.9766066\n",
      "Epoch:  310 \t Loss:  1.9723988\n",
      "Epoch:  311 \t Loss:  1.9682181\n",
      "Epoch:  312 \t Loss:  1.9640675\n",
      "Epoch:  313 \t Loss:  1.9599427\n",
      "Epoch:  314 \t Loss:  1.9558454\n",
      "Epoch:  315 \t Loss:  1.9517747\n",
      "Epoch:  316 \t Loss:  1.9477303\n",
      "Epoch:  317 \t Loss:  1.9437118\n",
      "Epoch:  318 \t Loss:  1.9397212\n",
      "Epoch:  319 \t Loss:  1.9357554\n",
      "Epoch:  320 \t Loss:  1.9318186\n",
      "Epoch:  321 \t Loss:  1.927904\n",
      "Epoch:  322 \t Loss:  1.924013\n",
      "Epoch:  323 \t Loss:  1.9201502\n",
      "Epoch:  324 \t Loss:  1.91631\n",
      "Epoch:  325 \t Loss:  1.9124944\n",
      "Epoch:  326 \t Loss:  1.9087055\n",
      "Epoch:  327 \t Loss:  1.9049354\n",
      "Epoch:  328 \t Loss:  1.9011931\n",
      "Epoch:  329 \t Loss:  1.8974766\n",
      "Epoch:  330 \t Loss:  1.8937769\n",
      "Epoch:  331 \t Loss:  1.8901049\n",
      "Epoch:  332 \t Loss:  1.8864526\n",
      "Epoch:  333 \t Loss:  1.8828249\n",
      "Epoch:  334 \t Loss:  1.8792185\n",
      "Epoch:  335 \t Loss:  1.8756341\n",
      "Epoch:  336 \t Loss:  1.8720737\n",
      "Epoch:  337 \t Loss:  1.8685325\n",
      "Epoch:  338 \t Loss:  1.8650147\n",
      "Epoch:  339 \t Loss:  1.8615166\n",
      "Epoch:  340 \t Loss:  1.8580431\n",
      "Epoch:  341 \t Loss:  1.8545879\n",
      "Epoch:  342 \t Loss:  1.8511531\n",
      "Epoch:  343 \t Loss:  1.8477386\n",
      "Epoch:  344 \t Loss:  1.8443453\n",
      "Epoch:  345 \t Loss:  1.8409728\n",
      "Epoch:  346 \t Loss:  1.8376187\n",
      "Epoch:  347 \t Loss:  1.8342847\n",
      "Epoch:  348 \t Loss:  1.8309712\n",
      "Epoch:  349 \t Loss:  1.8276753\n",
      "Epoch:  350 \t Loss:  1.8244048\n",
      "Epoch:  351 \t Loss:  1.8211472\n",
      "Epoch:  352 \t Loss:  1.8179119\n",
      "Epoch:  353 \t Loss:  1.8146929\n",
      "Epoch:  354 \t Loss:  1.8114921\n",
      "Epoch:  355 \t Loss:  1.8083128\n",
      "Epoch:  356 \t Loss:  1.8051503\n",
      "Epoch:  357 \t Loss:  1.8020067\n",
      "Epoch:  358 \t Loss:  1.7988784\n",
      "Epoch:  359 \t Loss:  1.79577\n",
      "Epoch:  360 \t Loss:  1.7926764\n",
      "Epoch:  361 \t Loss:  1.7896025\n",
      "Epoch:  362 \t Loss:  1.7865448\n",
      "Epoch:  363 \t Loss:  1.7835069\n",
      "Epoch:  364 \t Loss:  1.7804857\n",
      "Epoch:  365 \t Loss:  1.77748\n",
      "Epoch:  366 \t Loss:  1.7744915\n",
      "Epoch:  367 \t Loss:  1.7715181\n",
      "Epoch:  368 \t Loss:  1.768565\n",
      "Epoch:  369 \t Loss:  1.7656255\n",
      "Epoch:  370 \t Loss:  1.7627015\n",
      "Epoch:  371 \t Loss:  1.7597947\n",
      "Epoch:  372 \t Loss:  1.7569028\n",
      "Epoch:  373 \t Loss:  1.754027\n",
      "Epoch:  374 \t Loss:  1.7511675\n",
      "Epoch:  375 \t Loss:  1.7483213\n",
      "Epoch:  376 \t Loss:  1.7454947\n",
      "Epoch:  377 \t Loss:  1.7426798\n",
      "Epoch:  378 \t Loss:  1.73988\n",
      "Epoch:  379 \t Loss:  1.7371\n",
      "Epoch:  380 \t Loss:  1.73433\n",
      "Epoch:  381 \t Loss:  1.7315747\n",
      "Epoch:  382 \t Loss:  1.7288359\n",
      "Epoch:  383 \t Loss:  1.7261122\n",
      "Epoch:  384 \t Loss:  1.7234006\n",
      "Epoch:  385 \t Loss:  1.7207053\n",
      "Epoch:  386 \t Loss:  1.718024\n",
      "Epoch:  387 \t Loss:  1.715356\n",
      "Epoch:  388 \t Loss:  1.7126995\n",
      "Epoch:  389 \t Loss:  1.7100592\n",
      "Epoch:  390 \t Loss:  1.7074325\n",
      "Epoch:  391 \t Loss:  1.7048185\n",
      "Epoch:  392 \t Loss:  1.7022203\n",
      "Epoch:  393 \t Loss:  1.6996328\n",
      "Epoch:  394 \t Loss:  1.6970594\n",
      "Epoch:  395 \t Loss:  1.6944995\n",
      "Epoch:  396 \t Loss:  1.6919533\n",
      "Epoch:  397 \t Loss:  1.6894178\n",
      "Epoch:  398 \t Loss:  1.6868991\n",
      "Epoch:  399 \t Loss:  1.68439\n",
      "Epoch:  400 \t Loss:  1.6818947\n",
      "Epoch:  401 \t Loss:  1.6794089\n",
      "Epoch:  402 \t Loss:  1.67694\n",
      "Epoch:  403 \t Loss:  1.6744794\n",
      "Epoch:  404 \t Loss:  1.6720359\n",
      "Epoch:  405 \t Loss:  1.6696016\n",
      "Epoch:  406 \t Loss:  1.6671792\n",
      "Epoch:  407 \t Loss:  1.6647699\n",
      "Epoch:  408 \t Loss:  1.6623718\n",
      "Epoch:  409 \t Loss:  1.6599857\n",
      "Epoch:  410 \t Loss:  1.6576087\n",
      "Epoch:  411 \t Loss:  1.6552457\n",
      "Epoch:  412 \t Loss:  1.6528943\n",
      "Epoch:  413 \t Loss:  1.6505531\n",
      "Epoch:  414 \t Loss:  1.6482252\n",
      "Epoch:  415 \t Loss:  1.645909\n",
      "Epoch:  416 \t Loss:  1.643601\n",
      "Epoch:  417 \t Loss:  1.6413059\n",
      "Epoch:  418 \t Loss:  1.6390207\n",
      "Epoch:  419 \t Loss:  1.6367503\n",
      "Epoch:  420 \t Loss:  1.6344854\n",
      "Epoch:  421 \t Loss:  1.6322331\n",
      "Epoch:  422 \t Loss:  1.6299931\n",
      "Epoch:  423 \t Loss:  1.6277606\n",
      "Epoch:  424 \t Loss:  1.6255431\n",
      "Epoch:  425 \t Loss:  1.6233325\n",
      "Epoch:  426 \t Loss:  1.6211338\n",
      "Epoch:  427 \t Loss:  1.6189429\n",
      "Epoch:  428 \t Loss:  1.6167637\n",
      "Epoch:  429 \t Loss:  1.6145966\n",
      "Epoch:  430 \t Loss:  1.612438\n",
      "Epoch:  431 \t Loss:  1.61029\n",
      "Epoch:  432 \t Loss:  1.6081496\n",
      "Epoch:  433 \t Loss:  1.6060215\n",
      "Epoch:  434 \t Loss:  1.6039009\n",
      "Epoch:  435 \t Loss:  1.6017922\n",
      "Epoch:  436 \t Loss:  1.5996922\n",
      "Epoch:  437 \t Loss:  1.5976019\n",
      "Epoch:  438 \t Loss:  1.5955216\n",
      "Epoch:  439 \t Loss:  1.5934484\n",
      "Epoch:  440 \t Loss:  1.5913869\n",
      "Epoch:  441 \t Loss:  1.5893344\n",
      "Epoch:  442 \t Loss:  1.5872906\n",
      "Epoch:  443 \t Loss:  1.5852578\n",
      "Epoch:  444 \t Loss:  1.5832325\n",
      "Epoch:  445 \t Loss:  1.5812153\n",
      "Epoch:  446 \t Loss:  1.5792075\n",
      "Epoch:  447 \t Loss:  1.5772088\n",
      "Epoch:  448 \t Loss:  1.575219\n",
      "Epoch:  449 \t Loss:  1.5732396\n",
      "Epoch:  450 \t Loss:  1.5712674\n",
      "Epoch:  451 \t Loss:  1.5693046\n",
      "Epoch:  452 \t Loss:  1.5673494\n",
      "Epoch:  453 \t Loss:  1.5654016\n",
      "Epoch:  454 \t Loss:  1.5634642\n",
      "Epoch:  455 \t Loss:  1.5615367\n",
      "Epoch:  456 \t Loss:  1.5596141\n",
      "Epoch:  457 \t Loss:  1.5577035\n",
      "Epoch:  458 \t Loss:  1.5557972\n",
      "Epoch:  459 \t Loss:  1.5539004\n",
      "Epoch:  460 \t Loss:  1.5520122\n",
      "Epoch:  461 \t Loss:  1.5501337\n",
      "Epoch:  462 \t Loss:  1.5482613\n",
      "Epoch:  463 \t Loss:  1.5463973\n",
      "Epoch:  464 \t Loss:  1.5445387\n",
      "Epoch:  465 \t Loss:  1.5426911\n",
      "Epoch:  466 \t Loss:  1.5408503\n",
      "Epoch:  467 \t Loss:  1.5390182\n",
      "Epoch:  468 \t Loss:  1.5371928\n",
      "Epoch:  469 \t Loss:  1.5353771\n",
      "Epoch:  470 \t Loss:  1.5335667\n",
      "Epoch:  471 \t Loss:  1.531764\n",
      "Epoch:  472 \t Loss:  1.5299687\n",
      "Epoch:  473 \t Loss:  1.5281825\n",
      "Epoch:  474 \t Loss:  1.5264019\n",
      "Epoch:  475 \t Loss:  1.5246298\n",
      "Epoch:  476 \t Loss:  1.5228647\n",
      "Epoch:  477 \t Loss:  1.5211071\n",
      "Epoch:  478 \t Loss:  1.5193563\n",
      "Epoch:  479 \t Loss:  1.5176132\n",
      "Epoch:  480 \t Loss:  1.5158753\n",
      "Epoch:  481 \t Loss:  1.514148\n",
      "Epoch:  482 \t Loss:  1.5124253\n",
      "Epoch:  483 \t Loss:  1.5107094\n",
      "Epoch:  484 \t Loss:  1.5090007\n",
      "Epoch:  485 \t Loss:  1.5072985\n",
      "Epoch:  486 \t Loss:  1.5056034\n",
      "Epoch:  487 \t Loss:  1.5039172\n",
      "Epoch:  488 \t Loss:  1.5022331\n",
      "Epoch:  489 \t Loss:  1.5005609\n",
      "Epoch:  490 \t Loss:  1.4988918\n",
      "Epoch:  491 \t Loss:  1.4972303\n",
      "Epoch:  492 \t Loss:  1.4955772\n",
      "Epoch:  493 \t Loss:  1.4939286\n",
      "Epoch:  494 \t Loss:  1.4922869\n",
      "Epoch:  495 \t Loss:  1.4906512\n",
      "Epoch:  496 \t Loss:  1.4890226\n",
      "Epoch:  497 \t Loss:  1.4874009\n",
      "Epoch:  498 \t Loss:  1.4857862\n",
      "Epoch:  499 \t Loss:  1.4841747\n",
      "Epoch:  500 \t Loss:  1.4825722\n",
      "Epoch:  501 \t Loss:  1.4809752\n",
      "Epoch:  502 \t Loss:  1.4793837\n",
      "Epoch:  503 \t Loss:  1.4777987\n",
      "Epoch:  504 \t Loss:  1.4762205\n",
      "Epoch:  505 \t Loss:  1.4746462\n",
      "Epoch:  506 \t Loss:  1.4730812\n",
      "Epoch:  507 \t Loss:  1.4715194\n",
      "Epoch:  508 \t Loss:  1.4699659\n",
      "Epoch:  509 \t Loss:  1.4684165\n",
      "Epoch:  510 \t Loss:  1.4668719\n",
      "Epoch:  511 \t Loss:  1.4653347\n",
      "Epoch:  512 \t Loss:  1.463805\n",
      "Epoch:  513 \t Loss:  1.46228\n",
      "Epoch:  514 \t Loss:  1.4607604\n",
      "Epoch:  515 \t Loss:  1.4592454\n",
      "Epoch:  516 \t Loss:  1.4577388\n",
      "Epoch:  517 \t Loss:  1.4562356\n",
      "Epoch:  518 \t Loss:  1.4547391\n",
      "Epoch:  519 \t Loss:  1.4532459\n",
      "Epoch:  520 \t Loss:  1.4517596\n",
      "Epoch:  521 \t Loss:  1.45028\n",
      "Epoch:  522 \t Loss:  1.4488059\n",
      "Epoch:  523 \t Loss:  1.447334\n",
      "Epoch:  524 \t Loss:  1.4458718\n",
      "Epoch:  525 \t Loss:  1.444414\n",
      "Epoch:  526 \t Loss:  1.4429587\n",
      "Epoch:  527 \t Loss:  1.4415108\n",
      "Epoch:  528 \t Loss:  1.4400681\n",
      "Epoch:  529 \t Loss:  1.4386305\n",
      "Epoch:  530 \t Loss:  1.4371969\n",
      "Epoch:  531 \t Loss:  1.435771\n",
      "Epoch:  532 \t Loss:  1.4343504\n",
      "Epoch:  533 \t Loss:  1.4329319\n",
      "Epoch:  534 \t Loss:  1.4315197\n",
      "Epoch:  535 \t Loss:  1.4301124\n",
      "Epoch:  536 \t Loss:  1.4287094\n",
      "Epoch:  537 \t Loss:  1.4273156\n",
      "Epoch:  538 \t Loss:  1.4259229\n",
      "Epoch:  539 \t Loss:  1.4245362\n",
      "Epoch:  540 \t Loss:  1.4231533\n",
      "Epoch:  541 \t Loss:  1.4217763\n",
      "Epoch:  542 \t Loss:  1.420405\n",
      "Epoch:  543 \t Loss:  1.4190372\n",
      "Epoch:  544 \t Loss:  1.4176753\n",
      "Epoch:  545 \t Loss:  1.4163165\n",
      "Epoch:  546 \t Loss:  1.4149635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  547 \t Loss:  1.413615\n",
      "Epoch:  548 \t Loss:  1.4122732\n",
      "Epoch:  549 \t Loss:  1.4109328\n",
      "Epoch:  550 \t Loss:  1.4095985\n",
      "Epoch:  551 \t Loss:  1.4082694\n",
      "Epoch:  552 \t Loss:  1.4069443\n",
      "Epoch:  553 \t Loss:  1.4056244\n",
      "Epoch:  554 \t Loss:  1.4043076\n",
      "Epoch:  555 \t Loss:  1.4029967\n",
      "Epoch:  556 \t Loss:  1.4016901\n",
      "Epoch:  557 \t Loss:  1.4003869\n",
      "Epoch:  558 \t Loss:  1.3990893\n",
      "Epoch:  559 \t Loss:  1.3977948\n",
      "Epoch:  560 \t Loss:  1.3965051\n",
      "Epoch:  561 \t Loss:  1.395221\n",
      "Epoch:  562 \t Loss:  1.3939393\n",
      "Epoch:  563 \t Loss:  1.3926648\n",
      "Epoch:  564 \t Loss:  1.3913925\n",
      "Epoch:  565 \t Loss:  1.3901237\n",
      "Epoch:  566 \t Loss:  1.3888615\n",
      "Epoch:  567 \t Loss:  1.3876022\n",
      "Epoch:  568 \t Loss:  1.3863475\n",
      "Epoch:  569 \t Loss:  1.3850969\n",
      "Epoch:  570 \t Loss:  1.3838488\n",
      "Epoch:  571 \t Loss:  1.382607\n",
      "Epoch:  572 \t Loss:  1.3813694\n",
      "Epoch:  573 \t Loss:  1.3801337\n",
      "Epoch:  574 \t Loss:  1.3789034\n",
      "Epoch:  575 \t Loss:  1.3776772\n",
      "Epoch:  576 \t Loss:  1.3764561\n",
      "Epoch:  577 \t Loss:  1.3752369\n",
      "Epoch:  578 \t Loss:  1.3740228\n",
      "Epoch:  579 \t Loss:  1.3728124\n",
      "Epoch:  580 \t Loss:  1.371608\n",
      "Epoch:  581 \t Loss:  1.3704044\n",
      "Epoch:  582 \t Loss:  1.3692044\n",
      "Epoch:  583 \t Loss:  1.3680109\n",
      "Epoch:  584 \t Loss:  1.36682\n",
      "Epoch:  585 \t Loss:  1.3656328\n",
      "Epoch:  586 \t Loss:  1.36445\n",
      "Epoch:  587 \t Loss:  1.3632693\n",
      "Epoch:  588 \t Loss:  1.3620946\n",
      "Epoch:  589 \t Loss:  1.3609227\n",
      "Epoch:  590 \t Loss:  1.3597552\n",
      "Epoch:  591 \t Loss:  1.3585894\n",
      "Epoch:  592 \t Loss:  1.357429\n",
      "Epoch:  593 \t Loss:  1.3562715\n",
      "Epoch:  594 \t Loss:  1.3551183\n",
      "Epoch:  595 \t Loss:  1.3539691\n",
      "Epoch:  596 \t Loss:  1.352823\n",
      "Epoch:  597 \t Loss:  1.3516812\n",
      "Epoch:  598 \t Loss:  1.3505418\n",
      "Epoch:  599 \t Loss:  1.3494064\n",
      "Epoch:  600 \t Loss:  1.3482747\n",
      "Epoch:  601 \t Loss:  1.3471457\n",
      "Epoch:  602 \t Loss:  1.3460209\n",
      "Epoch:  603 \t Loss:  1.3448976\n",
      "Epoch:  604 \t Loss:  1.3437821\n",
      "Epoch:  605 \t Loss:  1.3426669\n",
      "Epoch:  606 \t Loss:  1.3415544\n",
      "Epoch:  607 \t Loss:  1.3404477\n",
      "Epoch:  608 \t Loss:  1.3393434\n",
      "Epoch:  609 \t Loss:  1.3382422\n",
      "Epoch:  610 \t Loss:  1.3371449\n",
      "Epoch:  611 \t Loss:  1.3360497\n",
      "Epoch:  612 \t Loss:  1.3349597\n",
      "Epoch:  613 \t Loss:  1.3338712\n",
      "Epoch:  614 \t Loss:  1.3327881\n",
      "Epoch:  615 \t Loss:  1.331707\n",
      "Epoch:  616 \t Loss:  1.3306289\n",
      "Epoch:  617 \t Loss:  1.3295535\n",
      "Epoch:  618 \t Loss:  1.3284831\n",
      "Epoch:  619 \t Loss:  1.3274138\n",
      "Epoch:  620 \t Loss:  1.3263485\n",
      "Epoch:  621 \t Loss:  1.3252872\n",
      "Epoch:  622 \t Loss:  1.3242294\n",
      "Epoch:  623 \t Loss:  1.3231744\n",
      "Epoch:  624 \t Loss:  1.3221223\n",
      "Epoch:  625 \t Loss:  1.3210725\n",
      "Epoch:  626 \t Loss:  1.320026\n",
      "Epoch:  627 \t Loss:  1.3189831\n",
      "Epoch:  628 \t Loss:  1.3179433\n",
      "Epoch:  629 \t Loss:  1.3169057\n",
      "Epoch:  630 \t Loss:  1.3158718\n",
      "Epoch:  631 \t Loss:  1.3148415\n",
      "Epoch:  632 \t Loss:  1.3138138\n",
      "Epoch:  633 \t Loss:  1.3127887\n",
      "Epoch:  634 \t Loss:  1.3117669\n",
      "Epoch:  635 \t Loss:  1.3107485\n",
      "Epoch:  636 \t Loss:  1.3097318\n",
      "Epoch:  637 \t Loss:  1.3087189\n",
      "Epoch:  638 \t Loss:  1.3077089\n",
      "Epoch:  639 \t Loss:  1.3067014\n",
      "Epoch:  640 \t Loss:  1.3056973\n",
      "Epoch:  641 \t Loss:  1.3046954\n",
      "Epoch:  642 \t Loss:  1.3036966\n",
      "Epoch:  643 \t Loss:  1.3027005\n",
      "Epoch:  644 \t Loss:  1.3017089\n",
      "Epoch:  645 \t Loss:  1.3007189\n",
      "Epoch:  646 \t Loss:  1.2997317\n",
      "Epoch:  647 \t Loss:  1.2987474\n",
      "Epoch:  648 \t Loss:  1.2977649\n",
      "Epoch:  649 \t Loss:  1.296786\n",
      "Epoch:  650 \t Loss:  1.2958107\n",
      "Epoch:  651 \t Loss:  1.2948369\n",
      "Epoch:  652 \t Loss:  1.2938648\n",
      "Epoch:  653 \t Loss:  1.2928982\n",
      "Epoch:  654 \t Loss:  1.291934\n",
      "Epoch:  655 \t Loss:  1.2909696\n",
      "Epoch:  656 \t Loss:  1.2900102\n",
      "Epoch:  657 \t Loss:  1.2890536\n",
      "Epoch:  658 \t Loss:  1.2880987\n",
      "Epoch:  659 \t Loss:  1.2871476\n",
      "Epoch:  660 \t Loss:  1.2861974\n",
      "Epoch:  661 \t Loss:  1.2852511\n",
      "Epoch:  662 \t Loss:  1.2843059\n",
      "Epoch:  663 \t Loss:  1.2833642\n",
      "Epoch:  664 \t Loss:  1.282424\n",
      "Epoch:  665 \t Loss:  1.2814875\n",
      "Epoch:  666 \t Loss:  1.2805555\n",
      "Epoch:  667 \t Loss:  1.2796229\n",
      "Epoch:  668 \t Loss:  1.2786944\n",
      "Epoch:  669 \t Loss:  1.2777692\n",
      "Epoch:  670 \t Loss:  1.2768453\n",
      "Epoch:  671 \t Loss:  1.2759237\n",
      "Epoch:  672 \t Loss:  1.2750045\n",
      "Epoch:  673 \t Loss:  1.2740878\n",
      "Epoch:  674 \t Loss:  1.2731731\n",
      "Epoch:  675 \t Loss:  1.2722614\n",
      "Epoch:  676 \t Loss:  1.2713535\n",
      "Epoch:  677 \t Loss:  1.2704464\n",
      "Epoch:  678 \t Loss:  1.2695436\n",
      "Epoch:  679 \t Loss:  1.2686406\n",
      "Epoch:  680 \t Loss:  1.2677417\n",
      "Epoch:  681 \t Loss:  1.2668462\n",
      "Epoch:  682 \t Loss:  1.2659495\n",
      "Epoch:  683 \t Loss:  1.2650586\n",
      "Epoch:  684 \t Loss:  1.2641684\n",
      "Epoch:  685 \t Loss:  1.2632805\n",
      "Epoch:  686 \t Loss:  1.2623942\n",
      "Epoch:  687 \t Loss:  1.2615114\n",
      "Epoch:  688 \t Loss:  1.2606324\n",
      "Epoch:  689 \t Loss:  1.2597525\n",
      "Epoch:  690 \t Loss:  1.2588751\n",
      "Epoch:  691 \t Loss:  1.2580028\n",
      "Epoch:  692 \t Loss:  1.2571317\n",
      "Epoch:  693 \t Loss:  1.2562613\n",
      "Epoch:  694 \t Loss:  1.2553941\n",
      "Epoch:  695 \t Loss:  1.2545291\n",
      "Epoch:  696 \t Loss:  1.2536663\n",
      "Epoch:  697 \t Loss:  1.2528067\n",
      "Epoch:  698 \t Loss:  1.2519497\n",
      "Epoch:  699 \t Loss:  1.251092\n",
      "Epoch:  700 \t Loss:  1.2502385\n",
      "Epoch:  701 \t Loss:  1.2493876\n",
      "Epoch:  702 \t Loss:  1.2485378\n",
      "Epoch:  703 \t Loss:  1.2476897\n",
      "Epoch:  704 \t Loss:  1.2468462\n",
      "Epoch:  705 \t Loss:  1.2460015\n",
      "Epoch:  706 \t Loss:  1.2451615\n",
      "Epoch:  707 \t Loss:  1.2443212\n",
      "Epoch:  708 \t Loss:  1.2434856\n",
      "Epoch:  709 \t Loss:  1.2426515\n",
      "Epoch:  710 \t Loss:  1.2418172\n",
      "Epoch:  711 \t Loss:  1.2409875\n",
      "Epoch:  712 \t Loss:  1.2401578\n",
      "Epoch:  713 \t Loss:  1.2393332\n",
      "Epoch:  714 \t Loss:  1.2385074\n",
      "Epoch:  715 \t Loss:  1.2376858\n",
      "Epoch:  716 \t Loss:  1.2368664\n",
      "Epoch:  717 \t Loss:  1.2360479\n",
      "Epoch:  718 \t Loss:  1.2352304\n",
      "Epoch:  719 \t Loss:  1.2344164\n",
      "Epoch:  720 \t Loss:  1.233605\n",
      "Epoch:  721 \t Loss:  1.2327954\n",
      "Epoch:  722 \t Loss:  1.2319868\n",
      "Epoch:  723 \t Loss:  1.2311802\n",
      "Epoch:  724 \t Loss:  1.2303766\n",
      "Epoch:  725 \t Loss:  1.2295731\n",
      "Epoch:  726 \t Loss:  1.2287732\n",
      "Epoch:  727 \t Loss:  1.2279739\n",
      "Epoch:  728 \t Loss:  1.2271781\n",
      "Epoch:  729 \t Loss:  1.2263826\n",
      "Epoch:  730 \t Loss:  1.225591\n",
      "Epoch:  731 \t Loss:  1.2248003\n",
      "Epoch:  732 \t Loss:  1.2240099\n",
      "Epoch:  733 \t Loss:  1.2232233\n",
      "Epoch:  734 \t Loss:  1.2224393\n",
      "Epoch:  735 \t Loss:  1.221656\n",
      "Epoch:  736 \t Loss:  1.2208742\n",
      "Epoch:  737 \t Loss:  1.2200947\n",
      "Epoch:  738 \t Loss:  1.2193168\n",
      "Epoch:  739 \t Loss:  1.2185411\n",
      "Epoch:  740 \t Loss:  1.2177668\n",
      "Epoch:  741 \t Loss:  1.216993\n",
      "Epoch:  742 \t Loss:  1.216223\n",
      "Epoch:  743 \t Loss:  1.2154548\n",
      "Epoch:  744 \t Loss:  1.2146875\n",
      "Epoch:  745 \t Loss:  1.2139229\n",
      "Epoch:  746 \t Loss:  1.2131597\n",
      "Epoch:  747 \t Loss:  1.212399\n",
      "Epoch:  748 \t Loss:  1.2116375\n",
      "Epoch:  749 \t Loss:  1.2108793\n",
      "Epoch:  750 \t Loss:  1.2101235\n",
      "Epoch:  751 \t Loss:  1.2093687\n",
      "Epoch:  752 \t Loss:  1.2086159\n",
      "Epoch:  753 \t Loss:  1.2078644\n",
      "Epoch:  754 \t Loss:  1.2071164\n",
      "Epoch:  755 \t Loss:  1.2063686\n",
      "Epoch:  756 \t Loss:  1.2056228\n",
      "Epoch:  757 \t Loss:  1.2048784\n",
      "Epoch:  758 \t Loss:  1.2041358\n",
      "Epoch:  759 \t Loss:  1.2033957\n",
      "Epoch:  760 \t Loss:  1.2026565\n",
      "Epoch:  761 \t Loss:  1.201919\n",
      "Epoch:  762 \t Loss:  1.2011833\n",
      "Epoch:  763 \t Loss:  1.2004489\n",
      "Epoch:  764 \t Loss:  1.199717\n",
      "Epoch:  765 \t Loss:  1.1989864\n",
      "Epoch:  766 \t Loss:  1.1982569\n",
      "Epoch:  767 \t Loss:  1.1975294\n",
      "Epoch:  768 \t Loss:  1.1968045\n",
      "Epoch:  769 \t Loss:  1.196079\n",
      "Epoch:  770 \t Loss:  1.1953562\n",
      "Epoch:  771 \t Loss:  1.1946357\n",
      "Epoch:  772 \t Loss:  1.1939164\n",
      "Epoch:  773 \t Loss:  1.193199\n",
      "Epoch:  774 \t Loss:  1.1924818\n",
      "Epoch:  775 \t Loss:  1.1917677\n",
      "Epoch:  776 \t Loss:  1.1910546\n",
      "Epoch:  777 \t Loss:  1.1903428\n",
      "Epoch:  778 \t Loss:  1.1896329\n",
      "Epoch:  779 \t Loss:  1.1889246\n",
      "Epoch:  780 \t Loss:  1.1882174\n",
      "Epoch:  781 \t Loss:  1.1875126\n",
      "Epoch:  782 \t Loss:  1.1868076\n",
      "Epoch:  783 \t Loss:  1.1861063\n",
      "Epoch:  784 \t Loss:  1.1854057\n",
      "Epoch:  785 \t Loss:  1.184707\n",
      "Epoch:  786 \t Loss:  1.184009\n",
      "Epoch:  787 \t Loss:  1.1833128\n",
      "Epoch:  788 \t Loss:  1.1826178\n",
      "Epoch:  789 \t Loss:  1.1819236\n",
      "Epoch:  790 \t Loss:  1.1812321\n",
      "Epoch:  791 \t Loss:  1.1805423\n",
      "Epoch:  792 \t Loss:  1.179853\n",
      "Epoch:  793 \t Loss:  1.1791675\n",
      "Epoch:  794 \t Loss:  1.1784794\n",
      "Epoch:  795 \t Loss:  1.177795\n",
      "Epoch:  796 \t Loss:  1.1771125\n",
      "Epoch:  797 \t Loss:  1.176431\n",
      "Epoch:  798 \t Loss:  1.1757516\n",
      "Epoch:  799 \t Loss:  1.1750735\n",
      "Epoch:  800 \t Loss:  1.1743954\n",
      "Epoch:  801 \t Loss:  1.1737199\n",
      "Epoch:  802 \t Loss:  1.1730467\n",
      "Epoch:  803 \t Loss:  1.1723725\n",
      "Epoch:  804 \t Loss:  1.1717011\n",
      "Epoch:  805 \t Loss:  1.1710318\n",
      "Epoch:  806 \t Loss:  1.1703625\n",
      "Epoch:  807 \t Loss:  1.1696956\n",
      "Epoch:  808 \t Loss:  1.1690289\n",
      "Epoch:  809 \t Loss:  1.1683635\n",
      "Epoch:  810 \t Loss:  1.1677009\n",
      "Epoch:  811 \t Loss:  1.1670406\n",
      "Epoch:  812 \t Loss:  1.1663791\n",
      "Epoch:  813 \t Loss:  1.1657201\n",
      "Epoch:  814 \t Loss:  1.1650618\n",
      "Epoch:  815 \t Loss:  1.1644049\n",
      "Epoch:  816 \t Loss:  1.1637506\n",
      "Epoch:  817 \t Loss:  1.1630971\n",
      "Epoch:  818 \t Loss:  1.1624447\n",
      "Epoch:  819 \t Loss:  1.1617938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  820 \t Loss:  1.1611433\n",
      "Epoch:  821 \t Loss:  1.1604959\n",
      "Epoch:  822 \t Loss:  1.159848\n",
      "Epoch:  823 \t Loss:  1.1592028\n",
      "Epoch:  824 \t Loss:  1.1585587\n",
      "Epoch:  825 \t Loss:  1.1579154\n",
      "Epoch:  826 \t Loss:  1.1572727\n",
      "Epoch:  827 \t Loss:  1.1566316\n",
      "Epoch:  828 \t Loss:  1.1559923\n",
      "Epoch:  829 \t Loss:  1.155355\n",
      "Epoch:  830 \t Loss:  1.1547183\n",
      "Epoch:  831 \t Loss:  1.1540827\n",
      "Epoch:  832 \t Loss:  1.1534482\n",
      "Epoch:  833 \t Loss:  1.1528139\n",
      "Epoch:  834 \t Loss:  1.1521829\n",
      "Epoch:  835 \t Loss:  1.1515532\n",
      "Epoch:  836 \t Loss:  1.1509225\n",
      "Epoch:  837 \t Loss:  1.1502937\n",
      "Epoch:  838 \t Loss:  1.1496667\n",
      "Epoch:  839 \t Loss:  1.1490414\n",
      "Epoch:  840 \t Loss:  1.1484178\n",
      "Epoch:  841 \t Loss:  1.1477951\n",
      "Epoch:  842 \t Loss:  1.1471725\n",
      "Epoch:  843 \t Loss:  1.1465516\n",
      "Epoch:  844 \t Loss:  1.1459328\n",
      "Epoch:  845 \t Loss:  1.145314\n",
      "Epoch:  846 \t Loss:  1.1446955\n",
      "Epoch:  847 \t Loss:  1.1440798\n",
      "Epoch:  848 \t Loss:  1.1434653\n",
      "Epoch:  849 \t Loss:  1.1428516\n",
      "Epoch:  850 \t Loss:  1.1422392\n",
      "Epoch:  851 \t Loss:  1.141628\n",
      "Epoch:  852 \t Loss:  1.1410191\n",
      "Epoch:  853 \t Loss:  1.1404108\n",
      "Epoch:  854 \t Loss:  1.1398023\n",
      "Epoch:  855 \t Loss:  1.1391964\n",
      "Epoch:  856 \t Loss:  1.138589\n",
      "Epoch:  857 \t Loss:  1.1379845\n",
      "Epoch:  858 \t Loss:  1.1373819\n",
      "Epoch:  859 \t Loss:  1.1367798\n",
      "Epoch:  860 \t Loss:  1.1361784\n",
      "Epoch:  861 \t Loss:  1.1355791\n",
      "Epoch:  862 \t Loss:  1.13498\n",
      "Epoch:  863 \t Loss:  1.1343825\n",
      "Epoch:  864 \t Loss:  1.1337857\n",
      "Epoch:  865 \t Loss:  1.1331908\n",
      "Epoch:  866 \t Loss:  1.1325969\n",
      "Epoch:  867 \t Loss:  1.132003\n",
      "Epoch:  868 \t Loss:  1.1314104\n",
      "Epoch:  869 \t Loss:  1.1308206\n",
      "Epoch:  870 \t Loss:  1.1302304\n",
      "Epoch:  871 \t Loss:  1.1296414\n",
      "Epoch:  872 \t Loss:  1.129054\n",
      "Epoch:  873 \t Loss:  1.1284679\n",
      "Epoch:  874 \t Loss:  1.1278825\n",
      "Epoch:  875 \t Loss:  1.1272979\n",
      "Epoch:  876 \t Loss:  1.1267134\n",
      "Epoch:  877 \t Loss:  1.126131\n",
      "Epoch:  878 \t Loss:  1.1255507\n",
      "Epoch:  879 \t Loss:  1.1249715\n",
      "Epoch:  880 \t Loss:  1.1243929\n",
      "Epoch:  881 \t Loss:  1.1238134\n",
      "Epoch:  882 \t Loss:  1.1232363\n",
      "Epoch:  883 \t Loss:  1.1226598\n",
      "Epoch:  884 \t Loss:  1.1220856\n",
      "Epoch:  885 \t Loss:  1.1215118\n",
      "Epoch:  886 \t Loss:  1.1209388\n",
      "Epoch:  887 \t Loss:  1.120366\n",
      "Epoch:  888 \t Loss:  1.1197964\n",
      "Epoch:  889 \t Loss:  1.1192267\n",
      "Epoch:  890 \t Loss:  1.1186563\n",
      "Epoch:  891 \t Loss:  1.118089\n",
      "Epoch:  892 \t Loss:  1.1175215\n",
      "Epoch:  893 \t Loss:  1.1169567\n",
      "Epoch:  894 \t Loss:  1.1163926\n",
      "Epoch:  895 \t Loss:  1.1158277\n",
      "Epoch:  896 \t Loss:  1.1152664\n",
      "Epoch:  897 \t Loss:  1.114704\n",
      "Epoch:  898 \t Loss:  1.1141427\n",
      "Epoch:  899 \t Loss:  1.1135814\n",
      "Epoch:  900 \t Loss:  1.1130229\n",
      "Epoch:  901 \t Loss:  1.112465\n",
      "Epoch:  902 \t Loss:  1.1119094\n",
      "Epoch:  903 \t Loss:  1.1113522\n",
      "Epoch:  904 \t Loss:  1.1107968\n",
      "Epoch:  905 \t Loss:  1.1102436\n",
      "Epoch:  906 \t Loss:  1.1096911\n",
      "Epoch:  907 \t Loss:  1.1091367\n",
      "Epoch:  908 \t Loss:  1.1085874\n",
      "Epoch:  909 \t Loss:  1.1080363\n",
      "Epoch:  910 \t Loss:  1.1074867\n",
      "Epoch:  911 \t Loss:  1.1069394\n",
      "Epoch:  912 \t Loss:  1.1063915\n",
      "Epoch:  913 \t Loss:  1.105845\n",
      "Epoch:  914 \t Loss:  1.1052996\n",
      "Epoch:  915 \t Loss:  1.1047553\n",
      "Epoch:  916 \t Loss:  1.1042111\n",
      "Epoch:  917 \t Loss:  1.1036687\n",
      "Epoch:  918 \t Loss:  1.1031269\n",
      "Epoch:  919 \t Loss:  1.1025858\n",
      "Epoch:  920 \t Loss:  1.1020457\n",
      "Epoch:  921 \t Loss:  1.1015061\n",
      "Epoch:  922 \t Loss:  1.100968\n",
      "Epoch:  923 \t Loss:  1.1004299\n",
      "Epoch:  924 \t Loss:  1.0998936\n",
      "Epoch:  925 \t Loss:  1.0993576\n",
      "Epoch:  926 \t Loss:  1.0988227\n",
      "Epoch:  927 \t Loss:  1.0982895\n",
      "Epoch:  928 \t Loss:  1.0977572\n",
      "Epoch:  929 \t Loss:  1.0972238\n",
      "Epoch:  930 \t Loss:  1.0966929\n",
      "Epoch:  931 \t Loss:  1.0961629\n",
      "Epoch:  932 \t Loss:  1.0956323\n",
      "Epoch:  933 \t Loss:  1.0951041\n",
      "Epoch:  934 \t Loss:  1.0945771\n",
      "Epoch:  935 \t Loss:  1.0940505\n",
      "Epoch:  936 \t Loss:  1.0935243\n",
      "Epoch:  937 \t Loss:  1.0929993\n",
      "Epoch:  938 \t Loss:  1.0924749\n",
      "Epoch:  939 \t Loss:  1.0919516\n",
      "Epoch:  940 \t Loss:  1.0914303\n",
      "Epoch:  941 \t Loss:  1.0909079\n",
      "Epoch:  942 \t Loss:  1.0903862\n",
      "Epoch:  943 \t Loss:  1.0898658\n",
      "Epoch:  944 \t Loss:  1.0893468\n",
      "Epoch:  945 \t Loss:  1.0888296\n",
      "Epoch:  946 \t Loss:  1.0883124\n",
      "Epoch:  947 \t Loss:  1.0877949\n",
      "Epoch:  948 \t Loss:  1.0872787\n",
      "Epoch:  949 \t Loss:  1.0867642\n",
      "Epoch:  950 \t Loss:  1.0862501\n",
      "Epoch:  951 \t Loss:  1.085736\n",
      "Epoch:  952 \t Loss:  1.0852249\n",
      "Epoch:  953 \t Loss:  1.0847124\n",
      "Epoch:  954 \t Loss:  1.0842007\n",
      "Epoch:  955 \t Loss:  1.083692\n",
      "Epoch:  956 \t Loss:  1.0831803\n",
      "Epoch:  957 \t Loss:  1.0826728\n",
      "Epoch:  958 \t Loss:  1.0821644\n",
      "Epoch:  959 \t Loss:  1.0816573\n",
      "Epoch:  960 \t Loss:  1.0811511\n",
      "Epoch:  961 \t Loss:  1.0806452\n",
      "Epoch:  962 \t Loss:  1.080141\n",
      "Epoch:  963 \t Loss:  1.0796379\n",
      "Epoch:  964 \t Loss:  1.0791341\n",
      "Epoch:  965 \t Loss:  1.0786318\n",
      "Epoch:  966 \t Loss:  1.0781313\n",
      "Epoch:  967 \t Loss:  1.0776308\n",
      "Epoch:  968 \t Loss:  1.0771303\n",
      "Epoch:  969 \t Loss:  1.0766308\n",
      "Epoch:  970 \t Loss:  1.0761328\n",
      "Epoch:  971 \t Loss:  1.075635\n",
      "Epoch:  972 \t Loss:  1.0751383\n",
      "Epoch:  973 \t Loss:  1.0746416\n",
      "Epoch:  974 \t Loss:  1.0741465\n",
      "Epoch:  975 \t Loss:  1.0736519\n",
      "Epoch:  976 \t Loss:  1.0731577\n",
      "Epoch:  977 \t Loss:  1.0726633\n",
      "Epoch:  978 \t Loss:  1.0721722\n",
      "Epoch:  979 \t Loss:  1.0716797\n",
      "Epoch:  980 \t Loss:  1.0711889\n",
      "Epoch:  981 \t Loss:  1.0706983\n",
      "Epoch:  982 \t Loss:  1.0702091\n",
      "Epoch:  983 \t Loss:  1.0697194\n",
      "Epoch:  984 \t Loss:  1.069232\n",
      "Epoch:  985 \t Loss:  1.0687449\n",
      "Epoch:  986 \t Loss:  1.0682595\n",
      "Epoch:  987 \t Loss:  1.0677731\n",
      "Epoch:  988 \t Loss:  1.0672871\n",
      "Epoch:  989 \t Loss:  1.0668042\n",
      "Epoch:  990 \t Loss:  1.0663207\n",
      "Epoch:  991 \t Loss:  1.0658375\n",
      "Epoch:  992 \t Loss:  1.0653548\n",
      "Epoch:  993 \t Loss:  1.0648735\n",
      "Epoch:  994 \t Loss:  1.0643921\n",
      "Epoch:  995 \t Loss:  1.0639123\n",
      "Epoch:  996 \t Loss:  1.0634333\n",
      "Epoch:  997 \t Loss:  1.0629537\n",
      "Epoch:  998 \t Loss:  1.0624758\n",
      "Epoch:  999 \t Loss:  1.0619986\n",
      "Epoch:  1000 \t Loss:  1.061522\n",
      "Epoch:  1001 \t Loss:  1.0610447\n",
      "Epoch:  1002 \t Loss:  1.0605706\n",
      "Epoch:  1003 \t Loss:  1.0600966\n",
      "Epoch:  1004 \t Loss:  1.0596216\n",
      "Epoch:  1005 \t Loss:  1.0591488\n",
      "Epoch:  1006 \t Loss:  1.0586758\n",
      "Epoch:  1007 \t Loss:  1.0582048\n",
      "Epoch:  1008 \t Loss:  1.0577341\n",
      "Epoch:  1009 \t Loss:  1.0572625\n",
      "Epoch:  1010 \t Loss:  1.056793\n",
      "Epoch:  1011 \t Loss:  1.0563244\n",
      "Epoch:  1012 \t Loss:  1.055856\n",
      "Epoch:  1013 \t Loss:  1.0553876\n",
      "Epoch:  1014 \t Loss:  1.0549201\n",
      "Epoch:  1015 \t Loss:  1.054454\n",
      "Epoch:  1016 \t Loss:  1.053988\n",
      "Epoch:  1017 \t Loss:  1.0535223\n",
      "Epoch:  1018 \t Loss:  1.0530576\n",
      "Epoch:  1019 \t Loss:  1.052594\n",
      "Epoch:  1020 \t Loss:  1.0521306\n",
      "Epoch:  1021 \t Loss:  1.0516688\n",
      "Epoch:  1022 \t Loss:  1.051206\n",
      "Epoch:  1023 \t Loss:  1.0507439\n",
      "Epoch:  1024 \t Loss:  1.0502845\n",
      "Epoch:  1025 \t Loss:  1.0498239\n",
      "Epoch:  1026 \t Loss:  1.0493641\n",
      "Epoch:  1027 \t Loss:  1.0489055\n",
      "Epoch:  1028 \t Loss:  1.0484464\n",
      "Epoch:  1029 \t Loss:  1.0479897\n",
      "Epoch:  1030 \t Loss:  1.0475326\n",
      "Epoch:  1031 \t Loss:  1.0470768\n",
      "Epoch:  1032 \t Loss:  1.0466206\n",
      "Epoch:  1033 \t Loss:  1.0461656\n",
      "Epoch:  1034 \t Loss:  1.0457115\n",
      "Epoch:  1035 \t Loss:  1.0452569\n",
      "Epoch:  1036 \t Loss:  1.0448049\n",
      "Epoch:  1037 \t Loss:  1.0443512\n",
      "Epoch:  1038 \t Loss:  1.0439\n",
      "Epoch:  1039 \t Loss:  1.0434482\n",
      "Epoch:  1040 \t Loss:  1.0429975\n",
      "Epoch:  1041 \t Loss:  1.0425469\n",
      "Epoch:  1042 \t Loss:  1.0420982\n",
      "Epoch:  1043 \t Loss:  1.0416476\n",
      "Epoch:  1044 \t Loss:  1.0411998\n",
      "Epoch:  1045 \t Loss:  1.0407519\n",
      "Epoch:  1046 \t Loss:  1.0403049\n",
      "Epoch:  1047 \t Loss:  1.0398585\n",
      "Epoch:  1048 \t Loss:  1.0394131\n",
      "Epoch:  1049 \t Loss:  1.0389676\n",
      "Epoch:  1050 \t Loss:  1.0385214\n",
      "Epoch:  1051 \t Loss:  1.0380774\n",
      "Epoch:  1052 \t Loss:  1.0376332\n",
      "Epoch:  1053 \t Loss:  1.0371902\n",
      "Epoch:  1054 \t Loss:  1.0367486\n",
      "Epoch:  1055 \t Loss:  1.0363064\n",
      "Epoch:  1056 \t Loss:  1.0358653\n",
      "Epoch:  1057 \t Loss:  1.0354244\n",
      "Epoch:  1058 \t Loss:  1.034985\n",
      "Epoch:  1059 \t Loss:  1.0345441\n",
      "Epoch:  1060 \t Loss:  1.0341058\n",
      "Epoch:  1061 \t Loss:  1.0336661\n",
      "Epoch:  1062 \t Loss:  1.0332291\n",
      "Epoch:  1063 \t Loss:  1.0327919\n",
      "Epoch:  1064 \t Loss:  1.0323546\n",
      "Epoch:  1065 \t Loss:  1.031919\n",
      "Epoch:  1066 \t Loss:  1.0314842\n",
      "Epoch:  1067 \t Loss:  1.0310479\n",
      "Epoch:  1068 \t Loss:  1.030614\n",
      "Epoch:  1069 \t Loss:  1.0301793\n",
      "Epoch:  1070 \t Loss:  1.0297465\n",
      "Epoch:  1071 \t Loss:  1.0293118\n",
      "Epoch:  1072 \t Loss:  1.0288801\n",
      "Epoch:  1073 \t Loss:  1.0284487\n",
      "Epoch:  1074 \t Loss:  1.0280164\n",
      "Epoch:  1075 \t Loss:  1.0275865\n",
      "Epoch:  1076 \t Loss:  1.027156\n",
      "Epoch:  1077 \t Loss:  1.0267266\n",
      "Epoch:  1078 \t Loss:  1.0262982\n",
      "Epoch:  1079 \t Loss:  1.0258685\n",
      "Epoch:  1080 \t Loss:  1.0254408\n",
      "Epoch:  1081 \t Loss:  1.0250139\n",
      "Epoch:  1082 \t Loss:  1.024586\n",
      "Epoch:  1083 \t Loss:  1.0241605\n",
      "Epoch:  1084 \t Loss:  1.0237339\n",
      "Epoch:  1085 \t Loss:  1.0233096\n",
      "Epoch:  1086 \t Loss:  1.0228841\n",
      "Epoch:  1087 \t Loss:  1.0224601\n",
      "Epoch:  1088 \t Loss:  1.0220354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1089 \t Loss:  1.0216119\n",
      "Epoch:  1090 \t Loss:  1.02119\n",
      "Epoch:  1091 \t Loss:  1.0207677\n",
      "Epoch:  1092 \t Loss:  1.0203458\n",
      "Epoch:  1093 \t Loss:  1.0199246\n",
      "Epoch:  1094 \t Loss:  1.0195031\n",
      "Epoch:  1095 \t Loss:  1.0190839\n",
      "Epoch:  1096 \t Loss:  1.0186641\n",
      "Epoch:  1097 \t Loss:  1.0182457\n",
      "Epoch:  1098 \t Loss:  1.0178267\n",
      "Epoch:  1099 \t Loss:  1.0174085\n",
      "Epoch:  1100 \t Loss:  1.0169909\n",
      "Epoch:  1101 \t Loss:  1.0165735\n",
      "Epoch:  1102 \t Loss:  1.0161572\n",
      "Epoch:  1103 \t Loss:  1.0157396\n",
      "Epoch:  1104 \t Loss:  1.0153245\n",
      "Epoch:  1105 \t Loss:  1.0149099\n",
      "Epoch:  1106 \t Loss:  1.0144948\n",
      "Epoch:  1107 \t Loss:  1.0140802\n",
      "Epoch:  1108 \t Loss:  1.0136656\n",
      "Epoch:  1109 \t Loss:  1.0132521\n",
      "Epoch:  1110 \t Loss:  1.0128411\n",
      "Epoch:  1111 \t Loss:  1.0124288\n",
      "Epoch:  1112 \t Loss:  1.012017\n",
      "Epoch:  1113 \t Loss:  1.0116048\n",
      "Epoch:  1114 \t Loss:  1.0111941\n",
      "Epoch:  1115 \t Loss:  1.0107838\n",
      "Epoch:  1116 \t Loss:  1.0103745\n",
      "Epoch:  1117 \t Loss:  1.0099648\n",
      "Epoch:  1118 \t Loss:  1.0095565\n",
      "Epoch:  1119 \t Loss:  1.0091472\n",
      "Epoch:  1120 \t Loss:  1.0087384\n",
      "Epoch:  1121 \t Loss:  1.0083314\n",
      "Epoch:  1122 \t Loss:  1.0079241\n",
      "Epoch:  1123 \t Loss:  1.0075182\n",
      "Epoch:  1124 \t Loss:  1.0071124\n",
      "Epoch:  1125 \t Loss:  1.0067065\n",
      "Epoch:  1126 \t Loss:  1.0063015\n",
      "Epoch:  1127 \t Loss:  1.0058973\n",
      "Epoch:  1128 \t Loss:  1.0054938\n",
      "Epoch:  1129 \t Loss:  1.0050892\n",
      "Epoch:  1130 \t Loss:  1.0046861\n",
      "Epoch:  1131 \t Loss:  1.004283\n",
      "Epoch:  1132 \t Loss:  1.003881\n",
      "Epoch:  1133 \t Loss:  1.0034795\n",
      "Epoch:  1134 \t Loss:  1.0030774\n",
      "Epoch:  1135 \t Loss:  1.0026768\n",
      "Epoch:  1136 \t Loss:  1.0022757\n",
      "Epoch:  1137 \t Loss:  1.0018761\n",
      "Epoch:  1138 \t Loss:  1.001477\n",
      "Epoch:  1139 \t Loss:  1.0010775\n",
      "Epoch:  1140 \t Loss:  1.0006785\n",
      "Epoch:  1141 \t Loss:  1.0002805\n",
      "Epoch:  1142 \t Loss:  0.99988174\n",
      "Epoch:  1143 \t Loss:  0.999484\n",
      "Epoch:  1144 \t Loss:  0.9990864\n",
      "Epoch:  1145 \t Loss:  0.99869126\n",
      "Epoch:  1146 \t Loss:  0.9982951\n",
      "Epoch:  1147 \t Loss:  0.99789923\n",
      "Epoch:  1148 \t Loss:  0.99750453\n",
      "Epoch:  1149 \t Loss:  0.9971097\n",
      "Epoch:  1150 \t Loss:  0.9967153\n",
      "Epoch:  1151 \t Loss:  0.99632186\n",
      "Epoch:  1152 \t Loss:  0.99592906\n",
      "Epoch:  1153 \t Loss:  0.995536\n",
      "Epoch:  1154 \t Loss:  0.9951428\n",
      "Epoch:  1155 \t Loss:  0.99474955\n",
      "Epoch:  1156 \t Loss:  0.9943579\n",
      "Epoch:  1157 \t Loss:  0.9939669\n",
      "Epoch:  1158 \t Loss:  0.9935772\n",
      "Epoch:  1159 \t Loss:  0.9931854\n",
      "Epoch:  1160 \t Loss:  0.99279594\n",
      "Epoch:  1161 \t Loss:  0.9924047\n",
      "Epoch:  1162 \t Loss:  0.99201554\n",
      "Epoch:  1163 \t Loss:  0.99162805\n",
      "Epoch:  1164 \t Loss:  0.9912391\n",
      "Epoch:  1165 \t Loss:  0.9908514\n",
      "Epoch:  1166 \t Loss:  0.9904641\n",
      "Epoch:  1167 \t Loss:  0.99007624\n",
      "Epoch:  1168 \t Loss:  0.9896898\n",
      "Epoch:  1169 \t Loss:  0.98930436\n",
      "Epoch:  1170 \t Loss:  0.9889178\n",
      "Epoch:  1171 \t Loss:  0.9885317\n",
      "Epoch:  1172 \t Loss:  0.9881464\n",
      "Epoch:  1173 \t Loss:  0.9877628\n",
      "Epoch:  1174 \t Loss:  0.98737806\n",
      "Epoch:  1175 \t Loss:  0.9869945\n",
      "Epoch:  1176 \t Loss:  0.98661107\n",
      "Epoch:  1177 \t Loss:  0.9862276\n",
      "Epoch:  1178 \t Loss:  0.9858456\n",
      "Epoch:  1179 \t Loss:  0.9854625\n",
      "Epoch:  1180 \t Loss:  0.985081\n",
      "Epoch:  1181 \t Loss:  0.98469967\n",
      "Epoch:  1182 \t Loss:  0.98431844\n",
      "Epoch:  1183 \t Loss:  0.9839386\n",
      "Epoch:  1184 \t Loss:  0.98355716\n",
      "Epoch:  1185 \t Loss:  0.9831775\n",
      "Epoch:  1186 \t Loss:  0.9827973\n",
      "Epoch:  1187 \t Loss:  0.98242015\n",
      "Epoch:  1188 \t Loss:  0.9820401\n",
      "Epoch:  1189 \t Loss:  0.9816622\n",
      "Epoch:  1190 \t Loss:  0.9812842\n",
      "Epoch:  1191 \t Loss:  0.98090655\n",
      "Epoch:  1192 \t Loss:  0.9805296\n",
      "Epoch:  1193 \t Loss:  0.9801525\n",
      "Epoch:  1194 \t Loss:  0.9797761\n",
      "Epoch:  1195 \t Loss:  0.9793999\n",
      "Epoch:  1196 \t Loss:  0.97902435\n",
      "Epoch:  1197 \t Loss:  0.97864956\n",
      "Epoch:  1198 \t Loss:  0.9782753\n",
      "Epoch:  1199 \t Loss:  0.9779007\n",
      "Epoch:  1200 \t Loss:  0.97752625\n",
      "Epoch:  1201 \t Loss:  0.9771528\n",
      "Epoch:  1202 \t Loss:  0.97677916\n",
      "Epoch:  1203 \t Loss:  0.97640604\n",
      "Epoch:  1204 \t Loss:  0.97603333\n",
      "Epoch:  1205 \t Loss:  0.97566104\n",
      "Epoch:  1206 \t Loss:  0.9752903\n",
      "Epoch:  1207 \t Loss:  0.97491765\n",
      "Epoch:  1208 \t Loss:  0.97454673\n",
      "Epoch:  1209 \t Loss:  0.97417545\n",
      "Epoch:  1210 \t Loss:  0.9738043\n",
      "Epoch:  1211 \t Loss:  0.97343564\n",
      "Epoch:  1212 \t Loss:  0.9730658\n",
      "Epoch:  1213 \t Loss:  0.97269624\n",
      "Epoch:  1214 \t Loss:  0.97232735\n",
      "Epoch:  1215 \t Loss:  0.97195923\n",
      "Epoch:  1216 \t Loss:  0.9715909\n",
      "Epoch:  1217 \t Loss:  0.9712218\n",
      "Epoch:  1218 \t Loss:  0.9708548\n",
      "Epoch:  1219 \t Loss:  0.97048783\n",
      "Epoch:  1220 \t Loss:  0.9701215\n",
      "Epoch:  1221 \t Loss:  0.9697545\n",
      "Epoch:  1222 \t Loss:  0.9693886\n",
      "Epoch:  1223 \t Loss:  0.9690236\n",
      "Epoch:  1224 \t Loss:  0.968658\n",
      "Epoch:  1225 \t Loss:  0.96829313\n",
      "Epoch:  1226 \t Loss:  0.9679283\n",
      "Epoch:  1227 \t Loss:  0.9675646\n",
      "Epoch:  1228 \t Loss:  0.9672\n",
      "Epoch:  1229 \t Loss:  0.9668359\n",
      "Epoch:  1230 \t Loss:  0.9664733\n",
      "Epoch:  1231 \t Loss:  0.9661094\n",
      "Epoch:  1232 \t Loss:  0.96574765\n",
      "Epoch:  1233 \t Loss:  0.96538526\n",
      "Epoch:  1234 \t Loss:  0.9650233\n",
      "Epoch:  1235 \t Loss:  0.96466315\n",
      "Epoch:  1236 \t Loss:  0.9643017\n",
      "Epoch:  1237 \t Loss:  0.9639398\n",
      "Epoch:  1238 \t Loss:  0.96357936\n",
      "Epoch:  1239 \t Loss:  0.9632192\n",
      "Epoch:  1240 \t Loss:  0.96285844\n",
      "Epoch:  1241 \t Loss:  0.9624991\n",
      "Epoch:  1242 \t Loss:  0.9621403\n",
      "Epoch:  1243 \t Loss:  0.9617824\n",
      "Epoch:  1244 \t Loss:  0.9614245\n",
      "Epoch:  1245 \t Loss:  0.96106607\n",
      "Epoch:  1246 \t Loss:  0.96070826\n",
      "Epoch:  1247 \t Loss:  0.96035075\n",
      "Epoch:  1248 \t Loss:  0.9599939\n",
      "Epoch:  1249 \t Loss:  0.9596359\n",
      "Epoch:  1250 \t Loss:  0.95928013\n",
      "Epoch:  1251 \t Loss:  0.9589235\n",
      "Epoch:  1252 \t Loss:  0.9585678\n",
      "Epoch:  1253 \t Loss:  0.958213\n",
      "Epoch:  1254 \t Loss:  0.9578579\n",
      "Epoch:  1255 \t Loss:  0.9575025\n",
      "Epoch:  1256 \t Loss:  0.95714784\n",
      "Epoch:  1257 \t Loss:  0.9567936\n",
      "Epoch:  1258 \t Loss:  0.9564403\n",
      "Epoch:  1259 \t Loss:  0.95608634\n",
      "Epoch:  1260 \t Loss:  0.9557332\n",
      "Epoch:  1261 \t Loss:  0.95538014\n",
      "Epoch:  1262 \t Loss:  0.9550283\n",
      "Epoch:  1263 \t Loss:  0.95467615\n",
      "Epoch:  1264 \t Loss:  0.9543236\n",
      "Epoch:  1265 \t Loss:  0.9539725\n",
      "Epoch:  1266 \t Loss:  0.9536213\n",
      "Epoch:  1267 \t Loss:  0.9532696\n",
      "Epoch:  1268 \t Loss:  0.952919\n",
      "Epoch:  1269 \t Loss:  0.95257\n",
      "Epoch:  1270 \t Loss:  0.95221907\n",
      "Epoch:  1271 \t Loss:  0.95186937\n",
      "Epoch:  1272 \t Loss:  0.9515199\n",
      "Epoch:  1273 \t Loss:  0.9511717\n",
      "Epoch:  1274 \t Loss:  0.95082206\n",
      "Epoch:  1275 \t Loss:  0.95047444\n",
      "Epoch:  1276 \t Loss:  0.9501271\n",
      "Epoch:  1277 \t Loss:  0.9497792\n",
      "Epoch:  1278 \t Loss:  0.94943124\n",
      "Epoch:  1279 \t Loss:  0.94908404\n",
      "Epoch:  1280 \t Loss:  0.94873667\n",
      "Epoch:  1281 \t Loss:  0.9483902\n",
      "Epoch:  1282 \t Loss:  0.94804513\n",
      "Epoch:  1283 \t Loss:  0.9476988\n",
      "Epoch:  1284 \t Loss:  0.9473523\n",
      "Epoch:  1285 \t Loss:  0.94700724\n",
      "Epoch:  1286 \t Loss:  0.9466619\n",
      "Epoch:  1287 \t Loss:  0.9463172\n",
      "Epoch:  1288 \t Loss:  0.9459732\n",
      "Epoch:  1289 \t Loss:  0.9456285\n",
      "Epoch:  1290 \t Loss:  0.94528437\n",
      "Epoch:  1291 \t Loss:  0.9449416\n",
      "Epoch:  1292 \t Loss:  0.944597\n",
      "Epoch:  1293 \t Loss:  0.9442559\n",
      "Epoch:  1294 \t Loss:  0.9439133\n",
      "Epoch:  1295 \t Loss:  0.9435701\n",
      "Epoch:  1296 \t Loss:  0.9432289\n",
      "Epoch:  1297 \t Loss:  0.9428857\n",
      "Epoch:  1298 \t Loss:  0.94254553\n",
      "Epoch:  1299 \t Loss:  0.9422036\n",
      "Epoch:  1300 \t Loss:  0.94186205\n",
      "Epoch:  1301 \t Loss:  0.94152164\n",
      "Epoch:  1302 \t Loss:  0.941182\n",
      "Epoch:  1303 \t Loss:  0.94084173\n",
      "Epoch:  1304 \t Loss:  0.94050163\n",
      "Epoch:  1305 \t Loss:  0.9401629\n",
      "Epoch:  1306 \t Loss:  0.93982315\n",
      "Epoch:  1307 \t Loss:  0.93948513\n",
      "Epoch:  1308 \t Loss:  0.9391476\n",
      "Epoch:  1309 \t Loss:  0.93880904\n",
      "Epoch:  1310 \t Loss:  0.93847024\n",
      "Epoch:  1311 \t Loss:  0.9381331\n",
      "Epoch:  1312 \t Loss:  0.93779546\n",
      "Epoch:  1313 \t Loss:  0.93745893\n",
      "Epoch:  1314 \t Loss:  0.9371218\n",
      "Epoch:  1315 \t Loss:  0.936786\n",
      "Epoch:  1316 \t Loss:  0.9364493\n",
      "Epoch:  1317 \t Loss:  0.93611276\n",
      "Epoch:  1318 \t Loss:  0.93577564\n",
      "Epoch:  1319 \t Loss:  0.9354411\n",
      "Epoch:  1320 \t Loss:  0.9351058\n",
      "Epoch:  1321 \t Loss:  0.9347705\n",
      "Epoch:  1322 \t Loss:  0.93443674\n",
      "Epoch:  1323 \t Loss:  0.9341032\n",
      "Epoch:  1324 \t Loss:  0.93376905\n",
      "Epoch:  1325 \t Loss:  0.93343514\n",
      "Epoch:  1326 \t Loss:  0.93310076\n",
      "Epoch:  1327 \t Loss:  0.93276924\n",
      "Epoch:  1328 \t Loss:  0.93243563\n",
      "Epoch:  1329 \t Loss:  0.932103\n",
      "Epoch:  1330 \t Loss:  0.9317707\n",
      "Epoch:  1331 \t Loss:  0.93143815\n",
      "Epoch:  1332 \t Loss:  0.93110687\n",
      "Epoch:  1333 \t Loss:  0.9307746\n",
      "Epoch:  1334 \t Loss:  0.9304436\n",
      "Epoch:  1335 \t Loss:  0.9301119\n",
      "Epoch:  1336 \t Loss:  0.92978185\n",
      "Epoch:  1337 \t Loss:  0.9294514\n",
      "Epoch:  1338 \t Loss:  0.9291216\n",
      "Epoch:  1339 \t Loss:  0.9287918\n",
      "Epoch:  1340 \t Loss:  0.9284627\n",
      "Epoch:  1341 \t Loss:  0.9281329\n",
      "Epoch:  1342 \t Loss:  0.92780405\n",
      "Epoch:  1343 \t Loss:  0.9274747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1344 \t Loss:  0.92714626\n",
      "Epoch:  1345 \t Loss:  0.92681813\n",
      "Epoch:  1346 \t Loss:  0.9264903\n",
      "Epoch:  1347 \t Loss:  0.9261625\n",
      "Epoch:  1348 \t Loss:  0.925835\n",
      "Epoch:  1349 \t Loss:  0.92550766\n",
      "Epoch:  1350 \t Loss:  0.92517966\n",
      "Epoch:  1351 \t Loss:  0.9248539\n",
      "Epoch:  1352 \t Loss:  0.92452735\n",
      "Epoch:  1353 \t Loss:  0.92420095\n",
      "Epoch:  1354 \t Loss:  0.9238766\n",
      "Epoch:  1355 \t Loss:  0.9235496\n",
      "Epoch:  1356 \t Loss:  0.9232244\n",
      "Epoch:  1357 \t Loss:  0.9228981\n",
      "Epoch:  1358 \t Loss:  0.92257476\n",
      "Epoch:  1359 \t Loss:  0.9222501\n",
      "Epoch:  1360 \t Loss:  0.9219256\n",
      "Epoch:  1361 \t Loss:  0.92160237\n",
      "Epoch:  1362 \t Loss:  0.92127764\n",
      "Epoch:  1363 \t Loss:  0.9209542\n",
      "Epoch:  1364 \t Loss:  0.9206311\n",
      "Epoch:  1365 \t Loss:  0.92030686\n",
      "Epoch:  1366 \t Loss:  0.919985\n",
      "Epoch:  1367 \t Loss:  0.9196622\n",
      "Epoch:  1368 \t Loss:  0.9193398\n",
      "Epoch:  1369 \t Loss:  0.919018\n",
      "Epoch:  1370 \t Loss:  0.9186959\n",
      "Epoch:  1371 \t Loss:  0.9183753\n",
      "Epoch:  1372 \t Loss:  0.91805345\n",
      "Epoch:  1373 \t Loss:  0.9177319\n",
      "Epoch:  1374 \t Loss:  0.9174121\n",
      "Epoch:  1375 \t Loss:  0.9170917\n",
      "Epoch:  1376 \t Loss:  0.91677034\n",
      "Epoch:  1377 \t Loss:  0.9164519\n",
      "Epoch:  1378 \t Loss:  0.9161306\n",
      "Epoch:  1379 \t Loss:  0.9158117\n",
      "Epoch:  1380 \t Loss:  0.91549194\n",
      "Epoch:  1381 \t Loss:  0.9151733\n",
      "Epoch:  1382 \t Loss:  0.9148536\n",
      "Epoch:  1383 \t Loss:  0.91453624\n",
      "Epoch:  1384 \t Loss:  0.9142173\n",
      "Epoch:  1385 \t Loss:  0.9139\n",
      "Epoch:  1386 \t Loss:  0.9135823\n",
      "Epoch:  1387 \t Loss:  0.9132644\n",
      "Epoch:  1388 \t Loss:  0.91294706\n",
      "Epoch:  1389 \t Loss:  0.9126305\n",
      "Epoch:  1390 \t Loss:  0.9123128\n",
      "Epoch:  1391 \t Loss:  0.91199607\n",
      "Epoch:  1392 \t Loss:  0.91167974\n",
      "Epoch:  1393 \t Loss:  0.91136444\n",
      "Epoch:  1394 \t Loss:  0.9110479\n",
      "Epoch:  1395 \t Loss:  0.9107323\n",
      "Epoch:  1396 \t Loss:  0.9104164\n",
      "Epoch:  1397 \t Loss:  0.9101029\n",
      "Epoch:  1398 \t Loss:  0.9097877\n",
      "Epoch:  1399 \t Loss:  0.9094725\n",
      "Epoch:  1400 \t Loss:  0.90915805\n",
      "Epoch:  1401 \t Loss:  0.9088448\n",
      "Epoch:  1402 \t Loss:  0.90853095\n",
      "Epoch:  1403 \t Loss:  0.90821767\n",
      "Epoch:  1404 \t Loss:  0.90790385\n",
      "Epoch:  1405 \t Loss:  0.9075903\n",
      "Epoch:  1406 \t Loss:  0.90727717\n",
      "Epoch:  1407 \t Loss:  0.90696573\n",
      "Epoch:  1408 \t Loss:  0.9066523\n",
      "Epoch:  1409 \t Loss:  0.9063386\n",
      "Epoch:  1410 \t Loss:  0.9060283\n",
      "Epoch:  1411 \t Loss:  0.9057169\n",
      "Epoch:  1412 \t Loss:  0.90540516\n",
      "Epoch:  1413 \t Loss:  0.90509343\n",
      "Epoch:  1414 \t Loss:  0.9047822\n",
      "Epoch:  1415 \t Loss:  0.90447205\n",
      "Epoch:  1416 \t Loss:  0.9041609\n",
      "Epoch:  1417 \t Loss:  0.9038508\n",
      "Epoch:  1418 \t Loss:  0.9035399\n",
      "Epoch:  1419 \t Loss:  0.90323126\n",
      "Epoch:  1420 \t Loss:  0.9029206\n",
      "Epoch:  1421 \t Loss:  0.9026105\n",
      "Epoch:  1422 \t Loss:  0.90230185\n",
      "Epoch:  1423 \t Loss:  0.90199167\n",
      "Epoch:  1424 \t Loss:  0.90168345\n",
      "Epoch:  1425 \t Loss:  0.9013757\n",
      "Epoch:  1426 \t Loss:  0.9010672\n",
      "Epoch:  1427 \t Loss:  0.90076065\n",
      "Epoch:  1428 \t Loss:  0.9004517\n",
      "Epoch:  1429 \t Loss:  0.9001436\n",
      "Epoch:  1430 \t Loss:  0.8998373\n",
      "Epoch:  1431 \t Loss:  0.8995305\n",
      "Epoch:  1432 \t Loss:  0.89922243\n",
      "Epoch:  1433 \t Loss:  0.8989162\n",
      "Epoch:  1434 \t Loss:  0.8986093\n",
      "Epoch:  1435 \t Loss:  0.8983025\n",
      "Epoch:  1436 \t Loss:  0.8979969\n",
      "Epoch:  1437 \t Loss:  0.8976908\n",
      "Epoch:  1438 \t Loss:  0.8973852\n",
      "Epoch:  1439 \t Loss:  0.89708006\n",
      "Epoch:  1440 \t Loss:  0.8967758\n",
      "Epoch:  1441 \t Loss:  0.8964693\n",
      "Epoch:  1442 \t Loss:  0.8961656\n",
      "Epoch:  1443 \t Loss:  0.89586085\n",
      "Epoch:  1444 \t Loss:  0.89555657\n",
      "Epoch:  1445 \t Loss:  0.8952532\n",
      "Epoch:  1446 \t Loss:  0.8949487\n",
      "Epoch:  1447 \t Loss:  0.89464587\n",
      "Epoch:  1448 \t Loss:  0.8943425\n",
      "Epoch:  1449 \t Loss:  0.89403903\n",
      "Epoch:  1450 \t Loss:  0.89373595\n",
      "Epoch:  1451 \t Loss:  0.89343375\n",
      "Epoch:  1452 \t Loss:  0.89313143\n",
      "Epoch:  1453 \t Loss:  0.8928303\n",
      "Epoch:  1454 \t Loss:  0.8925274\n",
      "Epoch:  1455 \t Loss:  0.892225\n",
      "Epoch:  1456 \t Loss:  0.8919237\n",
      "Epoch:  1457 \t Loss:  0.8916217\n",
      "Epoch:  1458 \t Loss:  0.8913204\n",
      "Epoch:  1459 \t Loss:  0.89102\n",
      "Epoch:  1460 \t Loss:  0.8907182\n",
      "Epoch:  1461 \t Loss:  0.8904193\n",
      "Epoch:  1462 \t Loss:  0.89011854\n",
      "Epoch:  1463 \t Loss:  0.88981795\n",
      "Epoch:  1464 \t Loss:  0.8895189\n",
      "Epoch:  1465 \t Loss:  0.88921845\n",
      "Epoch:  1466 \t Loss:  0.8889198\n",
      "Epoch:  1467 \t Loss:  0.88862085\n",
      "Epoch:  1468 \t Loss:  0.8883209\n",
      "Epoch:  1469 \t Loss:  0.888023\n",
      "Epoch:  1470 \t Loss:  0.88772464\n",
      "Epoch:  1471 \t Loss:  0.8874256\n",
      "Epoch:  1472 \t Loss:  0.88712764\n",
      "Epoch:  1473 \t Loss:  0.88682956\n",
      "Epoch:  1474 \t Loss:  0.8865314\n",
      "Epoch:  1475 \t Loss:  0.88623375\n",
      "Epoch:  1476 \t Loss:  0.8859362\n",
      "Epoch:  1477 \t Loss:  0.8856393\n",
      "Epoch:  1478 \t Loss:  0.8853426\n",
      "Epoch:  1479 \t Loss:  0.88504595\n",
      "Epoch:  1480 \t Loss:  0.88474905\n",
      "Epoch:  1481 \t Loss:  0.8844533\n",
      "Epoch:  1482 \t Loss:  0.8841564\n",
      "Epoch:  1483 \t Loss:  0.88386106\n",
      "Epoch:  1484 \t Loss:  0.8835657\n",
      "Epoch:  1485 \t Loss:  0.8832699\n",
      "Epoch:  1486 \t Loss:  0.88297576\n",
      "Epoch:  1487 \t Loss:  0.8826811\n",
      "Epoch:  1488 \t Loss:  0.88238627\n",
      "Epoch:  1489 \t Loss:  0.88209194\n",
      "Epoch:  1490 \t Loss:  0.8817978\n",
      "Epoch:  1491 \t Loss:  0.8815035\n",
      "Epoch:  1492 \t Loss:  0.88120955\n",
      "Epoch:  1493 \t Loss:  0.88091516\n",
      "Epoch:  1494 \t Loss:  0.8806225\n",
      "Epoch:  1495 \t Loss:  0.8803278\n",
      "Epoch:  1496 \t Loss:  0.880035\n",
      "Epoch:  1497 \t Loss:  0.8797429\n",
      "Epoch:  1498 \t Loss:  0.8794501\n",
      "Epoch:  1499 \t Loss:  0.87915623\n",
      "Epoch:  1500 \t Loss:  0.8788648\n",
      "Epoch:  1501 \t Loss:  0.8785734\n",
      "Epoch:  1502 \t Loss:  0.8782814\n",
      "Epoch:  1503 \t Loss:  0.87798995\n",
      "Epoch:  1504 \t Loss:  0.87769854\n",
      "Epoch:  1505 \t Loss:  0.87740576\n",
      "Epoch:  1506 \t Loss:  0.8771153\n",
      "Epoch:  1507 \t Loss:  0.87682515\n",
      "Epoch:  1508 \t Loss:  0.8765342\n",
      "Epoch:  1509 \t Loss:  0.8762432\n",
      "Epoch:  1510 \t Loss:  0.8759536\n",
      "Epoch:  1511 \t Loss:  0.8756627\n",
      "Epoch:  1512 \t Loss:  0.8753722\n",
      "Epoch:  1513 \t Loss:  0.87508345\n",
      "Epoch:  1514 \t Loss:  0.87479466\n",
      "Epoch:  1515 \t Loss:  0.8745041\n",
      "Epoch:  1516 \t Loss:  0.8742161\n",
      "Epoch:  1517 \t Loss:  0.87392783\n",
      "Epoch:  1518 \t Loss:  0.8736398\n",
      "Epoch:  1519 \t Loss:  0.8733511\n",
      "Epoch:  1520 \t Loss:  0.87306255\n",
      "Epoch:  1521 \t Loss:  0.87277436\n",
      "Epoch:  1522 \t Loss:  0.8724864\n",
      "Epoch:  1523 \t Loss:  0.87219906\n",
      "Epoch:  1524 \t Loss:  0.8719112\n",
      "Epoch:  1525 \t Loss:  0.8716247\n",
      "Epoch:  1526 \t Loss:  0.8713374\n",
      "Epoch:  1527 \t Loss:  0.87105095\n",
      "Epoch:  1528 \t Loss:  0.8707647\n",
      "Epoch:  1529 \t Loss:  0.87047726\n",
      "Epoch:  1530 \t Loss:  0.8701919\n",
      "Epoch:  1531 \t Loss:  0.8699048\n",
      "Epoch:  1532 \t Loss:  0.8696192\n",
      "Epoch:  1533 \t Loss:  0.86933315\n",
      "Epoch:  1534 \t Loss:  0.86904866\n",
      "Epoch:  1535 \t Loss:  0.8687632\n",
      "Epoch:  1536 \t Loss:  0.86847734\n",
      "Epoch:  1537 \t Loss:  0.86819315\n",
      "Epoch:  1538 \t Loss:  0.86790854\n",
      "Epoch:  1539 \t Loss:  0.86762416\n",
      "Epoch:  1540 \t Loss:  0.86733985\n",
      "Epoch:  1541 \t Loss:  0.8670564\n",
      "Epoch:  1542 \t Loss:  0.8667717\n",
      "Epoch:  1543 \t Loss:  0.8664877\n",
      "Epoch:  1544 \t Loss:  0.8662049\n",
      "Epoch:  1545 \t Loss:  0.8659216\n",
      "Epoch:  1546 \t Loss:  0.8656389\n",
      "Epoch:  1547 \t Loss:  0.8653555\n",
      "Epoch:  1548 \t Loss:  0.8650725\n",
      "Epoch:  1549 \t Loss:  0.8647908\n",
      "Epoch:  1550 \t Loss:  0.864508\n",
      "Epoch:  1551 \t Loss:  0.8642257\n",
      "Epoch:  1552 \t Loss:  0.86394405\n",
      "Epoch:  1553 \t Loss:  0.8636617\n",
      "Epoch:  1554 \t Loss:  0.8633806\n",
      "Epoch:  1555 \t Loss:  0.86310047\n",
      "Epoch:  1556 \t Loss:  0.8628176\n",
      "Epoch:  1557 \t Loss:  0.8625367\n",
      "Epoch:  1558 \t Loss:  0.86225545\n",
      "Epoch:  1559 \t Loss:  0.861975\n",
      "Epoch:  1560 \t Loss:  0.8616947\n",
      "Epoch:  1561 \t Loss:  0.8614153\n",
      "Epoch:  1562 \t Loss:  0.861136\n",
      "Epoch:  1563 \t Loss:  0.8608559\n",
      "Epoch:  1564 \t Loss:  0.8605766\n",
      "Epoch:  1565 \t Loss:  0.86029595\n",
      "Epoch:  1566 \t Loss:  0.86001736\n",
      "Epoch:  1567 \t Loss:  0.8597384\n",
      "Epoch:  1568 \t Loss:  0.8594598\n",
      "Epoch:  1569 \t Loss:  0.8591808\n",
      "Epoch:  1570 \t Loss:  0.8589028\n",
      "Epoch:  1571 \t Loss:  0.8586247\n",
      "Epoch:  1572 \t Loss:  0.8583464\n",
      "Epoch:  1573 \t Loss:  0.85806865\n",
      "Epoch:  1574 \t Loss:  0.8577902\n",
      "Epoch:  1575 \t Loss:  0.85751295\n",
      "Epoch:  1576 \t Loss:  0.85723585\n",
      "Epoch:  1577 \t Loss:  0.85695827\n",
      "Epoch:  1578 \t Loss:  0.8566811\n",
      "Epoch:  1579 \t Loss:  0.8564049\n",
      "Epoch:  1580 \t Loss:  0.8561278\n",
      "Epoch:  1581 \t Loss:  0.855852\n",
      "Epoch:  1582 \t Loss:  0.85557485\n",
      "Epoch:  1583 \t Loss:  0.8552983\n",
      "Epoch:  1584 \t Loss:  0.8550235\n",
      "Epoch:  1585 \t Loss:  0.85474634\n",
      "Epoch:  1586 \t Loss:  0.85447127\n",
      "Epoch:  1587 \t Loss:  0.8541961\n",
      "Epoch:  1588 \t Loss:  0.8539222\n",
      "Epoch:  1589 \t Loss:  0.8536469\n",
      "Epoch:  1590 \t Loss:  0.85337234\n",
      "Epoch:  1591 \t Loss:  0.8530975\n",
      "Epoch:  1592 \t Loss:  0.85282326\n",
      "Epoch:  1593 \t Loss:  0.8525489\n",
      "Epoch:  1594 \t Loss:  0.8522748\n",
      "Epoch:  1595 \t Loss:  0.8520017\n",
      "Epoch:  1596 \t Loss:  0.8517267\n",
      "Epoch:  1597 \t Loss:  0.8514533\n",
      "Epoch:  1598 \t Loss:  0.8511808\n",
      "Epoch:  1599 \t Loss:  0.85090804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1600 \t Loss:  0.85063463\n",
      "Epoch:  1601 \t Loss:  0.8503622\n",
      "Epoch:  1602 \t Loss:  0.8500894\n",
      "Epoch:  1603 \t Loss:  0.8498172\n",
      "Epoch:  1604 \t Loss:  0.849545\n",
      "Epoch:  1605 \t Loss:  0.84927315\n",
      "Epoch:  1606 \t Loss:  0.84900236\n",
      "Epoch:  1607 \t Loss:  0.8487292\n",
      "Epoch:  1608 \t Loss:  0.8484581\n",
      "Epoch:  1609 \t Loss:  0.8481872\n",
      "Epoch:  1610 \t Loss:  0.84791607\n",
      "Epoch:  1611 \t Loss:  0.84764564\n",
      "Epoch:  1612 \t Loss:  0.8473746\n",
      "Epoch:  1613 \t Loss:  0.8471046\n",
      "Epoch:  1614 \t Loss:  0.8468336\n",
      "Epoch:  1615 \t Loss:  0.84656453\n",
      "Epoch:  1616 \t Loss:  0.8462938\n",
      "Epoch:  1617 \t Loss:  0.84602445\n",
      "Epoch:  1618 \t Loss:  0.84575516\n",
      "Epoch:  1619 \t Loss:  0.84548604\n",
      "Epoch:  1620 \t Loss:  0.84521616\n",
      "Epoch:  1621 \t Loss:  0.84494805\n",
      "Epoch:  1622 \t Loss:  0.8446786\n",
      "Epoch:  1623 \t Loss:  0.84441054\n",
      "Epoch:  1624 \t Loss:  0.84414065\n",
      "Epoch:  1625 \t Loss:  0.8438732\n",
      "Epoch:  1626 \t Loss:  0.8436047\n",
      "Epoch:  1627 \t Loss:  0.84333616\n",
      "Epoch:  1628 \t Loss:  0.8430681\n",
      "Epoch:  1629 \t Loss:  0.8428008\n",
      "Epoch:  1630 \t Loss:  0.8425333\n",
      "Epoch:  1631 \t Loss:  0.84226686\n",
      "Epoch:  1632 \t Loss:  0.84199935\n",
      "Epoch:  1633 \t Loss:  0.8417321\n",
      "Epoch:  1634 \t Loss:  0.8414653\n",
      "Epoch:  1635 \t Loss:  0.8411992\n",
      "Epoch:  1636 \t Loss:  0.840932\n",
      "Epoch:  1637 \t Loss:  0.8406667\n",
      "Epoch:  1638 \t Loss:  0.84040034\n",
      "Epoch:  1639 \t Loss:  0.8401343\n",
      "Epoch:  1640 \t Loss:  0.8398692\n",
      "Epoch:  1641 \t Loss:  0.83960414\n",
      "Epoch:  1642 \t Loss:  0.83933765\n",
      "Epoch:  1643 \t Loss:  0.8390733\n",
      "Epoch:  1644 \t Loss:  0.83880836\n",
      "Epoch:  1645 \t Loss:  0.8385439\n",
      "Epoch:  1646 \t Loss:  0.8382781\n",
      "Epoch:  1647 \t Loss:  0.8380139\n",
      "Epoch:  1648 \t Loss:  0.8377509\n",
      "Epoch:  1649 \t Loss:  0.8374864\n",
      "Epoch:  1650 \t Loss:  0.83722204\n",
      "Epoch:  1651 \t Loss:  0.8369586\n",
      "Epoch:  1652 \t Loss:  0.83669525\n",
      "Epoch:  1653 \t Loss:  0.8364323\n",
      "Epoch:  1654 \t Loss:  0.83616877\n",
      "Epoch:  1655 \t Loss:  0.83590627\n",
      "Epoch:  1656 \t Loss:  0.8356436\n",
      "Epoch:  1657 \t Loss:  0.83538014\n",
      "Epoch:  1658 \t Loss:  0.8351188\n",
      "Epoch:  1659 \t Loss:  0.83485705\n",
      "Epoch:  1660 \t Loss:  0.8345943\n",
      "Epoch:  1661 \t Loss:  0.8343316\n",
      "Epoch:  1662 \t Loss:  0.83407\n",
      "Epoch:  1663 \t Loss:  0.83380735\n",
      "Epoch:  1664 \t Loss:  0.83354706\n",
      "Epoch:  1665 \t Loss:  0.8332859\n",
      "Epoch:  1666 \t Loss:  0.8330247\n",
      "Epoch:  1667 \t Loss:  0.8327639\n",
      "Epoch:  1668 \t Loss:  0.83250415\n",
      "Epoch:  1669 \t Loss:  0.8322422\n",
      "Epoch:  1670 \t Loss:  0.83198243\n",
      "Epoch:  1671 \t Loss:  0.83172256\n",
      "Epoch:  1672 \t Loss:  0.8314623\n",
      "Epoch:  1673 \t Loss:  0.83120257\n",
      "Epoch:  1674 \t Loss:  0.8309423\n",
      "Epoch:  1675 \t Loss:  0.83068264\n",
      "Epoch:  1676 \t Loss:  0.83042353\n",
      "Epoch:  1677 \t Loss:  0.8301648\n",
      "Epoch:  1678 \t Loss:  0.8299064\n",
      "Epoch:  1679 \t Loss:  0.82964736\n",
      "Epoch:  1680 \t Loss:  0.82938844\n",
      "Epoch:  1681 \t Loss:  0.8291302\n",
      "Epoch:  1682 \t Loss:  0.8288717\n",
      "Epoch:  1683 \t Loss:  0.8286135\n",
      "Epoch:  1684 \t Loss:  0.8283558\n",
      "Epoch:  1685 \t Loss:  0.8280975\n",
      "Epoch:  1686 \t Loss:  0.8278399\n",
      "Epoch:  1687 \t Loss:  0.8275818\n",
      "Epoch:  1688 \t Loss:  0.82732517\n",
      "Epoch:  1689 \t Loss:  0.827068\n",
      "Epoch:  1690 \t Loss:  0.8268113\n",
      "Epoch:  1691 \t Loss:  0.82655424\n",
      "Epoch:  1692 \t Loss:  0.8262983\n",
      "Epoch:  1693 \t Loss:  0.8260417\n",
      "Epoch:  1694 \t Loss:  0.82578474\n",
      "Epoch:  1695 \t Loss:  0.82552874\n",
      "Epoch:  1696 \t Loss:  0.82527477\n",
      "Epoch:  1697 \t Loss:  0.82501864\n",
      "Epoch:  1698 \t Loss:  0.8247618\n",
      "Epoch:  1699 \t Loss:  0.82450736\n",
      "Epoch:  1700 \t Loss:  0.8242512\n",
      "Epoch:  1701 \t Loss:  0.8239963\n",
      "Epoch:  1702 \t Loss:  0.82374203\n",
      "Epoch:  1703 \t Loss:  0.82348716\n",
      "Epoch:  1704 \t Loss:  0.8232334\n",
      "Epoch:  1705 \t Loss:  0.8229786\n",
      "Epoch:  1706 \t Loss:  0.8227242\n",
      "Epoch:  1707 \t Loss:  0.8224698\n",
      "Epoch:  1708 \t Loss:  0.82221603\n",
      "Epoch:  1709 \t Loss:  0.8219627\n",
      "Epoch:  1710 \t Loss:  0.82170904\n",
      "Epoch:  1711 \t Loss:  0.8214558\n",
      "Epoch:  1712 \t Loss:  0.82120174\n",
      "Epoch:  1713 \t Loss:  0.8209492\n",
      "Epoch:  1714 \t Loss:  0.8206964\n",
      "Epoch:  1715 \t Loss:  0.8204437\n",
      "Epoch:  1716 \t Loss:  0.82019186\n",
      "Epoch:  1717 \t Loss:  0.8199394\n",
      "Epoch:  1718 \t Loss:  0.81968796\n",
      "Epoch:  1719 \t Loss:  0.81943583\n",
      "Epoch:  1720 \t Loss:  0.8191839\n",
      "Epoch:  1721 \t Loss:  0.81893164\n",
      "Epoch:  1722 \t Loss:  0.8186809\n",
      "Epoch:  1723 \t Loss:  0.8184291\n",
      "Epoch:  1724 \t Loss:  0.8181783\n",
      "Epoch:  1725 \t Loss:  0.817927\n",
      "Epoch:  1726 \t Loss:  0.817676\n",
      "Epoch:  1727 \t Loss:  0.8174254\n",
      "Epoch:  1728 \t Loss:  0.8171753\n",
      "Epoch:  1729 \t Loss:  0.81692594\n",
      "Epoch:  1730 \t Loss:  0.8166743\n",
      "Epoch:  1731 \t Loss:  0.816425\n",
      "Epoch:  1732 \t Loss:  0.8161749\n",
      "Epoch:  1733 \t Loss:  0.815925\n",
      "Epoch:  1734 \t Loss:  0.8156757\n",
      "Epoch:  1735 \t Loss:  0.8154272\n",
      "Epoch:  1736 \t Loss:  0.8151773\n",
      "Epoch:  1737 \t Loss:  0.81492877\n",
      "Epoch:  1738 \t Loss:  0.8146793\n",
      "Epoch:  1739 \t Loss:  0.81443083\n",
      "Epoch:  1740 \t Loss:  0.8141818\n",
      "Epoch:  1741 \t Loss:  0.81393397\n",
      "Epoch:  1742 \t Loss:  0.8136853\n",
      "Epoch:  1743 \t Loss:  0.8134374\n",
      "Epoch:  1744 \t Loss:  0.81319064\n",
      "Epoch:  1745 \t Loss:  0.81294155\n",
      "Epoch:  1746 \t Loss:  0.81269467\n",
      "Epoch:  1747 \t Loss:  0.8124474\n",
      "Epoch:  1748 \t Loss:  0.81219983\n",
      "Epoch:  1749 \t Loss:  0.8119526\n",
      "Epoch:  1750 \t Loss:  0.8117059\n",
      "Epoch:  1751 \t Loss:  0.81145984\n",
      "Epoch:  1752 \t Loss:  0.81121224\n",
      "Epoch:  1753 \t Loss:  0.81096685\n",
      "Epoch:  1754 \t Loss:  0.81072044\n",
      "Epoch:  1755 \t Loss:  0.810475\n",
      "Epoch:  1756 \t Loss:  0.81022936\n",
      "Epoch:  1757 \t Loss:  0.80998284\n",
      "Epoch:  1758 \t Loss:  0.80973876\n",
      "Epoch:  1759 \t Loss:  0.80949265\n",
      "Epoch:  1760 \t Loss:  0.809247\n",
      "Epoch:  1761 \t Loss:  0.8090029\n",
      "Epoch:  1762 \t Loss:  0.8087572\n",
      "Epoch:  1763 \t Loss:  0.8085126\n",
      "Epoch:  1764 \t Loss:  0.80826765\n",
      "Epoch:  1765 \t Loss:  0.8080238\n",
      "Epoch:  1766 \t Loss:  0.80778015\n",
      "Epoch:  1767 \t Loss:  0.807536\n",
      "Epoch:  1768 \t Loss:  0.80729175\n",
      "Epoch:  1769 \t Loss:  0.80704767\n",
      "Epoch:  1770 \t Loss:  0.80680346\n",
      "Epoch:  1771 \t Loss:  0.8065616\n",
      "Epoch:  1772 \t Loss:  0.8063183\n",
      "Epoch:  1773 \t Loss:  0.80607474\n",
      "Epoch:  1774 \t Loss:  0.80583286\n",
      "Epoch:  1775 \t Loss:  0.8055889\n",
      "Epoch:  1776 \t Loss:  0.80534595\n",
      "Epoch:  1777 \t Loss:  0.8051044\n",
      "Epoch:  1778 \t Loss:  0.80486125\n",
      "Epoch:  1779 \t Loss:  0.80461967\n",
      "Epoch:  1780 \t Loss:  0.8043777\n",
      "Epoch:  1781 \t Loss:  0.8041356\n",
      "Epoch:  1782 \t Loss:  0.8038952\n",
      "Epoch:  1783 \t Loss:  0.8036538\n",
      "Epoch:  1784 \t Loss:  0.80341154\n",
      "Epoch:  1785 \t Loss:  0.80317044\n",
      "Epoch:  1786 \t Loss:  0.8029297\n",
      "Epoch:  1787 \t Loss:  0.8026891\n",
      "Epoch:  1788 \t Loss:  0.8024486\n",
      "Epoch:  1789 \t Loss:  0.8022073\n",
      "Epoch:  1790 \t Loss:  0.80196685\n",
      "Epoch:  1791 \t Loss:  0.80172795\n",
      "Epoch:  1792 \t Loss:  0.8014882\n",
      "Epoch:  1793 \t Loss:  0.80124736\n",
      "Epoch:  1794 \t Loss:  0.8010086\n",
      "Epoch:  1795 \t Loss:  0.80076844\n",
      "Epoch:  1796 \t Loss:  0.80053\n",
      "Epoch:  1797 \t Loss:  0.80029047\n",
      "Epoch:  1798 \t Loss:  0.8000506\n",
      "Epoch:  1799 \t Loss:  0.79981273\n",
      "Epoch:  1800 \t Loss:  0.7995736\n",
      "Epoch:  1801 \t Loss:  0.799335\n",
      "Epoch:  1802 \t Loss:  0.7990959\n",
      "Epoch:  1803 \t Loss:  0.7988583\n",
      "Epoch:  1804 \t Loss:  0.79862046\n",
      "Epoch:  1805 \t Loss:  0.79838234\n",
      "Epoch:  1806 \t Loss:  0.7981438\n",
      "Epoch:  1807 \t Loss:  0.7979069\n",
      "Epoch:  1808 \t Loss:  0.7976686\n",
      "Epoch:  1809 \t Loss:  0.7974322\n",
      "Epoch:  1810 \t Loss:  0.7971946\n",
      "Epoch:  1811 \t Loss:  0.7969577\n",
      "Epoch:  1812 \t Loss:  0.79672104\n",
      "Epoch:  1813 \t Loss:  0.79648453\n",
      "Epoch:  1814 \t Loss:  0.7962484\n",
      "Epoch:  1815 \t Loss:  0.79601157\n",
      "Epoch:  1816 \t Loss:  0.79577625\n",
      "Epoch:  1817 \t Loss:  0.7955391\n",
      "Epoch:  1818 \t Loss:  0.79530376\n",
      "Epoch:  1819 \t Loss:  0.79506767\n",
      "Epoch:  1820 \t Loss:  0.7948326\n",
      "Epoch:  1821 \t Loss:  0.79459655\n",
      "Epoch:  1822 \t Loss:  0.79436076\n",
      "Epoch:  1823 \t Loss:  0.7941258\n",
      "Epoch:  1824 \t Loss:  0.79389167\n",
      "Epoch:  1825 \t Loss:  0.79365623\n",
      "Epoch:  1826 \t Loss:  0.7934218\n",
      "Epoch:  1827 \t Loss:  0.7931864\n",
      "Epoch:  1828 \t Loss:  0.79295313\n",
      "Epoch:  1829 \t Loss:  0.79271966\n",
      "Epoch:  1830 \t Loss:  0.7924848\n",
      "Epoch:  1831 \t Loss:  0.79225045\n",
      "Epoch:  1832 \t Loss:  0.7920176\n",
      "Epoch:  1833 \t Loss:  0.7917839\n",
      "Epoch:  1834 \t Loss:  0.79155093\n",
      "Epoch:  1835 \t Loss:  0.7913169\n",
      "Epoch:  1836 \t Loss:  0.79108405\n",
      "Epoch:  1837 \t Loss:  0.7908519\n",
      "Epoch:  1838 \t Loss:  0.7906188\n",
      "Epoch:  1839 \t Loss:  0.7903857\n",
      "Epoch:  1840 \t Loss:  0.7901537\n",
      "Epoch:  1841 \t Loss:  0.7899218\n",
      "Epoch:  1842 \t Loss:  0.78968954\n",
      "Epoch:  1843 \t Loss:  0.7894583\n",
      "Epoch:  1844 \t Loss:  0.789225\n",
      "Epoch:  1845 \t Loss:  0.7889943\n",
      "Epoch:  1846 \t Loss:  0.78876287\n",
      "Epoch:  1847 \t Loss:  0.7885312\n",
      "Epoch:  1848 \t Loss:  0.78830063\n",
      "Epoch:  1849 \t Loss:  0.78806937\n",
      "Epoch:  1850 \t Loss:  0.78783876\n",
      "Epoch:  1851 \t Loss:  0.7876085\n",
      "Epoch:  1852 \t Loss:  0.7873769\n",
      "Epoch:  1853 \t Loss:  0.78714585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1854 \t Loss:  0.78691643\n",
      "Epoch:  1855 \t Loss:  0.7866861\n",
      "Epoch:  1856 \t Loss:  0.7864566\n",
      "Epoch:  1857 \t Loss:  0.78622705\n",
      "Epoch:  1858 \t Loss:  0.78599733\n",
      "Epoch:  1859 \t Loss:  0.78576857\n",
      "Epoch:  1860 \t Loss:  0.78553915\n",
      "Epoch:  1861 \t Loss:  0.78530985\n",
      "Epoch:  1862 \t Loss:  0.7850809\n",
      "Epoch:  1863 \t Loss:  0.78485173\n",
      "Epoch:  1864 \t Loss:  0.7846227\n",
      "Epoch:  1865 \t Loss:  0.78439337\n",
      "Epoch:  1866 \t Loss:  0.78416544\n",
      "Epoch:  1867 \t Loss:  0.78393704\n",
      "Epoch:  1868 \t Loss:  0.78370893\n",
      "Epoch:  1869 \t Loss:  0.7834815\n",
      "Epoch:  1870 \t Loss:  0.78325343\n",
      "Epoch:  1871 \t Loss:  0.78302515\n",
      "Epoch:  1872 \t Loss:  0.7827984\n",
      "Epoch:  1873 \t Loss:  0.7825708\n",
      "Epoch:  1874 \t Loss:  0.78234315\n",
      "Epoch:  1875 \t Loss:  0.7821167\n",
      "Epoch:  1876 \t Loss:  0.7818899\n",
      "Epoch:  1877 \t Loss:  0.7816628\n",
      "Epoch:  1878 \t Loss:  0.7814361\n",
      "Epoch:  1879 \t Loss:  0.78121054\n",
      "Epoch:  1880 \t Loss:  0.78098446\n",
      "Epoch:  1881 \t Loss:  0.7807587\n",
      "Epoch:  1882 \t Loss:  0.780532\n",
      "Epoch:  1883 \t Loss:  0.7803059\n",
      "Epoch:  1884 \t Loss:  0.7800805\n",
      "Epoch:  1885 \t Loss:  0.77985406\n",
      "Epoch:  1886 \t Loss:  0.779629\n",
      "Epoch:  1887 \t Loss:  0.77940357\n",
      "Epoch:  1888 \t Loss:  0.7791786\n",
      "Epoch:  1889 \t Loss:  0.7789544\n",
      "Epoch:  1890 \t Loss:  0.7787297\n",
      "Epoch:  1891 \t Loss:  0.77850467\n",
      "Epoch:  1892 \t Loss:  0.7782808\n",
      "Epoch:  1893 \t Loss:  0.77805656\n",
      "Epoch:  1894 \t Loss:  0.77783185\n",
      "Epoch:  1895 \t Loss:  0.77760756\n",
      "Epoch:  1896 \t Loss:  0.77738357\n",
      "Epoch:  1897 \t Loss:  0.77716017\n",
      "Epoch:  1898 \t Loss:  0.77693695\n",
      "Epoch:  1899 \t Loss:  0.77671283\n",
      "Epoch:  1900 \t Loss:  0.77649045\n",
      "Epoch:  1901 \t Loss:  0.7762669\n",
      "Epoch:  1902 \t Loss:  0.77604467\n",
      "Epoch:  1903 \t Loss:  0.7758205\n",
      "Epoch:  1904 \t Loss:  0.7755981\n",
      "Epoch:  1905 \t Loss:  0.7753765\n",
      "Epoch:  1906 \t Loss:  0.77515423\n",
      "Epoch:  1907 \t Loss:  0.7749302\n",
      "Epoch:  1908 \t Loss:  0.7747078\n",
      "Epoch:  1909 \t Loss:  0.7744875\n",
      "Epoch:  1910 \t Loss:  0.77426517\n",
      "Epoch:  1911 \t Loss:  0.7740425\n",
      "Epoch:  1912 \t Loss:  0.7738217\n",
      "Epoch:  1913 \t Loss:  0.77360094\n",
      "Epoch:  1914 \t Loss:  0.7733797\n",
      "Epoch:  1915 \t Loss:  0.7731585\n",
      "Epoch:  1916 \t Loss:  0.77293783\n",
      "Epoch:  1917 \t Loss:  0.7727168\n",
      "Epoch:  1918 \t Loss:  0.7724964\n",
      "Epoch:  1919 \t Loss:  0.7722761\n",
      "Epoch:  1920 \t Loss:  0.7720552\n",
      "Epoch:  1921 \t Loss:  0.7718362\n",
      "Epoch:  1922 \t Loss:  0.77161604\n",
      "Epoch:  1923 \t Loss:  0.7713956\n",
      "Epoch:  1924 \t Loss:  0.7711769\n",
      "Epoch:  1925 \t Loss:  0.7709567\n",
      "Epoch:  1926 \t Loss:  0.7707378\n",
      "Epoch:  1927 \t Loss:  0.7705183\n",
      "Epoch:  1928 \t Loss:  0.77029806\n",
      "Epoch:  1929 \t Loss:  0.77007985\n",
      "Epoch:  1930 \t Loss:  0.7698611\n",
      "Epoch:  1931 \t Loss:  0.7696427\n",
      "Epoch:  1932 \t Loss:  0.7694234\n",
      "Epoch:  1933 \t Loss:  0.76920563\n",
      "Epoch:  1934 \t Loss:  0.76898706\n",
      "Epoch:  1935 \t Loss:  0.76876915\n",
      "Epoch:  1936 \t Loss:  0.7685521\n",
      "Epoch:  1937 \t Loss:  0.76833314\n",
      "Epoch:  1938 \t Loss:  0.7681164\n",
      "Epoch:  1939 \t Loss:  0.7678986\n",
      "Epoch:  1940 \t Loss:  0.7676815\n",
      "Epoch:  1941 \t Loss:  0.767464\n",
      "Epoch:  1942 \t Loss:  0.76724666\n",
      "Epoch:  1943 \t Loss:  0.7670295\n",
      "Epoch:  1944 \t Loss:  0.7668127\n",
      "Epoch:  1945 \t Loss:  0.7665968\n",
      "Epoch:  1946 \t Loss:  0.7663789\n",
      "Epoch:  1947 \t Loss:  0.766163\n",
      "Epoch:  1948 \t Loss:  0.7659466\n",
      "Epoch:  1949 \t Loss:  0.76573104\n",
      "Epoch:  1950 \t Loss:  0.7655148\n",
      "Epoch:  1951 \t Loss:  0.76529884\n",
      "Epoch:  1952 \t Loss:  0.7650839\n",
      "Epoch:  1953 \t Loss:  0.76486796\n",
      "Epoch:  1954 \t Loss:  0.76465225\n",
      "Epoch:  1955 \t Loss:  0.7644378\n",
      "Epoch:  1956 \t Loss:  0.76422244\n",
      "Epoch:  1957 \t Loss:  0.76400805\n",
      "Epoch:  1958 \t Loss:  0.7637928\n",
      "Epoch:  1959 \t Loss:  0.7635783\n",
      "Epoch:  1960 \t Loss:  0.7633638\n",
      "Epoch:  1961 \t Loss:  0.76315\n",
      "Epoch:  1962 \t Loss:  0.7629349\n",
      "Epoch:  1963 \t Loss:  0.7627203\n",
      "Epoch:  1964 \t Loss:  0.76250666\n",
      "Epoch:  1965 \t Loss:  0.762293\n",
      "Epoch:  1966 \t Loss:  0.76207983\n",
      "Epoch:  1967 \t Loss:  0.7618659\n",
      "Epoch:  1968 \t Loss:  0.76165235\n",
      "Epoch:  1969 \t Loss:  0.76144046\n",
      "Epoch:  1970 \t Loss:  0.7612269\n",
      "Epoch:  1971 \t Loss:  0.76101315\n",
      "Epoch:  1972 \t Loss:  0.760801\n",
      "Epoch:  1973 \t Loss:  0.7605885\n",
      "Epoch:  1974 \t Loss:  0.76037544\n",
      "Epoch:  1975 \t Loss:  0.7601634\n",
      "Epoch:  1976 \t Loss:  0.7599514\n",
      "Epoch:  1977 \t Loss:  0.7597397\n",
      "Epoch:  1978 \t Loss:  0.7595272\n",
      "Epoch:  1979 \t Loss:  0.7593151\n",
      "Epoch:  1980 \t Loss:  0.7591036\n",
      "Epoch:  1981 \t Loss:  0.7588919\n",
      "Epoch:  1982 \t Loss:  0.75868124\n",
      "Epoch:  1983 \t Loss:  0.75847006\n",
      "Epoch:  1984 \t Loss:  0.75825906\n",
      "Epoch:  1985 \t Loss:  0.7580486\n",
      "Epoch:  1986 \t Loss:  0.7578366\n",
      "Epoch:  1987 \t Loss:  0.75762546\n",
      "Epoch:  1988 \t Loss:  0.75741565\n",
      "Epoch:  1989 \t Loss:  0.75720483\n",
      "Epoch:  1990 \t Loss:  0.75699425\n",
      "Epoch:  1991 \t Loss:  0.7567844\n",
      "Epoch:  1992 \t Loss:  0.75657517\n",
      "Epoch:  1993 \t Loss:  0.7563657\n",
      "Epoch:  1994 \t Loss:  0.75615513\n",
      "Epoch:  1995 \t Loss:  0.7559458\n",
      "Epoch:  1996 \t Loss:  0.75573725\n",
      "Epoch:  1997 \t Loss:  0.75552714\n",
      "Epoch:  1998 \t Loss:  0.75531846\n",
      "Epoch:  1999 \t Loss:  0.75510937\n",
      "Epoch:  2000 \t Loss:  0.7549019\n",
      "Epoch:  2001 \t Loss:  0.7546922\n",
      "Epoch:  2002 \t Loss:  0.7544831\n",
      "Epoch:  2003 \t Loss:  0.75427437\n",
      "Epoch:  2004 \t Loss:  0.75406575\n",
      "Epoch:  2005 \t Loss:  0.75385827\n",
      "Epoch:  2006 \t Loss:  0.7536503\n",
      "Epoch:  2007 \t Loss:  0.7534418\n",
      "Epoch:  2008 \t Loss:  0.75323445\n",
      "Epoch:  2009 \t Loss:  0.75302744\n",
      "Epoch:  2010 \t Loss:  0.7528198\n",
      "Epoch:  2011 \t Loss:  0.7526116\n",
      "Epoch:  2012 \t Loss:  0.7524047\n",
      "Epoch:  2013 \t Loss:  0.7521975\n",
      "Epoch:  2014 \t Loss:  0.7519917\n",
      "Epoch:  2015 \t Loss:  0.75178444\n",
      "Epoch:  2016 \t Loss:  0.75157815\n",
      "Epoch:  2017 \t Loss:  0.75137097\n",
      "Epoch:  2018 \t Loss:  0.75116456\n",
      "Epoch:  2019 \t Loss:  0.7509594\n",
      "Epoch:  2020 \t Loss:  0.75075376\n",
      "Epoch:  2021 \t Loss:  0.7505463\n",
      "Epoch:  2022 \t Loss:  0.75034124\n",
      "Epoch:  2023 \t Loss:  0.7501355\n",
      "Epoch:  2024 \t Loss:  0.74992955\n",
      "Epoch:  2025 \t Loss:  0.749724\n",
      "Epoch:  2026 \t Loss:  0.74951893\n",
      "Epoch:  2027 \t Loss:  0.7493138\n",
      "Epoch:  2028 \t Loss:  0.74910927\n",
      "Epoch:  2029 \t Loss:  0.74890393\n",
      "Epoch:  2030 \t Loss:  0.74869907\n",
      "Epoch:  2031 \t Loss:  0.7484947\n",
      "Epoch:  2032 \t Loss:  0.7482905\n",
      "Epoch:  2033 \t Loss:  0.7480858\n",
      "Epoch:  2034 \t Loss:  0.747882\n",
      "Epoch:  2035 \t Loss:  0.7476778\n",
      "Epoch:  2036 \t Loss:  0.7474742\n",
      "Epoch:  2037 \t Loss:  0.74727076\n",
      "Epoch:  2038 \t Loss:  0.74706674\n",
      "Epoch:  2039 \t Loss:  0.7468633\n",
      "Epoch:  2040 \t Loss:  0.74666\n",
      "Epoch:  2041 \t Loss:  0.7464563\n",
      "Epoch:  2042 \t Loss:  0.7462529\n",
      "Epoch:  2043 \t Loss:  0.7460505\n",
      "Epoch:  2044 \t Loss:  0.74584734\n",
      "Epoch:  2045 \t Loss:  0.7456444\n",
      "Epoch:  2046 \t Loss:  0.74544173\n",
      "Epoch:  2047 \t Loss:  0.7452396\n",
      "Epoch:  2048 \t Loss:  0.745037\n",
      "Epoch:  2049 \t Loss:  0.7448354\n",
      "Epoch:  2050 \t Loss:  0.74463296\n",
      "Epoch:  2051 \t Loss:  0.74443096\n",
      "Epoch:  2052 \t Loss:  0.74422914\n",
      "Epoch:  2053 \t Loss:  0.744028\n",
      "Epoch:  2054 \t Loss:  0.7438264\n",
      "Epoch:  2055 \t Loss:  0.743625\n",
      "Epoch:  2056 \t Loss:  0.7434239\n",
      "Epoch:  2057 \t Loss:  0.7432236\n",
      "Epoch:  2058 \t Loss:  0.74302185\n",
      "Epoch:  2059 \t Loss:  0.7428214\n",
      "Epoch:  2060 \t Loss:  0.7426203\n",
      "Epoch:  2061 \t Loss:  0.7424204\n",
      "Epoch:  2062 \t Loss:  0.7422195\n",
      "Epoch:  2063 \t Loss:  0.74201876\n",
      "Epoch:  2064 \t Loss:  0.7418186\n",
      "Epoch:  2065 \t Loss:  0.74161905\n",
      "Epoch:  2066 \t Loss:  0.7414191\n",
      "Epoch:  2067 \t Loss:  0.7412195\n",
      "Epoch:  2068 \t Loss:  0.74101955\n",
      "Epoch:  2069 \t Loss:  0.74081963\n",
      "Epoch:  2070 \t Loss:  0.74062055\n",
      "Epoch:  2071 \t Loss:  0.74042124\n",
      "Epoch:  2072 \t Loss:  0.74022186\n",
      "Epoch:  2073 \t Loss:  0.74002296\n",
      "Epoch:  2074 \t Loss:  0.73982406\n",
      "Epoch:  2075 \t Loss:  0.73962575\n",
      "Epoch:  2076 \t Loss:  0.7394272\n",
      "Epoch:  2077 \t Loss:  0.7392282\n",
      "Epoch:  2078 \t Loss:  0.73903096\n",
      "Epoch:  2079 \t Loss:  0.73883235\n",
      "Epoch:  2080 \t Loss:  0.7386335\n",
      "Epoch:  2081 \t Loss:  0.7384361\n",
      "Epoch:  2082 \t Loss:  0.7382379\n",
      "Epoch:  2083 \t Loss:  0.7380406\n",
      "Epoch:  2084 \t Loss:  0.7378433\n",
      "Epoch:  2085 \t Loss:  0.7376453\n",
      "Epoch:  2086 \t Loss:  0.7374486\n",
      "Epoch:  2087 \t Loss:  0.7372509\n",
      "Epoch:  2088 \t Loss:  0.73705405\n",
      "Epoch:  2089 \t Loss:  0.7368569\n",
      "Epoch:  2090 \t Loss:  0.7366606\n",
      "Epoch:  2091 \t Loss:  0.7364637\n",
      "Epoch:  2092 \t Loss:  0.73626685\n",
      "Epoch:  2093 \t Loss:  0.7360714\n",
      "Epoch:  2094 \t Loss:  0.7358747\n",
      "Epoch:  2095 \t Loss:  0.73567796\n",
      "Epoch:  2096 \t Loss:  0.73548234\n",
      "Epoch:  2097 \t Loss:  0.7352867\n",
      "Epoch:  2098 \t Loss:  0.7350903\n",
      "Epoch:  2099 \t Loss:  0.73489463\n",
      "Epoch:  2100 \t Loss:  0.7346994\n",
      "Epoch:  2101 \t Loss:  0.7345047\n",
      "Epoch:  2102 \t Loss:  0.7343089\n",
      "Epoch:  2103 \t Loss:  0.73411393\n",
      "Epoch:  2104 \t Loss:  0.7339186\n",
      "Epoch:  2105 \t Loss:  0.73372346\n",
      "Epoch:  2106 \t Loss:  0.7335289\n",
      "Epoch:  2107 \t Loss:  0.73333454\n",
      "Epoch:  2108 \t Loss:  0.7331397\n",
      "Epoch:  2109 \t Loss:  0.73294514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2110 \t Loss:  0.73275095\n",
      "Epoch:  2111 \t Loss:  0.7325569\n",
      "Epoch:  2112 \t Loss:  0.7323639\n",
      "Epoch:  2113 \t Loss:  0.7321687\n",
      "Epoch:  2114 \t Loss:  0.7319753\n",
      "Epoch:  2115 \t Loss:  0.7317827\n",
      "Epoch:  2116 \t Loss:  0.7315886\n",
      "Epoch:  2117 \t Loss:  0.7313955\n",
      "Epoch:  2118 \t Loss:  0.73120236\n",
      "Epoch:  2119 \t Loss:  0.73100936\n",
      "Epoch:  2120 \t Loss:  0.7308162\n",
      "Epoch:  2121 \t Loss:  0.73062295\n",
      "Epoch:  2122 \t Loss:  0.7304303\n",
      "Epoch:  2123 \t Loss:  0.73023766\n",
      "Epoch:  2124 \t Loss:  0.73004484\n",
      "Epoch:  2125 \t Loss:  0.72985244\n",
      "Epoch:  2126 \t Loss:  0.72966045\n",
      "Epoch:  2127 \t Loss:  0.7294681\n",
      "Epoch:  2128 \t Loss:  0.729277\n",
      "Epoch:  2129 \t Loss:  0.7290847\n",
      "Epoch:  2130 \t Loss:  0.72889304\n",
      "Epoch:  2131 \t Loss:  0.72870094\n",
      "Epoch:  2132 \t Loss:  0.72850937\n",
      "Epoch:  2133 \t Loss:  0.728318\n",
      "Epoch:  2134 \t Loss:  0.7281275\n",
      "Epoch:  2135 \t Loss:  0.7279356\n",
      "Epoch:  2136 \t Loss:  0.7277448\n",
      "Epoch:  2137 \t Loss:  0.72755456\n",
      "Epoch:  2138 \t Loss:  0.7273633\n",
      "Epoch:  2139 \t Loss:  0.7271729\n",
      "Epoch:  2140 \t Loss:  0.72698206\n",
      "Epoch:  2141 \t Loss:  0.7267923\n",
      "Epoch:  2142 \t Loss:  0.7266014\n",
      "Epoch:  2143 \t Loss:  0.7264114\n",
      "Epoch:  2144 \t Loss:  0.726222\n",
      "Epoch:  2145 \t Loss:  0.72603106\n",
      "Epoch:  2146 \t Loss:  0.7258417\n",
      "Epoch:  2147 \t Loss:  0.72565234\n",
      "Epoch:  2148 \t Loss:  0.7254625\n",
      "Epoch:  2149 \t Loss:  0.72527367\n",
      "Epoch:  2150 \t Loss:  0.72508466\n",
      "Epoch:  2151 \t Loss:  0.7248952\n",
      "Epoch:  2152 \t Loss:  0.72470564\n",
      "Epoch:  2153 \t Loss:  0.7245169\n",
      "Epoch:  2154 \t Loss:  0.724328\n",
      "Epoch:  2155 \t Loss:  0.72414047\n",
      "Epoch:  2156 \t Loss:  0.72395146\n",
      "Epoch:  2157 \t Loss:  0.72376263\n",
      "Epoch:  2158 \t Loss:  0.7235747\n",
      "Epoch:  2159 \t Loss:  0.72338545\n",
      "Epoch:  2160 \t Loss:  0.72319764\n",
      "Epoch:  2161 \t Loss:  0.7230098\n",
      "Epoch:  2162 \t Loss:  0.7228225\n",
      "Epoch:  2163 \t Loss:  0.72263455\n",
      "Epoch:  2164 \t Loss:  0.72244704\n",
      "Epoch:  2165 \t Loss:  0.7222596\n",
      "Epoch:  2166 \t Loss:  0.72207266\n",
      "Epoch:  2167 \t Loss:  0.72188526\n",
      "Epoch:  2168 \t Loss:  0.7216983\n",
      "Epoch:  2169 \t Loss:  0.72151047\n",
      "Epoch:  2170 \t Loss:  0.7213229\n",
      "Epoch:  2171 \t Loss:  0.72113717\n",
      "Epoch:  2172 \t Loss:  0.72095126\n",
      "Epoch:  2173 \t Loss:  0.7207645\n",
      "Epoch:  2174 \t Loss:  0.72057873\n",
      "Epoch:  2175 \t Loss:  0.72039086\n",
      "Epoch:  2176 \t Loss:  0.7202064\n",
      "Epoch:  2177 \t Loss:  0.7200198\n",
      "Epoch:  2178 \t Loss:  0.71983343\n",
      "Epoch:  2179 \t Loss:  0.7196486\n",
      "Epoch:  2180 \t Loss:  0.7194623\n",
      "Epoch:  2181 \t Loss:  0.7192763\n",
      "Epoch:  2182 \t Loss:  0.7190907\n",
      "Epoch:  2183 \t Loss:  0.71890485\n",
      "Epoch:  2184 \t Loss:  0.7187203\n",
      "Epoch:  2185 \t Loss:  0.718536\n",
      "Epoch:  2186 \t Loss:  0.71835065\n",
      "Epoch:  2187 \t Loss:  0.71816546\n",
      "Epoch:  2188 \t Loss:  0.7179813\n",
      "Epoch:  2189 \t Loss:  0.7177964\n",
      "Epoch:  2190 \t Loss:  0.71761155\n",
      "Epoch:  2191 \t Loss:  0.7174278\n",
      "Epoch:  2192 \t Loss:  0.71724296\n",
      "Epoch:  2193 \t Loss:  0.71705896\n",
      "Epoch:  2194 \t Loss:  0.71687526\n",
      "Epoch:  2195 \t Loss:  0.7166913\n",
      "Epoch:  2196 \t Loss:  0.7165075\n",
      "Epoch:  2197 \t Loss:  0.71632415\n",
      "Epoch:  2198 \t Loss:  0.7161406\n",
      "Epoch:  2199 \t Loss:  0.71595687\n",
      "Epoch:  2200 \t Loss:  0.7157739\n",
      "Epoch:  2201 \t Loss:  0.7155906\n",
      "Epoch:  2202 \t Loss:  0.7154078\n",
      "Epoch:  2203 \t Loss:  0.7152242\n",
      "Epoch:  2204 \t Loss:  0.71504134\n",
      "Epoch:  2205 \t Loss:  0.7148583\n",
      "Epoch:  2206 \t Loss:  0.71467656\n",
      "Epoch:  2207 \t Loss:  0.71449375\n",
      "Epoch:  2208 \t Loss:  0.71431047\n",
      "Epoch:  2209 \t Loss:  0.71412873\n",
      "Epoch:  2210 \t Loss:  0.7139464\n",
      "Epoch:  2211 \t Loss:  0.7137641\n",
      "Epoch:  2212 \t Loss:  0.71358216\n",
      "Epoch:  2213 \t Loss:  0.7134007\n",
      "Epoch:  2214 \t Loss:  0.713219\n",
      "Epoch:  2215 \t Loss:  0.7130375\n",
      "Epoch:  2216 \t Loss:  0.71285623\n",
      "Epoch:  2217 \t Loss:  0.7126749\n",
      "Epoch:  2218 \t Loss:  0.7124934\n",
      "Epoch:  2219 \t Loss:  0.7123123\n",
      "Epoch:  2220 \t Loss:  0.7121314\n",
      "Epoch:  2221 \t Loss:  0.7119503\n",
      "Epoch:  2222 \t Loss:  0.7117695\n",
      "Epoch:  2223 \t Loss:  0.711589\n",
      "Epoch:  2224 \t Loss:  0.71140885\n",
      "Epoch:  2225 \t Loss:  0.7112284\n",
      "Epoch:  2226 \t Loss:  0.71104854\n",
      "Epoch:  2227 \t Loss:  0.7108675\n",
      "Epoch:  2228 \t Loss:  0.71068716\n",
      "Epoch:  2229 \t Loss:  0.7105073\n",
      "Epoch:  2230 \t Loss:  0.7103275\n",
      "Epoch:  2231 \t Loss:  0.7101475\n",
      "Epoch:  2232 \t Loss:  0.7099682\n",
      "Epoch:  2233 \t Loss:  0.7097887\n",
      "Epoch:  2234 \t Loss:  0.70960927\n",
      "Epoch:  2235 \t Loss:  0.7094295\n",
      "Epoch:  2236 \t Loss:  0.70925015\n",
      "Epoch:  2237 \t Loss:  0.7090714\n",
      "Epoch:  2238 \t Loss:  0.7088925\n",
      "Epoch:  2239 \t Loss:  0.7087136\n",
      "Epoch:  2240 \t Loss:  0.70853454\n",
      "Epoch:  2241 \t Loss:  0.70835596\n",
      "Epoch:  2242 \t Loss:  0.7081777\n",
      "Epoch:  2243 \t Loss:  0.70799935\n",
      "Epoch:  2244 \t Loss:  0.7078215\n",
      "Epoch:  2245 \t Loss:  0.70764273\n",
      "Epoch:  2246 \t Loss:  0.707465\n",
      "Epoch:  2247 \t Loss:  0.7072861\n",
      "Epoch:  2248 \t Loss:  0.7071089\n",
      "Epoch:  2249 \t Loss:  0.70693076\n",
      "Epoch:  2250 \t Loss:  0.70675296\n",
      "Epoch:  2251 \t Loss:  0.70657545\n",
      "Epoch:  2252 \t Loss:  0.7063982\n",
      "Epoch:  2253 \t Loss:  0.7062211\n",
      "Epoch:  2254 \t Loss:  0.7060434\n",
      "Epoch:  2255 \t Loss:  0.7058664\n",
      "Epoch:  2256 \t Loss:  0.7056894\n",
      "Epoch:  2257 \t Loss:  0.7055125\n",
      "Epoch:  2258 \t Loss:  0.70533526\n",
      "Epoch:  2259 \t Loss:  0.7051593\n",
      "Epoch:  2260 \t Loss:  0.704983\n",
      "Epoch:  2261 \t Loss:  0.7048056\n",
      "Epoch:  2262 \t Loss:  0.70462966\n",
      "Epoch:  2263 \t Loss:  0.704453\n",
      "Epoch:  2264 \t Loss:  0.704277\n",
      "Epoch:  2265 \t Loss:  0.7041014\n",
      "Epoch:  2266 \t Loss:  0.703925\n",
      "Epoch:  2267 \t Loss:  0.7037497\n",
      "Epoch:  2268 \t Loss:  0.7035733\n",
      "Epoch:  2269 \t Loss:  0.703398\n",
      "Epoch:  2270 \t Loss:  0.70322233\n",
      "Epoch:  2271 \t Loss:  0.7030475\n",
      "Epoch:  2272 \t Loss:  0.7028725\n",
      "Epoch:  2273 \t Loss:  0.70269716\n",
      "Epoch:  2274 \t Loss:  0.702522\n",
      "Epoch:  2275 \t Loss:  0.70234704\n",
      "Epoch:  2276 \t Loss:  0.70217186\n",
      "Epoch:  2277 \t Loss:  0.70199704\n",
      "Epoch:  2278 \t Loss:  0.7018224\n",
      "Epoch:  2279 \t Loss:  0.7016488\n",
      "Epoch:  2280 \t Loss:  0.70147413\n",
      "Epoch:  2281 \t Loss:  0.70129937\n",
      "Epoch:  2282 \t Loss:  0.70112544\n",
      "Epoch:  2283 \t Loss:  0.7009511\n",
      "Epoch:  2284 \t Loss:  0.7007778\n",
      "Epoch:  2285 \t Loss:  0.7006034\n",
      "Epoch:  2286 \t Loss:  0.70042974\n",
      "Epoch:  2287 \t Loss:  0.70025617\n",
      "Epoch:  2288 \t Loss:  0.7000827\n",
      "Epoch:  2289 \t Loss:  0.6999092\n",
      "Epoch:  2290 \t Loss:  0.69973516\n",
      "Epoch:  2291 \t Loss:  0.6995622\n",
      "Epoch:  2292 \t Loss:  0.69938946\n",
      "Epoch:  2293 \t Loss:  0.69921654\n",
      "Epoch:  2294 \t Loss:  0.69904405\n",
      "Epoch:  2295 \t Loss:  0.69887143\n",
      "Epoch:  2296 \t Loss:  0.6986981\n",
      "Epoch:  2297 \t Loss:  0.69852555\n",
      "Epoch:  2298 \t Loss:  0.69835263\n",
      "Epoch:  2299 \t Loss:  0.69818085\n",
      "Epoch:  2300 \t Loss:  0.6980084\n",
      "Epoch:  2301 \t Loss:  0.6978369\n",
      "Epoch:  2302 \t Loss:  0.6976643\n",
      "Epoch:  2303 \t Loss:  0.69749194\n",
      "Epoch:  2304 \t Loss:  0.69732064\n",
      "Epoch:  2305 \t Loss:  0.69714904\n",
      "Epoch:  2306 \t Loss:  0.69697756\n",
      "Epoch:  2307 \t Loss:  0.6968061\n",
      "Epoch:  2308 \t Loss:  0.6966341\n",
      "Epoch:  2309 \t Loss:  0.69646275\n",
      "Epoch:  2310 \t Loss:  0.69629204\n",
      "Epoch:  2311 \t Loss:  0.69612086\n",
      "Epoch:  2312 \t Loss:  0.6959492\n",
      "Epoch:  2313 \t Loss:  0.6957785\n",
      "Epoch:  2314 \t Loss:  0.6956082\n",
      "Epoch:  2315 \t Loss:  0.69543844\n",
      "Epoch:  2316 \t Loss:  0.6952665\n",
      "Epoch:  2317 \t Loss:  0.69509655\n",
      "Epoch:  2318 \t Loss:  0.6949255\n",
      "Epoch:  2319 \t Loss:  0.6947551\n",
      "Epoch:  2320 \t Loss:  0.6945859\n",
      "Epoch:  2321 \t Loss:  0.69441456\n",
      "Epoch:  2322 \t Loss:  0.69424546\n",
      "Epoch:  2323 \t Loss:  0.6940751\n",
      "Epoch:  2324 \t Loss:  0.693905\n",
      "Epoch:  2325 \t Loss:  0.6937352\n",
      "Epoch:  2326 \t Loss:  0.6935663\n",
      "Epoch:  2327 \t Loss:  0.69339633\n",
      "Epoch:  2328 \t Loss:  0.6932274\n",
      "Epoch:  2329 \t Loss:  0.6930575\n",
      "Epoch:  2330 \t Loss:  0.6928889\n",
      "Epoch:  2331 \t Loss:  0.6927194\n",
      "Epoch:  2332 \t Loss:  0.6925508\n",
      "Epoch:  2333 \t Loss:  0.69238126\n",
      "Epoch:  2334 \t Loss:  0.6922136\n",
      "Epoch:  2335 \t Loss:  0.69204444\n",
      "Epoch:  2336 \t Loss:  0.69187564\n",
      "Epoch:  2337 \t Loss:  0.691707\n",
      "Epoch:  2338 \t Loss:  0.69153905\n",
      "Epoch:  2339 \t Loss:  0.6913705\n",
      "Epoch:  2340 \t Loss:  0.6912029\n",
      "Epoch:  2341 \t Loss:  0.69103396\n",
      "Epoch:  2342 \t Loss:  0.6908667\n",
      "Epoch:  2343 \t Loss:  0.69069904\n",
      "Epoch:  2344 \t Loss:  0.69053155\n",
      "Epoch:  2345 \t Loss:  0.6903633\n",
      "Epoch:  2346 \t Loss:  0.6901959\n",
      "Epoch:  2347 \t Loss:  0.69002795\n",
      "Epoch:  2348 \t Loss:  0.6898607\n",
      "Epoch:  2349 \t Loss:  0.68969375\n",
      "Epoch:  2350 \t Loss:  0.68952584\n",
      "Epoch:  2351 \t Loss:  0.6893583\n",
      "Epoch:  2352 \t Loss:  0.68919235\n",
      "Epoch:  2353 \t Loss:  0.6890249\n",
      "Epoch:  2354 \t Loss:  0.6888589\n",
      "Epoch:  2355 \t Loss:  0.68869203\n",
      "Epoch:  2356 \t Loss:  0.6885255\n",
      "Epoch:  2357 \t Loss:  0.6883592\n",
      "Epoch:  2358 \t Loss:  0.68819267\n",
      "Epoch:  2359 \t Loss:  0.68802553\n",
      "Epoch:  2360 \t Loss:  0.68785936\n",
      "Epoch:  2361 \t Loss:  0.6876938\n",
      "Epoch:  2362 \t Loss:  0.68752843\n",
      "Epoch:  2363 \t Loss:  0.687362\n",
      "Epoch:  2364 \t Loss:  0.68719673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2365 \t Loss:  0.6870311\n",
      "Epoch:  2366 \t Loss:  0.6868658\n",
      "Epoch:  2367 \t Loss:  0.6866995\n",
      "Epoch:  2368 \t Loss:  0.6865342\n",
      "Epoch:  2369 \t Loss:  0.6863688\n",
      "Epoch:  2370 \t Loss:  0.6862043\n",
      "Epoch:  2371 \t Loss:  0.6860391\n",
      "Epoch:  2372 \t Loss:  0.6858745\n",
      "Epoch:  2373 \t Loss:  0.6857086\n",
      "Epoch:  2374 \t Loss:  0.68554413\n",
      "Epoch:  2375 \t Loss:  0.68537974\n",
      "Epoch:  2376 \t Loss:  0.6852147\n",
      "Epoch:  2377 \t Loss:  0.6850506\n",
      "Epoch:  2378 \t Loss:  0.68488604\n",
      "Epoch:  2379 \t Loss:  0.6847219\n",
      "Epoch:  2380 \t Loss:  0.6845573\n",
      "Epoch:  2381 \t Loss:  0.6843932\n",
      "Epoch:  2382 \t Loss:  0.6842292\n",
      "Epoch:  2383 \t Loss:  0.6840657\n",
      "Epoch:  2384 \t Loss:  0.6839017\n",
      "Epoch:  2385 \t Loss:  0.6837381\n",
      "Epoch:  2386 \t Loss:  0.68357396\n",
      "Epoch:  2387 \t Loss:  0.6834103\n",
      "Epoch:  2388 \t Loss:  0.68324673\n",
      "Epoch:  2389 \t Loss:  0.6830838\n",
      "Epoch:  2390 \t Loss:  0.68292034\n",
      "Epoch:  2391 \t Loss:  0.6827575\n",
      "Epoch:  2392 \t Loss:  0.6825939\n",
      "Epoch:  2393 \t Loss:  0.6824312\n",
      "Epoch:  2394 \t Loss:  0.68226826\n",
      "Epoch:  2395 \t Loss:  0.6821055\n",
      "Epoch:  2396 \t Loss:  0.6819432\n",
      "Epoch:  2397 \t Loss:  0.68178034\n",
      "Epoch:  2398 \t Loss:  0.6816175\n",
      "Epoch:  2399 \t Loss:  0.6814554\n",
      "Epoch:  2400 \t Loss:  0.6812926\n",
      "Epoch:  2401 \t Loss:  0.68113047\n",
      "Epoch:  2402 \t Loss:  0.6809683\n",
      "Epoch:  2403 \t Loss:  0.6808061\n",
      "Epoch:  2404 \t Loss:  0.68064505\n",
      "Epoch:  2405 \t Loss:  0.6804821\n",
      "Epoch:  2406 \t Loss:  0.68032163\n",
      "Epoch:  2407 \t Loss:  0.6801595\n",
      "Epoch:  2408 \t Loss:  0.6799975\n",
      "Epoch:  2409 \t Loss:  0.6798365\n",
      "Epoch:  2410 \t Loss:  0.67967564\n",
      "Epoch:  2411 \t Loss:  0.67951334\n",
      "Epoch:  2412 \t Loss:  0.67935294\n",
      "Epoch:  2413 \t Loss:  0.6791917\n",
      "Epoch:  2414 \t Loss:  0.6790306\n",
      "Epoch:  2415 \t Loss:  0.67886937\n",
      "Epoch:  2416 \t Loss:  0.67870814\n",
      "Epoch:  2417 \t Loss:  0.6785481\n",
      "Epoch:  2418 \t Loss:  0.6783875\n",
      "Epoch:  2419 \t Loss:  0.6782269\n",
      "Epoch:  2420 \t Loss:  0.6780666\n",
      "Epoch:  2421 \t Loss:  0.6779059\n",
      "Epoch:  2422 \t Loss:  0.6777464\n",
      "Epoch:  2423 \t Loss:  0.6775853\n",
      "Epoch:  2424 \t Loss:  0.6774253\n",
      "Epoch:  2425 \t Loss:  0.67726487\n",
      "Epoch:  2426 \t Loss:  0.6771053\n",
      "Epoch:  2427 \t Loss:  0.6769458\n",
      "Epoch:  2428 \t Loss:  0.67678547\n",
      "Epoch:  2429 \t Loss:  0.67662656\n",
      "Epoch:  2430 \t Loss:  0.67646706\n",
      "Epoch:  2431 \t Loss:  0.6763078\n",
      "Epoch:  2432 \t Loss:  0.676148\n",
      "Epoch:  2433 \t Loss:  0.67598987\n",
      "Epoch:  2434 \t Loss:  0.6758295\n",
      "Epoch:  2435 \t Loss:  0.67567086\n",
      "Epoch:  2436 \t Loss:  0.6755114\n",
      "Epoch:  2437 \t Loss:  0.67535263\n",
      "Epoch:  2438 \t Loss:  0.6751937\n",
      "Epoch:  2439 \t Loss:  0.67503506\n",
      "Epoch:  2440 \t Loss:  0.6748761\n",
      "Epoch:  2441 \t Loss:  0.67471814\n",
      "Epoch:  2442 \t Loss:  0.67455983\n",
      "Epoch:  2443 \t Loss:  0.6744017\n",
      "Epoch:  2444 \t Loss:  0.67424345\n",
      "Epoch:  2445 \t Loss:  0.67408556\n",
      "Epoch:  2446 \t Loss:  0.6739266\n",
      "Epoch:  2447 \t Loss:  0.67376924\n",
      "Epoch:  2448 \t Loss:  0.6736111\n",
      "Epoch:  2449 \t Loss:  0.67345315\n",
      "Epoch:  2450 \t Loss:  0.6732956\n",
      "Epoch:  2451 \t Loss:  0.6731378\n",
      "Epoch:  2452 \t Loss:  0.6729803\n",
      "Epoch:  2453 \t Loss:  0.67282313\n",
      "Epoch:  2454 \t Loss:  0.6726657\n",
      "Epoch:  2455 \t Loss:  0.6725082\n",
      "Epoch:  2456 \t Loss:  0.67235124\n",
      "Epoch:  2457 \t Loss:  0.67219406\n",
      "Epoch:  2458 \t Loss:  0.6720361\n",
      "Epoch:  2459 \t Loss:  0.6718795\n",
      "Epoch:  2460 \t Loss:  0.6717233\n",
      "Epoch:  2461 \t Loss:  0.6715662\n",
      "Epoch:  2462 \t Loss:  0.67141014\n",
      "Epoch:  2463 \t Loss:  0.6712537\n",
      "Epoch:  2464 \t Loss:  0.6710965\n",
      "Epoch:  2465 \t Loss:  0.6709404\n",
      "Epoch:  2466 \t Loss:  0.6707835\n",
      "Epoch:  2467 \t Loss:  0.6706283\n",
      "Epoch:  2468 \t Loss:  0.6704713\n",
      "Epoch:  2469 \t Loss:  0.6703158\n",
      "Epoch:  2470 \t Loss:  0.6701595\n",
      "Epoch:  2471 \t Loss:  0.6700038\n",
      "Epoch:  2472 \t Loss:  0.66984814\n",
      "Epoch:  2473 \t Loss:  0.6696924\n",
      "Epoch:  2474 \t Loss:  0.6695366\n",
      "Epoch:  2475 \t Loss:  0.6693818\n",
      "Epoch:  2476 \t Loss:  0.66922635\n",
      "Epoch:  2477 \t Loss:  0.66907036\n",
      "Epoch:  2478 \t Loss:  0.66891515\n",
      "Epoch:  2479 \t Loss:  0.6687604\n",
      "Epoch:  2480 \t Loss:  0.6686053\n",
      "Epoch:  2481 \t Loss:  0.6684503\n",
      "Epoch:  2482 \t Loss:  0.6682952\n",
      "Epoch:  2483 \t Loss:  0.66814125\n",
      "Epoch:  2484 \t Loss:  0.6679864\n",
      "Epoch:  2485 \t Loss:  0.66783077\n",
      "Epoch:  2486 \t Loss:  0.66767657\n",
      "Epoch:  2487 \t Loss:  0.6675217\n",
      "Epoch:  2488 \t Loss:  0.66736764\n",
      "Epoch:  2489 \t Loss:  0.66721386\n",
      "Epoch:  2490 \t Loss:  0.66706\n",
      "Epoch:  2491 \t Loss:  0.666905\n",
      "Epoch:  2492 \t Loss:  0.6667513\n",
      "Epoch:  2493 \t Loss:  0.6665967\n",
      "Epoch:  2494 \t Loss:  0.6664431\n",
      "Epoch:  2495 \t Loss:  0.66628927\n",
      "Epoch:  2496 \t Loss:  0.6661355\n",
      "Epoch:  2497 \t Loss:  0.6659819\n",
      "Epoch:  2498 \t Loss:  0.66582847\n",
      "Epoch:  2499 \t Loss:  0.665675\n",
      "Epoch:  2500 \t Loss:  0.6655214\n",
      "Epoch:  2501 \t Loss:  0.665369\n",
      "Epoch:  2502 \t Loss:  0.6652159\n",
      "Epoch:  2503 \t Loss:  0.6650624\n",
      "Epoch:  2504 \t Loss:  0.6649093\n",
      "Epoch:  2505 \t Loss:  0.6647564\n",
      "Epoch:  2506 \t Loss:  0.66460353\n",
      "Epoch:  2507 \t Loss:  0.66445076\n",
      "Epoch:  2508 \t Loss:  0.6642983\n",
      "Epoch:  2509 \t Loss:  0.664146\n",
      "Epoch:  2510 \t Loss:  0.66399354\n",
      "Epoch:  2511 \t Loss:  0.6638412\n",
      "Epoch:  2512 \t Loss:  0.6636888\n",
      "Epoch:  2513 \t Loss:  0.66353595\n",
      "Epoch:  2514 \t Loss:  0.6633828\n",
      "Epoch:  2515 \t Loss:  0.663232\n",
      "Epoch:  2516 \t Loss:  0.6630797\n",
      "Epoch:  2517 \t Loss:  0.6629282\n",
      "Epoch:  2518 \t Loss:  0.6627759\n",
      "Epoch:  2519 \t Loss:  0.66262436\n",
      "Epoch:  2520 \t Loss:  0.6624726\n",
      "Epoch:  2521 \t Loss:  0.6623211\n",
      "Epoch:  2522 \t Loss:  0.6621693\n",
      "Epoch:  2523 \t Loss:  0.6620186\n",
      "Epoch:  2524 \t Loss:  0.661867\n",
      "Epoch:  2525 \t Loss:  0.66171604\n",
      "Epoch:  2526 \t Loss:  0.66156435\n",
      "Epoch:  2527 \t Loss:  0.6614132\n",
      "Epoch:  2528 \t Loss:  0.66126186\n",
      "Epoch:  2529 \t Loss:  0.6611115\n",
      "Epoch:  2530 \t Loss:  0.6609608\n",
      "Epoch:  2531 \t Loss:  0.6608097\n",
      "Epoch:  2532 \t Loss:  0.66065955\n",
      "Epoch:  2533 \t Loss:  0.66050875\n",
      "Epoch:  2534 \t Loss:  0.66035765\n",
      "Epoch:  2535 \t Loss:  0.6602075\n",
      "Epoch:  2536 \t Loss:  0.66005737\n",
      "Epoch:  2537 \t Loss:  0.6599064\n",
      "Epoch:  2538 \t Loss:  0.6597565\n",
      "Epoch:  2539 \t Loss:  0.6596072\n",
      "Epoch:  2540 \t Loss:  0.6594572\n",
      "Epoch:  2541 \t Loss:  0.659307\n",
      "Epoch:  2542 \t Loss:  0.6591568\n",
      "Epoch:  2543 \t Loss:  0.659007\n",
      "Epoch:  2544 \t Loss:  0.6588574\n",
      "Epoch:  2545 \t Loss:  0.6587073\n",
      "Epoch:  2546 \t Loss:  0.65855813\n",
      "Epoch:  2547 \t Loss:  0.6584088\n",
      "Epoch:  2548 \t Loss:  0.6582593\n",
      "Epoch:  2549 \t Loss:  0.65811056\n",
      "Epoch:  2550 \t Loss:  0.6579611\n",
      "Epoch:  2551 \t Loss:  0.65781164\n",
      "Epoch:  2552 \t Loss:  0.65766263\n",
      "Epoch:  2553 \t Loss:  0.6575132\n",
      "Epoch:  2554 \t Loss:  0.65736514\n",
      "Epoch:  2555 \t Loss:  0.6572167\n",
      "Epoch:  2556 \t Loss:  0.65706736\n",
      "Epoch:  2557 \t Loss:  0.6569191\n",
      "Epoch:  2558 \t Loss:  0.65677017\n",
      "Epoch:  2559 \t Loss:  0.65662163\n",
      "Epoch:  2560 \t Loss:  0.6564732\n",
      "Epoch:  2561 \t Loss:  0.65632546\n",
      "Epoch:  2562 \t Loss:  0.65617687\n",
      "Epoch:  2563 \t Loss:  0.65602875\n",
      "Epoch:  2564 \t Loss:  0.6558808\n",
      "Epoch:  2565 \t Loss:  0.6557322\n",
      "Epoch:  2566 \t Loss:  0.65558505\n",
      "Epoch:  2567 \t Loss:  0.6554367\n",
      "Epoch:  2568 \t Loss:  0.6552889\n",
      "Epoch:  2569 \t Loss:  0.6551408\n",
      "Epoch:  2570 \t Loss:  0.6549938\n",
      "Epoch:  2571 \t Loss:  0.6548463\n",
      "Epoch:  2572 \t Loss:  0.6546992\n",
      "Epoch:  2573 \t Loss:  0.6545518\n",
      "Epoch:  2574 \t Loss:  0.6544042\n",
      "Epoch:  2575 \t Loss:  0.6542568\n",
      "Epoch:  2576 \t Loss:  0.6541095\n",
      "Epoch:  2577 \t Loss:  0.65396225\n",
      "Epoch:  2578 \t Loss:  0.6538159\n",
      "Epoch:  2579 \t Loss:  0.6536685\n",
      "Epoch:  2580 \t Loss:  0.65352213\n",
      "Epoch:  2581 \t Loss:  0.6533754\n",
      "Epoch:  2582 \t Loss:  0.6532282\n",
      "Epoch:  2583 \t Loss:  0.65308166\n",
      "Epoch:  2584 \t Loss:  0.65293545\n",
      "Epoch:  2585 \t Loss:  0.65278906\n",
      "Epoch:  2586 \t Loss:  0.6526423\n",
      "Epoch:  2587 \t Loss:  0.65249646\n",
      "Epoch:  2588 \t Loss:  0.65235\n",
      "Epoch:  2589 \t Loss:  0.652204\n",
      "Epoch:  2590 \t Loss:  0.6520584\n",
      "Epoch:  2591 \t Loss:  0.65191233\n",
      "Epoch:  2592 \t Loss:  0.6517655\n",
      "Epoch:  2593 \t Loss:  0.65162086\n",
      "Epoch:  2594 \t Loss:  0.6514747\n",
      "Epoch:  2595 \t Loss:  0.6513293\n",
      "Epoch:  2596 \t Loss:  0.6511831\n",
      "Epoch:  2597 \t Loss:  0.6510373\n",
      "Epoch:  2598 \t Loss:  0.6508919\n",
      "Epoch:  2599 \t Loss:  0.65074635\n",
      "Epoch:  2600 \t Loss:  0.6506016\n",
      "Epoch:  2601 \t Loss:  0.65045726\n",
      "Epoch:  2602 \t Loss:  0.6503112\n",
      "Epoch:  2603 \t Loss:  0.6501664\n",
      "Epoch:  2604 \t Loss:  0.6500217\n",
      "Epoch:  2605 \t Loss:  0.6498767\n",
      "Epoch:  2606 \t Loss:  0.6497317\n",
      "Epoch:  2607 \t Loss:  0.6495873\n",
      "Epoch:  2608 \t Loss:  0.6494429\n",
      "Epoch:  2609 \t Loss:  0.64929795\n",
      "Epoch:  2610 \t Loss:  0.6491537\n",
      "Epoch:  2611 \t Loss:  0.6490093\n",
      "Epoch:  2612 \t Loss:  0.6488646\n",
      "Epoch:  2613 \t Loss:  0.64872044\n",
      "Epoch:  2614 \t Loss:  0.64857596\n",
      "Epoch:  2615 \t Loss:  0.6484322\n",
      "Epoch:  2616 \t Loss:  0.6482876\n",
      "Epoch:  2617 \t Loss:  0.64814365\n",
      "Epoch:  2618 \t Loss:  0.64799947\n",
      "Epoch:  2619 \t Loss:  0.6478555\n",
      "Epoch:  2620 \t Loss:  0.6477124\n",
      "Epoch:  2621 \t Loss:  0.64756864\n",
      "Epoch:  2622 \t Loss:  0.64742565\n",
      "Epoch:  2623 \t Loss:  0.64728206\n",
      "Epoch:  2624 \t Loss:  0.6471383\n",
      "Epoch:  2625 \t Loss:  0.6469951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2626 \t Loss:  0.6468517\n",
      "Epoch:  2627 \t Loss:  0.6467086\n",
      "Epoch:  2628 \t Loss:  0.64656526\n",
      "Epoch:  2629 \t Loss:  0.6464217\n",
      "Epoch:  2630 \t Loss:  0.64627844\n",
      "Epoch:  2631 \t Loss:  0.64613616\n",
      "Epoch:  2632 \t Loss:  0.64599323\n",
      "Epoch:  2633 \t Loss:  0.6458505\n",
      "Epoch:  2634 \t Loss:  0.64570755\n",
      "Epoch:  2635 \t Loss:  0.645565\n",
      "Epoch:  2636 \t Loss:  0.6454224\n",
      "Epoch:  2637 \t Loss:  0.64528036\n",
      "Epoch:  2638 \t Loss:  0.64513767\n",
      "Epoch:  2639 \t Loss:  0.6449959\n",
      "Epoch:  2640 \t Loss:  0.644853\n",
      "Epoch:  2641 \t Loss:  0.6447113\n",
      "Epoch:  2642 \t Loss:  0.64456904\n",
      "Epoch:  2643 \t Loss:  0.64442664\n",
      "Epoch:  2644 \t Loss:  0.644284\n",
      "Epoch:  2645 \t Loss:  0.64414275\n",
      "Epoch:  2646 \t Loss:  0.6440008\n",
      "Epoch:  2647 \t Loss:  0.6438595\n",
      "Epoch:  2648 \t Loss:  0.64371765\n",
      "Epoch:  2649 \t Loss:  0.6435765\n",
      "Epoch:  2650 \t Loss:  0.64343435\n",
      "Epoch:  2651 \t Loss:  0.643293\n",
      "Epoch:  2652 \t Loss:  0.64315253\n",
      "Epoch:  2653 \t Loss:  0.64301103\n",
      "Epoch:  2654 \t Loss:  0.64286935\n",
      "Epoch:  2655 \t Loss:  0.6427277\n",
      "Epoch:  2656 \t Loss:  0.6425866\n",
      "Epoch:  2657 \t Loss:  0.64244556\n",
      "Epoch:  2658 \t Loss:  0.6423049\n",
      "Epoch:  2659 \t Loss:  0.6421642\n",
      "Epoch:  2660 \t Loss:  0.6420228\n",
      "Epoch:  2661 \t Loss:  0.64188236\n",
      "Epoch:  2662 \t Loss:  0.64174175\n",
      "Epoch:  2663 \t Loss:  0.64160186\n",
      "Epoch:  2664 \t Loss:  0.6414607\n",
      "Epoch:  2665 \t Loss:  0.64132047\n",
      "Epoch:  2666 \t Loss:  0.64117986\n",
      "Epoch:  2667 \t Loss:  0.64104\n",
      "Epoch:  2668 \t Loss:  0.6408996\n",
      "Epoch:  2669 \t Loss:  0.6407597\n",
      "Epoch:  2670 \t Loss:  0.6406195\n",
      "Epoch:  2671 \t Loss:  0.6404794\n",
      "Epoch:  2672 \t Loss:  0.6403397\n",
      "Epoch:  2673 \t Loss:  0.64019895\n",
      "Epoch:  2674 \t Loss:  0.64006037\n",
      "Epoch:  2675 \t Loss:  0.6399202\n",
      "Epoch:  2676 \t Loss:  0.6397806\n",
      "Epoch:  2677 \t Loss:  0.6396409\n",
      "Epoch:  2678 \t Loss:  0.6395017\n",
      "Epoch:  2679 \t Loss:  0.639361\n",
      "Epoch:  2680 \t Loss:  0.6392223\n",
      "Epoch:  2681 \t Loss:  0.63908345\n",
      "Epoch:  2682 \t Loss:  0.6389436\n",
      "Epoch:  2683 \t Loss:  0.6388049\n",
      "Epoch:  2684 \t Loss:  0.6386652\n",
      "Epoch:  2685 \t Loss:  0.6385272\n",
      "Epoch:  2686 \t Loss:  0.6383882\n",
      "Epoch:  2687 \t Loss:  0.6382493\n",
      "Epoch:  2688 \t Loss:  0.63810974\n",
      "Epoch:  2689 \t Loss:  0.63797134\n",
      "Epoch:  2690 \t Loss:  0.6378333\n",
      "Epoch:  2691 \t Loss:  0.6376945\n",
      "Epoch:  2692 \t Loss:  0.6375555\n",
      "Epoch:  2693 \t Loss:  0.637417\n",
      "Epoch:  2694 \t Loss:  0.6372788\n",
      "Epoch:  2695 \t Loss:  0.6371404\n",
      "Epoch:  2696 \t Loss:  0.63700217\n",
      "Epoch:  2697 \t Loss:  0.6368639\n",
      "Epoch:  2698 \t Loss:  0.636726\n",
      "Epoch:  2699 \t Loss:  0.6365883\n",
      "Epoch:  2700 \t Loss:  0.63645077\n",
      "Epoch:  2701 \t Loss:  0.63631177\n",
      "Epoch:  2702 \t Loss:  0.63617367\n",
      "Epoch:  2703 \t Loss:  0.63603634\n",
      "Epoch:  2704 \t Loss:  0.6358986\n",
      "Epoch:  2705 \t Loss:  0.63576037\n",
      "Epoch:  2706 \t Loss:  0.6356227\n",
      "Epoch:  2707 \t Loss:  0.63548535\n",
      "Epoch:  2708 \t Loss:  0.6353485\n",
      "Epoch:  2709 \t Loss:  0.63521075\n",
      "Epoch:  2710 \t Loss:  0.6350742\n",
      "Epoch:  2711 \t Loss:  0.63493663\n",
      "Epoch:  2712 \t Loss:  0.6347998\n",
      "Epoch:  2713 \t Loss:  0.63466257\n",
      "Epoch:  2714 \t Loss:  0.63452584\n",
      "Epoch:  2715 \t Loss:  0.6343884\n",
      "Epoch:  2716 \t Loss:  0.6342517\n",
      "Epoch:  2717 \t Loss:  0.6341151\n",
      "Epoch:  2718 \t Loss:  0.6339781\n",
      "Epoch:  2719 \t Loss:  0.63384104\n",
      "Epoch:  2720 \t Loss:  0.6337048\n",
      "Epoch:  2721 \t Loss:  0.6335691\n",
      "Epoch:  2722 \t Loss:  0.63343257\n",
      "Epoch:  2723 \t Loss:  0.63329583\n",
      "Epoch:  2724 \t Loss:  0.6331595\n",
      "Epoch:  2725 \t Loss:  0.63302296\n",
      "Epoch:  2726 \t Loss:  0.6328866\n",
      "Epoch:  2727 \t Loss:  0.6327506\n",
      "Epoch:  2728 \t Loss:  0.63261473\n",
      "Epoch:  2729 \t Loss:  0.63247806\n",
      "Epoch:  2730 \t Loss:  0.63234323\n",
      "Epoch:  2731 \t Loss:  0.6322072\n",
      "Epoch:  2732 \t Loss:  0.6320714\n",
      "Epoch:  2733 \t Loss:  0.6319358\n",
      "Epoch:  2734 \t Loss:  0.6318004\n",
      "Epoch:  2735 \t Loss:  0.631665\n",
      "Epoch:  2736 \t Loss:  0.6315288\n",
      "Epoch:  2737 \t Loss:  0.63139343\n",
      "Epoch:  2738 \t Loss:  0.6312579\n",
      "Epoch:  2739 \t Loss:  0.6311227\n",
      "Epoch:  2740 \t Loss:  0.6309871\n",
      "Epoch:  2741 \t Loss:  0.6308523\n",
      "Epoch:  2742 \t Loss:  0.6307174\n",
      "Epoch:  2743 \t Loss:  0.6305825\n",
      "Epoch:  2744 \t Loss:  0.63044757\n",
      "Epoch:  2745 \t Loss:  0.6303124\n",
      "Epoch:  2746 \t Loss:  0.6301778\n",
      "Epoch:  2747 \t Loss:  0.63004315\n",
      "Epoch:  2748 \t Loss:  0.6299079\n",
      "Epoch:  2749 \t Loss:  0.6297736\n",
      "Epoch:  2750 \t Loss:  0.62963927\n",
      "Epoch:  2751 \t Loss:  0.629505\n",
      "Epoch:  2752 \t Loss:  0.62937045\n",
      "Epoch:  2753 \t Loss:  0.6292359\n",
      "Epoch:  2754 \t Loss:  0.6291018\n",
      "Epoch:  2755 \t Loss:  0.6289673\n",
      "Epoch:  2756 \t Loss:  0.6288328\n",
      "Epoch:  2757 \t Loss:  0.62869877\n",
      "Epoch:  2758 \t Loss:  0.6285645\n",
      "Epoch:  2759 \t Loss:  0.6284306\n",
      "Epoch:  2760 \t Loss:  0.62829703\n",
      "Epoch:  2761 \t Loss:  0.628163\n",
      "Epoch:  2762 \t Loss:  0.6280288\n",
      "Epoch:  2763 \t Loss:  0.62789553\n",
      "Epoch:  2764 \t Loss:  0.62776166\n",
      "Epoch:  2765 \t Loss:  0.627628\n",
      "Epoch:  2766 \t Loss:  0.62749463\n",
      "Epoch:  2767 \t Loss:  0.62736094\n",
      "Epoch:  2768 \t Loss:  0.6272281\n",
      "Epoch:  2769 \t Loss:  0.6270952\n",
      "Epoch:  2770 \t Loss:  0.6269617\n",
      "Epoch:  2771 \t Loss:  0.62682813\n",
      "Epoch:  2772 \t Loss:  0.62669545\n",
      "Epoch:  2773 \t Loss:  0.6265618\n",
      "Epoch:  2774 \t Loss:  0.6264291\n",
      "Epoch:  2775 \t Loss:  0.6262958\n",
      "Epoch:  2776 \t Loss:  0.6261637\n",
      "Epoch:  2777 \t Loss:  0.6260311\n",
      "Epoch:  2778 \t Loss:  0.62589794\n",
      "Epoch:  2779 \t Loss:  0.62576497\n",
      "Epoch:  2780 \t Loss:  0.6256331\n",
      "Epoch:  2781 \t Loss:  0.6255003\n",
      "Epoch:  2782 \t Loss:  0.6253674\n",
      "Epoch:  2783 \t Loss:  0.62523514\n",
      "Epoch:  2784 \t Loss:  0.6251032\n",
      "Epoch:  2785 \t Loss:  0.6249705\n",
      "Epoch:  2786 \t Loss:  0.624838\n",
      "Epoch:  2787 \t Loss:  0.6247059\n",
      "Epoch:  2788 \t Loss:  0.6245738\n",
      "Epoch:  2789 \t Loss:  0.62444204\n",
      "Epoch:  2790 \t Loss:  0.62431\n",
      "Epoch:  2791 \t Loss:  0.6241781\n",
      "Epoch:  2792 \t Loss:  0.62404656\n",
      "Epoch:  2793 \t Loss:  0.6239146\n",
      "Epoch:  2794 \t Loss:  0.6237831\n",
      "Epoch:  2795 \t Loss:  0.6236508\n",
      "Epoch:  2796 \t Loss:  0.62351966\n",
      "Epoch:  2797 \t Loss:  0.62338823\n",
      "Epoch:  2798 \t Loss:  0.62325644\n",
      "Epoch:  2799 \t Loss:  0.62312514\n",
      "Epoch:  2800 \t Loss:  0.6229934\n",
      "Epoch:  2801 \t Loss:  0.6228626\n",
      "Epoch:  2802 \t Loss:  0.6227315\n",
      "Epoch:  2803 \t Loss:  0.6226005\n",
      "Epoch:  2804 \t Loss:  0.6224689\n",
      "Epoch:  2805 \t Loss:  0.6223382\n",
      "Epoch:  2806 \t Loss:  0.6222073\n",
      "Epoch:  2807 \t Loss:  0.6220763\n",
      "Epoch:  2808 \t Loss:  0.62194556\n",
      "Epoch:  2809 \t Loss:  0.62181485\n",
      "Epoch:  2810 \t Loss:  0.6216841\n",
      "Epoch:  2811 \t Loss:  0.62155294\n",
      "Epoch:  2812 \t Loss:  0.6214232\n",
      "Epoch:  2813 \t Loss:  0.6212925\n",
      "Epoch:  2814 \t Loss:  0.6211624\n",
      "Epoch:  2815 \t Loss:  0.6210318\n",
      "Epoch:  2816 \t Loss:  0.62090117\n",
      "Epoch:  2817 \t Loss:  0.620771\n",
      "Epoch:  2818 \t Loss:  0.620641\n",
      "Epoch:  2819 \t Loss:  0.6205105\n",
      "Epoch:  2820 \t Loss:  0.6203799\n",
      "Epoch:  2821 \t Loss:  0.62025064\n",
      "Epoch:  2822 \t Loss:  0.62012124\n",
      "Epoch:  2823 \t Loss:  0.61999124\n",
      "Epoch:  2824 \t Loss:  0.6198612\n",
      "Epoch:  2825 \t Loss:  0.6197313\n",
      "Epoch:  2826 \t Loss:  0.61960125\n",
      "Epoch:  2827 \t Loss:  0.61947155\n",
      "Epoch:  2828 \t Loss:  0.6193422\n",
      "Epoch:  2829 \t Loss:  0.6192126\n",
      "Epoch:  2830 \t Loss:  0.6190839\n",
      "Epoch:  2831 \t Loss:  0.61895424\n",
      "Epoch:  2832 \t Loss:  0.61882436\n",
      "Epoch:  2833 \t Loss:  0.6186955\n",
      "Epoch:  2834 \t Loss:  0.6185663\n",
      "Epoch:  2835 \t Loss:  0.6184375\n",
      "Epoch:  2836 \t Loss:  0.6183079\n",
      "Epoch:  2837 \t Loss:  0.6181789\n",
      "Epoch:  2838 \t Loss:  0.6180502\n",
      "Epoch:  2839 \t Loss:  0.617921\n",
      "Epoch:  2840 \t Loss:  0.6177926\n",
      "Epoch:  2841 \t Loss:  0.6176629\n",
      "Epoch:  2842 \t Loss:  0.617535\n",
      "Epoch:  2843 \t Loss:  0.61740655\n",
      "Epoch:  2844 \t Loss:  0.61727756\n",
      "Epoch:  2845 \t Loss:  0.6171496\n",
      "Epoch:  2846 \t Loss:  0.61702096\n",
      "Epoch:  2847 \t Loss:  0.616892\n",
      "Epoch:  2848 \t Loss:  0.6167633\n",
      "Epoch:  2849 \t Loss:  0.6166354\n",
      "Epoch:  2850 \t Loss:  0.6165076\n",
      "Epoch:  2851 \t Loss:  0.6163797\n",
      "Epoch:  2852 \t Loss:  0.61625135\n",
      "Epoch:  2853 \t Loss:  0.6161232\n",
      "Epoch:  2854 \t Loss:  0.6159953\n",
      "Epoch:  2855 \t Loss:  0.6158668\n",
      "Epoch:  2856 \t Loss:  0.61573905\n",
      "Epoch:  2857 \t Loss:  0.6156113\n",
      "Epoch:  2858 \t Loss:  0.6154838\n",
      "Epoch:  2859 \t Loss:  0.6153559\n",
      "Epoch:  2860 \t Loss:  0.615228\n",
      "Epoch:  2861 \t Loss:  0.6151006\n",
      "Epoch:  2862 \t Loss:  0.6149729\n",
      "Epoch:  2863 \t Loss:  0.6148457\n",
      "Epoch:  2864 \t Loss:  0.614718\n",
      "Epoch:  2865 \t Loss:  0.61459076\n",
      "Epoch:  2866 \t Loss:  0.6144639\n",
      "Epoch:  2867 \t Loss:  0.6143362\n",
      "Epoch:  2868 \t Loss:  0.61420923\n",
      "Epoch:  2869 \t Loss:  0.6140822\n",
      "Epoch:  2870 \t Loss:  0.6139549\n",
      "Epoch:  2871 \t Loss:  0.61382854\n",
      "Epoch:  2872 \t Loss:  0.61370146\n",
      "Epoch:  2873 \t Loss:  0.61357474\n",
      "Epoch:  2874 \t Loss:  0.6134479\n",
      "Epoch:  2875 \t Loss:  0.613321\n",
      "Epoch:  2876 \t Loss:  0.61319435\n",
      "Epoch:  2877 \t Loss:  0.6130681\n",
      "Epoch:  2878 \t Loss:  0.6129408\n",
      "Epoch:  2879 \t Loss:  0.6128151\n",
      "Epoch:  2880 \t Loss:  0.6126891\n",
      "Epoch:  2881 \t Loss:  0.61256194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2882 \t Loss:  0.61243576\n",
      "Epoch:  2883 \t Loss:  0.61230993\n",
      "Epoch:  2884 \t Loss:  0.61218315\n",
      "Epoch:  2885 \t Loss:  0.6120575\n",
      "Epoch:  2886 \t Loss:  0.61193115\n",
      "Epoch:  2887 \t Loss:  0.611805\n",
      "Epoch:  2888 \t Loss:  0.61167896\n",
      "Epoch:  2889 \t Loss:  0.61155295\n",
      "Epoch:  2890 \t Loss:  0.61142725\n",
      "Epoch:  2891 \t Loss:  0.611302\n",
      "Epoch:  2892 \t Loss:  0.6111755\n",
      "Epoch:  2893 \t Loss:  0.61105025\n",
      "Epoch:  2894 \t Loss:  0.6109238\n",
      "Epoch:  2895 \t Loss:  0.6107988\n",
      "Epoch:  2896 \t Loss:  0.6106736\n",
      "Epoch:  2897 \t Loss:  0.61054826\n",
      "Epoch:  2898 \t Loss:  0.6104224\n",
      "Epoch:  2899 \t Loss:  0.6102971\n",
      "Epoch:  2900 \t Loss:  0.610172\n",
      "Epoch:  2901 \t Loss:  0.6100473\n",
      "Epoch:  2902 \t Loss:  0.6099219\n",
      "Epoch:  2903 \t Loss:  0.6097962\n",
      "Epoch:  2904 \t Loss:  0.60967195\n",
      "Epoch:  2905 \t Loss:  0.6095468\n",
      "Epoch:  2906 \t Loss:  0.609422\n",
      "Epoch:  2907 \t Loss:  0.6092965\n",
      "Epoch:  2908 \t Loss:  0.6091724\n",
      "Epoch:  2909 \t Loss:  0.60904753\n",
      "Epoch:  2910 \t Loss:  0.6089221\n",
      "Epoch:  2911 \t Loss:  0.6087984\n",
      "Epoch:  2912 \t Loss:  0.6086736\n",
      "Epoch:  2913 \t Loss:  0.60854906\n",
      "Epoch:  2914 \t Loss:  0.6084244\n",
      "Epoch:  2915 \t Loss:  0.6082998\n",
      "Epoch:  2916 \t Loss:  0.60817575\n",
      "Epoch:  2917 \t Loss:  0.6080515\n",
      "Epoch:  2918 \t Loss:  0.60792685\n",
      "Epoch:  2919 \t Loss:  0.60780305\n",
      "Epoch:  2920 \t Loss:  0.6076793\n",
      "Epoch:  2921 \t Loss:  0.6075551\n",
      "Epoch:  2922 \t Loss:  0.6074307\n",
      "Epoch:  2923 \t Loss:  0.6073063\n",
      "Epoch:  2924 \t Loss:  0.6071827\n",
      "Epoch:  2925 \t Loss:  0.6070591\n",
      "Epoch:  2926 \t Loss:  0.6069357\n",
      "Epoch:  2927 \t Loss:  0.6068118\n",
      "Epoch:  2928 \t Loss:  0.60668826\n",
      "Epoch:  2929 \t Loss:  0.6065641\n",
      "Epoch:  2930 \t Loss:  0.6064401\n",
      "Epoch:  2931 \t Loss:  0.60631776\n",
      "Epoch:  2932 \t Loss:  0.60619444\n",
      "Epoch:  2933 \t Loss:  0.60607064\n",
      "Epoch:  2934 \t Loss:  0.60594696\n",
      "Epoch:  2935 \t Loss:  0.60582376\n",
      "Epoch:  2936 \t Loss:  0.60570025\n",
      "Epoch:  2937 \t Loss:  0.605578\n",
      "Epoch:  2938 \t Loss:  0.60545427\n",
      "Epoch:  2939 \t Loss:  0.6053313\n",
      "Epoch:  2940 \t Loss:  0.6052083\n",
      "Epoch:  2941 \t Loss:  0.60508543\n",
      "Epoch:  2942 \t Loss:  0.6049626\n",
      "Epoch:  2943 \t Loss:  0.6048395\n",
      "Epoch:  2944 \t Loss:  0.60471714\n",
      "Epoch:  2945 \t Loss:  0.6045946\n",
      "Epoch:  2946 \t Loss:  0.60447156\n",
      "Epoch:  2947 \t Loss:  0.60434866\n",
      "Epoch:  2948 \t Loss:  0.604227\n",
      "Epoch:  2949 \t Loss:  0.6041042\n",
      "Epoch:  2950 \t Loss:  0.60398144\n",
      "Epoch:  2951 \t Loss:  0.60385925\n",
      "Epoch:  2952 \t Loss:  0.603737\n",
      "Epoch:  2953 \t Loss:  0.6036147\n",
      "Epoch:  2954 \t Loss:  0.6034923\n",
      "Epoch:  2955 \t Loss:  0.60337096\n",
      "Epoch:  2956 \t Loss:  0.60324866\n",
      "Epoch:  2957 \t Loss:  0.6031259\n",
      "Epoch:  2958 \t Loss:  0.6030042\n",
      "Epoch:  2959 \t Loss:  0.6028827\n",
      "Epoch:  2960 \t Loss:  0.60276014\n",
      "Epoch:  2961 \t Loss:  0.6026386\n",
      "Epoch:  2962 \t Loss:  0.6025172\n",
      "Epoch:  2963 \t Loss:  0.6023951\n",
      "Epoch:  2964 \t Loss:  0.6022736\n",
      "Epoch:  2965 \t Loss:  0.6021522\n",
      "Epoch:  2966 \t Loss:  0.60203004\n",
      "Epoch:  2967 \t Loss:  0.6019089\n",
      "Epoch:  2968 \t Loss:  0.60178757\n",
      "Epoch:  2969 \t Loss:  0.6016663\n",
      "Epoch:  2970 \t Loss:  0.6015452\n",
      "Epoch:  2971 \t Loss:  0.60142314\n",
      "Epoch:  2972 \t Loss:  0.60130244\n",
      "Epoch:  2973 \t Loss:  0.60118115\n",
      "Epoch:  2974 \t Loss:  0.6010598\n",
      "Epoch:  2975 \t Loss:  0.600939\n",
      "Epoch:  2976 \t Loss:  0.6008177\n",
      "Epoch:  2977 \t Loss:  0.600697\n",
      "Epoch:  2978 \t Loss:  0.60057586\n",
      "Epoch:  2979 \t Loss:  0.6004551\n",
      "Epoch:  2980 \t Loss:  0.6003349\n",
      "Epoch:  2981 \t Loss:  0.600214\n",
      "Epoch:  2982 \t Loss:  0.6000932\n",
      "Epoch:  2983 \t Loss:  0.5999727\n",
      "Epoch:  2984 \t Loss:  0.599852\n",
      "Epoch:  2985 \t Loss:  0.59973204\n",
      "Epoch:  2986 \t Loss:  0.5996112\n",
      "Epoch:  2987 \t Loss:  0.5994907\n",
      "Epoch:  2988 \t Loss:  0.59937036\n",
      "Epoch:  2989 \t Loss:  0.59924954\n",
      "Epoch:  2990 \t Loss:  0.59913\n",
      "Epoch:  2991 \t Loss:  0.5990093\n",
      "Epoch:  2992 \t Loss:  0.5988896\n",
      "Epoch:  2993 \t Loss:  0.5987693\n",
      "Epoch:  2994 \t Loss:  0.5986498\n",
      "Epoch:  2995 \t Loss:  0.5985294\n",
      "Epoch:  2996 \t Loss:  0.59840983\n",
      "Epoch:  2997 \t Loss:  0.59828955\n",
      "Epoch:  2998 \t Loss:  0.59817\n",
      "Epoch:  2999 \t Loss:  0.59804994\n",
      "Epoch:  3000 \t Loss:  0.5979304\n",
      "Epoch:  3001 \t Loss:  0.5978106\n",
      "Epoch:  3002 \t Loss:  0.5976911\n",
      "Epoch:  3003 \t Loss:  0.59757155\n",
      "Epoch:  3004 \t Loss:  0.59745216\n",
      "Epoch:  3005 \t Loss:  0.5973325\n",
      "Epoch:  3006 \t Loss:  0.59721345\n",
      "Epoch:  3007 \t Loss:  0.59709406\n",
      "Epoch:  3008 \t Loss:  0.59697455\n",
      "Epoch:  3009 \t Loss:  0.5968554\n",
      "Epoch:  3010 \t Loss:  0.5967365\n",
      "Epoch:  3011 \t Loss:  0.59661716\n",
      "Epoch:  3012 \t Loss:  0.5964984\n",
      "Epoch:  3013 \t Loss:  0.59637874\n",
      "Epoch:  3014 \t Loss:  0.5962602\n",
      "Epoch:  3015 \t Loss:  0.59614104\n",
      "Epoch:  3016 \t Loss:  0.5960228\n",
      "Epoch:  3017 \t Loss:  0.5959032\n",
      "Epoch:  3018 \t Loss:  0.59578526\n",
      "Epoch:  3019 \t Loss:  0.5956661\n",
      "Epoch:  3020 \t Loss:  0.5955472\n",
      "Epoch:  3021 \t Loss:  0.5954286\n",
      "Epoch:  3022 \t Loss:  0.59531033\n",
      "Epoch:  3023 \t Loss:  0.59519166\n",
      "Epoch:  3024 \t Loss:  0.5950741\n",
      "Epoch:  3025 \t Loss:  0.5949558\n",
      "Epoch:  3026 \t Loss:  0.59483707\n",
      "Epoch:  3027 \t Loss:  0.5947181\n",
      "Epoch:  3028 \t Loss:  0.5946004\n",
      "Epoch:  3029 \t Loss:  0.59448195\n",
      "Epoch:  3030 \t Loss:  0.59436387\n",
      "Epoch:  3031 \t Loss:  0.5942458\n",
      "Epoch:  3032 \t Loss:  0.59412766\n",
      "Epoch:  3033 \t Loss:  0.5940098\n",
      "Epoch:  3034 \t Loss:  0.593892\n",
      "Epoch:  3035 \t Loss:  0.5937743\n",
      "Epoch:  3036 \t Loss:  0.59365666\n",
      "Epoch:  3037 \t Loss:  0.59353846\n",
      "Epoch:  3038 \t Loss:  0.59342104\n",
      "Epoch:  3039 \t Loss:  0.593303\n",
      "Epoch:  3040 \t Loss:  0.59318525\n",
      "Epoch:  3041 \t Loss:  0.593068\n",
      "Epoch:  3042 \t Loss:  0.59295034\n",
      "Epoch:  3043 \t Loss:  0.592833\n",
      "Epoch:  3044 \t Loss:  0.5927151\n",
      "Epoch:  3045 \t Loss:  0.59259796\n",
      "Epoch:  3046 \t Loss:  0.5924805\n",
      "Epoch:  3047 \t Loss:  0.5923639\n",
      "Epoch:  3048 \t Loss:  0.59224623\n",
      "Epoch:  3049 \t Loss:  0.59212935\n",
      "Epoch:  3050 \t Loss:  0.59201264\n",
      "Epoch:  3051 \t Loss:  0.5918953\n",
      "Epoch:  3052 \t Loss:  0.59177834\n",
      "Epoch:  3053 \t Loss:  0.59166104\n",
      "Epoch:  3054 \t Loss:  0.5915449\n",
      "Epoch:  3055 \t Loss:  0.5914281\n",
      "Epoch:  3056 \t Loss:  0.5913113\n",
      "Epoch:  3057 \t Loss:  0.59119385\n",
      "Epoch:  3058 \t Loss:  0.59107643\n",
      "Epoch:  3059 \t Loss:  0.5909605\n",
      "Epoch:  3060 \t Loss:  0.590844\n",
      "Epoch:  3061 \t Loss:  0.59072757\n",
      "Epoch:  3062 \t Loss:  0.5906109\n",
      "Epoch:  3063 \t Loss:  0.5904944\n",
      "Epoch:  3064 \t Loss:  0.590378\n",
      "Epoch:  3065 \t Loss:  0.59026176\n",
      "Epoch:  3066 \t Loss:  0.5901455\n",
      "Epoch:  3067 \t Loss:  0.5900297\n",
      "Epoch:  3068 \t Loss:  0.58991265\n",
      "Epoch:  3069 \t Loss:  0.58979666\n",
      "Epoch:  3070 \t Loss:  0.5896809\n",
      "Epoch:  3071 \t Loss:  0.589565\n",
      "Epoch:  3072 \t Loss:  0.58944917\n",
      "Epoch:  3073 \t Loss:  0.5893332\n",
      "Epoch:  3074 \t Loss:  0.58921736\n",
      "Epoch:  3075 \t Loss:  0.58910143\n",
      "Epoch:  3076 \t Loss:  0.58898556\n",
      "Epoch:  3077 \t Loss:  0.5888705\n",
      "Epoch:  3078 \t Loss:  0.5887546\n",
      "Epoch:  3079 \t Loss:  0.5886382\n",
      "Epoch:  3080 \t Loss:  0.58852303\n",
      "Epoch:  3081 \t Loss:  0.5884076\n",
      "Epoch:  3082 \t Loss:  0.5882916\n",
      "Epoch:  3083 \t Loss:  0.5881762\n",
      "Epoch:  3084 \t Loss:  0.5880607\n",
      "Epoch:  3085 \t Loss:  0.58794516\n",
      "Epoch:  3086 \t Loss:  0.58783\n",
      "Epoch:  3087 \t Loss:  0.5877149\n",
      "Epoch:  3088 \t Loss:  0.5876004\n",
      "Epoch:  3089 \t Loss:  0.5874854\n",
      "Epoch:  3090 \t Loss:  0.58737046\n",
      "Epoch:  3091 \t Loss:  0.5872551\n",
      "Epoch:  3092 \t Loss:  0.5871403\n",
      "Epoch:  3093 \t Loss:  0.5870259\n",
      "Epoch:  3094 \t Loss:  0.58691055\n",
      "Epoch:  3095 \t Loss:  0.58679575\n",
      "Epoch:  3096 \t Loss:  0.5866809\n",
      "Epoch:  3097 \t Loss:  0.58656603\n",
      "Epoch:  3098 \t Loss:  0.5864515\n",
      "Epoch:  3099 \t Loss:  0.5863368\n",
      "Epoch:  3100 \t Loss:  0.58622193\n",
      "Epoch:  3101 \t Loss:  0.5861077\n",
      "Epoch:  3102 \t Loss:  0.58599335\n",
      "Epoch:  3103 \t Loss:  0.5858792\n",
      "Epoch:  3104 \t Loss:  0.5857647\n",
      "Epoch:  3105 \t Loss:  0.5856504\n",
      "Epoch:  3106 \t Loss:  0.5855362\n",
      "Epoch:  3107 \t Loss:  0.5854218\n",
      "Epoch:  3108 \t Loss:  0.5853077\n",
      "Epoch:  3109 \t Loss:  0.585193\n",
      "Epoch:  3110 \t Loss:  0.58507854\n",
      "Epoch:  3111 \t Loss:  0.5849644\n",
      "Epoch:  3112 \t Loss:  0.5848508\n",
      "Epoch:  3113 \t Loss:  0.5847368\n",
      "Epoch:  3114 \t Loss:  0.5846229\n",
      "Epoch:  3115 \t Loss:  0.58450955\n",
      "Epoch:  3116 \t Loss:  0.58439577\n",
      "Epoch:  3117 \t Loss:  0.5842821\n",
      "Epoch:  3118 \t Loss:  0.5841685\n",
      "Epoch:  3119 \t Loss:  0.5840549\n",
      "Epoch:  3120 \t Loss:  0.5839408\n",
      "Epoch:  3121 \t Loss:  0.5838268\n",
      "Epoch:  3122 \t Loss:  0.5837132\n",
      "Epoch:  3123 \t Loss:  0.5835999\n",
      "Epoch:  3124 \t Loss:  0.58348745\n",
      "Epoch:  3125 \t Loss:  0.5833737\n",
      "Epoch:  3126 \t Loss:  0.58326006\n",
      "Epoch:  3127 \t Loss:  0.58314675\n",
      "Epoch:  3128 \t Loss:  0.5830338\n",
      "Epoch:  3129 \t Loss:  0.5829211\n",
      "Epoch:  3130 \t Loss:  0.58280784\n",
      "Epoch:  3131 \t Loss:  0.58269435\n",
      "Epoch:  3132 \t Loss:  0.58258134\n",
      "Epoch:  3133 \t Loss:  0.58246917\n",
      "Epoch:  3134 \t Loss:  0.5823558\n",
      "Epoch:  3135 \t Loss:  0.58224255\n",
      "Epoch:  3136 \t Loss:  0.5821301\n",
      "Epoch:  3137 \t Loss:  0.58201724\n",
      "Epoch:  3138 \t Loss:  0.58190393\n",
      "Epoch:  3139 \t Loss:  0.58179206\n",
      "Epoch:  3140 \t Loss:  0.58167917\n",
      "Epoch:  3141 \t Loss:  0.5815667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3142 \t Loss:  0.58145416\n",
      "Epoch:  3143 \t Loss:  0.5813418\n",
      "Epoch:  3144 \t Loss:  0.5812296\n",
      "Epoch:  3145 \t Loss:  0.581117\n",
      "Epoch:  3146 \t Loss:  0.5810047\n",
      "Epoch:  3147 \t Loss:  0.5808925\n",
      "Epoch:  3148 \t Loss:  0.58078015\n",
      "Epoch:  3149 \t Loss:  0.58066803\n",
      "Epoch:  3150 \t Loss:  0.5805563\n",
      "Epoch:  3151 \t Loss:  0.58044374\n",
      "Epoch:  3152 \t Loss:  0.5803315\n",
      "Epoch:  3153 \t Loss:  0.58022034\n",
      "Epoch:  3154 \t Loss:  0.5801071\n",
      "Epoch:  3155 \t Loss:  0.57999533\n",
      "Epoch:  3156 \t Loss:  0.579884\n",
      "Epoch:  3157 \t Loss:  0.5797713\n",
      "Epoch:  3158 \t Loss:  0.57966024\n",
      "Epoch:  3159 \t Loss:  0.57954866\n",
      "Epoch:  3160 \t Loss:  0.5794362\n",
      "Epoch:  3161 \t Loss:  0.5793239\n",
      "Epoch:  3162 \t Loss:  0.57921255\n",
      "Epoch:  3163 \t Loss:  0.57910156\n",
      "Epoch:  3164 \t Loss:  0.5789901\n",
      "Epoch:  3165 \t Loss:  0.57887834\n",
      "Epoch:  3166 \t Loss:  0.57876694\n",
      "Epoch:  3167 \t Loss:  0.57865614\n",
      "Epoch:  3168 \t Loss:  0.57854515\n",
      "Epoch:  3169 \t Loss:  0.578434\n",
      "Epoch:  3170 \t Loss:  0.5783227\n",
      "Epoch:  3171 \t Loss:  0.5782113\n",
      "Epoch:  3172 \t Loss:  0.57809913\n",
      "Epoch:  3173 \t Loss:  0.577988\n",
      "Epoch:  3174 \t Loss:  0.57787716\n",
      "Epoch:  3175 \t Loss:  0.5777665\n",
      "Epoch:  3176 \t Loss:  0.5776559\n",
      "Epoch:  3177 \t Loss:  0.5775453\n",
      "Epoch:  3178 \t Loss:  0.57743466\n",
      "Epoch:  3179 \t Loss:  0.57732344\n",
      "Epoch:  3180 \t Loss:  0.57721245\n",
      "Epoch:  3181 \t Loss:  0.57710165\n",
      "Epoch:  3182 \t Loss:  0.57699084\n",
      "Epoch:  3183 \t Loss:  0.5768809\n",
      "Epoch:  3184 \t Loss:  0.57677037\n",
      "Epoch:  3185 \t Loss:  0.5766593\n",
      "Epoch:  3186 \t Loss:  0.5765488\n",
      "Epoch:  3187 \t Loss:  0.5764386\n",
      "Epoch:  3188 \t Loss:  0.5763282\n",
      "Epoch:  3189 \t Loss:  0.5762174\n",
      "Epoch:  3190 \t Loss:  0.5761079\n",
      "Epoch:  3191 \t Loss:  0.5759969\n",
      "Epoch:  3192 \t Loss:  0.57588726\n",
      "Epoch:  3193 \t Loss:  0.5757776\n",
      "Epoch:  3194 \t Loss:  0.57566696\n",
      "Epoch:  3195 \t Loss:  0.57555693\n",
      "Epoch:  3196 \t Loss:  0.57544655\n",
      "Epoch:  3197 \t Loss:  0.57533705\n",
      "Epoch:  3198 \t Loss:  0.57522655\n",
      "Epoch:  3199 \t Loss:  0.5751168\n",
      "Epoch:  3200 \t Loss:  0.5750071\n",
      "Epoch:  3201 \t Loss:  0.5748973\n",
      "Epoch:  3202 \t Loss:  0.5747875\n",
      "Epoch:  3203 \t Loss:  0.5746778\n",
      "Epoch:  3204 \t Loss:  0.57456875\n",
      "Epoch:  3205 \t Loss:  0.57445884\n",
      "Epoch:  3206 \t Loss:  0.57434946\n",
      "Epoch:  3207 \t Loss:  0.5742404\n",
      "Epoch:  3208 \t Loss:  0.57413083\n",
      "Epoch:  3209 \t Loss:  0.57402086\n",
      "Epoch:  3210 \t Loss:  0.57391167\n",
      "Epoch:  3211 \t Loss:  0.5738022\n",
      "Epoch:  3212 \t Loss:  0.57369256\n",
      "Epoch:  3213 \t Loss:  0.57358384\n",
      "Epoch:  3214 \t Loss:  0.5734742\n",
      "Epoch:  3215 \t Loss:  0.5733653\n",
      "Epoch:  3216 \t Loss:  0.5732566\n",
      "Epoch:  3217 \t Loss:  0.57314736\n",
      "Epoch:  3218 \t Loss:  0.573039\n",
      "Epoch:  3219 \t Loss:  0.5729292\n",
      "Epoch:  3220 \t Loss:  0.57282054\n",
      "Epoch:  3221 \t Loss:  0.57271135\n",
      "Epoch:  3222 \t Loss:  0.5726022\n",
      "Epoch:  3223 \t Loss:  0.5724938\n",
      "Epoch:  3224 \t Loss:  0.5723856\n",
      "Epoch:  3225 \t Loss:  0.5722762\n",
      "Epoch:  3226 \t Loss:  0.5721675\n",
      "Epoch:  3227 \t Loss:  0.572059\n",
      "Epoch:  3228 \t Loss:  0.5719507\n",
      "Epoch:  3229 \t Loss:  0.57184225\n",
      "Epoch:  3230 \t Loss:  0.5717336\n",
      "Epoch:  3231 \t Loss:  0.57162505\n",
      "Epoch:  3232 \t Loss:  0.57151616\n",
      "Epoch:  3233 \t Loss:  0.57140833\n",
      "Epoch:  3234 \t Loss:  0.57130045\n",
      "Epoch:  3235 \t Loss:  0.5711921\n",
      "Epoch:  3236 \t Loss:  0.57108396\n",
      "Epoch:  3237 \t Loss:  0.5709755\n",
      "Epoch:  3238 \t Loss:  0.5708675\n",
      "Epoch:  3239 \t Loss:  0.5707583\n",
      "Epoch:  3240 \t Loss:  0.5706511\n",
      "Epoch:  3241 \t Loss:  0.5705428\n",
      "Epoch:  3242 \t Loss:  0.57043517\n",
      "Epoch:  3243 \t Loss:  0.5703272\n",
      "Epoch:  3244 \t Loss:  0.5702192\n",
      "Epoch:  3245 \t Loss:  0.5701116\n",
      "Epoch:  3246 \t Loss:  0.5700038\n",
      "Epoch:  3247 \t Loss:  0.56989616\n",
      "Epoch:  3248 \t Loss:  0.5697886\n",
      "Epoch:  3249 \t Loss:  0.5696808\n",
      "Epoch:  3250 \t Loss:  0.56957334\n",
      "Epoch:  3251 \t Loss:  0.5694655\n",
      "Epoch:  3252 \t Loss:  0.5693584\n",
      "Epoch:  3253 \t Loss:  0.56925094\n",
      "Epoch:  3254 \t Loss:  0.56914365\n",
      "Epoch:  3255 \t Loss:  0.56903636\n",
      "Epoch:  3256 \t Loss:  0.56892884\n",
      "Epoch:  3257 \t Loss:  0.56882125\n",
      "Epoch:  3258 \t Loss:  0.5687141\n",
      "Epoch:  3259 \t Loss:  0.56860703\n",
      "Epoch:  3260 \t Loss:  0.5685001\n",
      "Epoch:  3261 \t Loss:  0.56839305\n",
      "Epoch:  3262 \t Loss:  0.5682859\n",
      "Epoch:  3263 \t Loss:  0.5681787\n",
      "Epoch:  3264 \t Loss:  0.5680717\n",
      "Epoch:  3265 \t Loss:  0.567965\n",
      "Epoch:  3266 \t Loss:  0.56785816\n",
      "Epoch:  3267 \t Loss:  0.5677511\n",
      "Epoch:  3268 \t Loss:  0.567645\n",
      "Epoch:  3269 \t Loss:  0.56753784\n",
      "Epoch:  3270 \t Loss:  0.567431\n",
      "Epoch:  3271 \t Loss:  0.5673238\n",
      "Epoch:  3272 \t Loss:  0.56721765\n",
      "Epoch:  3273 \t Loss:  0.56711096\n",
      "Epoch:  3274 \t Loss:  0.5670046\n",
      "Epoch:  3275 \t Loss:  0.5668978\n",
      "Epoch:  3276 \t Loss:  0.56679165\n",
      "Epoch:  3277 \t Loss:  0.56668544\n",
      "Epoch:  3278 \t Loss:  0.5665787\n",
      "Epoch:  3279 \t Loss:  0.5664734\n",
      "Epoch:  3280 \t Loss:  0.56636673\n",
      "Epoch:  3281 \t Loss:  0.5662602\n",
      "Epoch:  3282 \t Loss:  0.5661538\n",
      "Epoch:  3283 \t Loss:  0.5660481\n",
      "Epoch:  3284 \t Loss:  0.56594193\n",
      "Epoch:  3285 \t Loss:  0.5658355\n",
      "Epoch:  3286 \t Loss:  0.5657294\n",
      "Epoch:  3287 \t Loss:  0.56562364\n",
      "Epoch:  3288 \t Loss:  0.565518\n",
      "Epoch:  3289 \t Loss:  0.56541204\n",
      "Epoch:  3290 \t Loss:  0.56530595\n",
      "Epoch:  3291 \t Loss:  0.5652003\n",
      "Epoch:  3292 \t Loss:  0.56509477\n",
      "Epoch:  3293 \t Loss:  0.5649891\n",
      "Epoch:  3294 \t Loss:  0.56488323\n",
      "Epoch:  3295 \t Loss:  0.5647774\n",
      "Epoch:  3296 \t Loss:  0.5646721\n",
      "Epoch:  3297 \t Loss:  0.56456625\n",
      "Epoch:  3298 \t Loss:  0.5644609\n",
      "Epoch:  3299 \t Loss:  0.5643551\n",
      "Epoch:  3300 \t Loss:  0.56425023\n",
      "Epoch:  3301 \t Loss:  0.5641442\n",
      "Epoch:  3302 \t Loss:  0.5640394\n",
      "Epoch:  3303 \t Loss:  0.56393415\n",
      "Epoch:  3304 \t Loss:  0.5638281\n",
      "Epoch:  3305 \t Loss:  0.5637244\n",
      "Epoch:  3306 \t Loss:  0.5636182\n",
      "Epoch:  3307 \t Loss:  0.5635122\n",
      "Epoch:  3308 \t Loss:  0.56340796\n",
      "Epoch:  3309 \t Loss:  0.5633029\n",
      "Epoch:  3310 \t Loss:  0.563198\n",
      "Epoch:  3311 \t Loss:  0.5630928\n",
      "Epoch:  3312 \t Loss:  0.56298774\n",
      "Epoch:  3313 \t Loss:  0.56288266\n",
      "Epoch:  3314 \t Loss:  0.56277823\n",
      "Epoch:  3315 \t Loss:  0.5626729\n",
      "Epoch:  3316 \t Loss:  0.5625694\n",
      "Epoch:  3317 \t Loss:  0.56246436\n",
      "Epoch:  3318 \t Loss:  0.56235945\n",
      "Epoch:  3319 \t Loss:  0.5622554\n",
      "Epoch:  3320 \t Loss:  0.5621506\n",
      "Epoch:  3321 \t Loss:  0.5620459\n",
      "Epoch:  3322 \t Loss:  0.56194204\n",
      "Epoch:  3323 \t Loss:  0.56183726\n",
      "Epoch:  3324 \t Loss:  0.5617326\n",
      "Epoch:  3325 \t Loss:  0.5616281\n",
      "Epoch:  3326 \t Loss:  0.561524\n",
      "Epoch:  3327 \t Loss:  0.56142026\n",
      "Epoch:  3328 \t Loss:  0.5613155\n",
      "Epoch:  3329 \t Loss:  0.5612115\n",
      "Epoch:  3330 \t Loss:  0.5611074\n",
      "Epoch:  3331 \t Loss:  0.56100315\n",
      "Epoch:  3332 \t Loss:  0.56089914\n",
      "Epoch:  3333 \t Loss:  0.56079435\n",
      "Epoch:  3334 \t Loss:  0.5606904\n",
      "Epoch:  3335 \t Loss:  0.5605864\n",
      "Epoch:  3336 \t Loss:  0.560483\n",
      "Epoch:  3337 \t Loss:  0.5603794\n",
      "Epoch:  3338 \t Loss:  0.5602756\n",
      "Epoch:  3339 \t Loss:  0.56017154\n",
      "Epoch:  3340 \t Loss:  0.5600681\n",
      "Epoch:  3341 \t Loss:  0.55996424\n",
      "Epoch:  3342 \t Loss:  0.5598603\n",
      "Epoch:  3343 \t Loss:  0.5597568\n",
      "Epoch:  3344 \t Loss:  0.55965334\n",
      "Epoch:  3345 \t Loss:  0.55955\n",
      "Epoch:  3346 \t Loss:  0.5594465\n",
      "Epoch:  3347 \t Loss:  0.5593427\n",
      "Epoch:  3348 \t Loss:  0.5592389\n",
      "Epoch:  3349 \t Loss:  0.55913633\n",
      "Epoch:  3350 \t Loss:  0.55903256\n",
      "Epoch:  3351 \t Loss:  0.55892974\n",
      "Epoch:  3352 \t Loss:  0.55882627\n",
      "Epoch:  3353 \t Loss:  0.5587233\n",
      "Epoch:  3354 \t Loss:  0.55861974\n",
      "Epoch:  3355 \t Loss:  0.558517\n",
      "Epoch:  3356 \t Loss:  0.55841345\n",
      "Epoch:  3357 \t Loss:  0.55831045\n",
      "Epoch:  3358 \t Loss:  0.558208\n",
      "Epoch:  3359 \t Loss:  0.5581048\n",
      "Epoch:  3360 \t Loss:  0.55800164\n",
      "Epoch:  3361 \t Loss:  0.5578987\n",
      "Epoch:  3362 \t Loss:  0.55779636\n",
      "Epoch:  3363 \t Loss:  0.55769295\n",
      "Epoch:  3364 \t Loss:  0.5575901\n",
      "Epoch:  3365 \t Loss:  0.5574876\n",
      "Epoch:  3366 \t Loss:  0.5573854\n",
      "Epoch:  3367 \t Loss:  0.5572827\n",
      "Epoch:  3368 \t Loss:  0.5571798\n",
      "Epoch:  3369 \t Loss:  0.5570777\n",
      "Epoch:  3370 \t Loss:  0.556975\n",
      "Epoch:  3371 \t Loss:  0.5568726\n",
      "Epoch:  3372 \t Loss:  0.5567704\n",
      "Epoch:  3373 \t Loss:  0.556668\n",
      "Epoch:  3374 \t Loss:  0.5565657\n",
      "Epoch:  3375 \t Loss:  0.5564633\n",
      "Epoch:  3376 \t Loss:  0.5563608\n",
      "Epoch:  3377 \t Loss:  0.55625886\n",
      "Epoch:  3378 \t Loss:  0.5561566\n",
      "Epoch:  3379 \t Loss:  0.55605435\n",
      "Epoch:  3380 \t Loss:  0.55595237\n",
      "Epoch:  3381 \t Loss:  0.5558501\n",
      "Epoch:  3382 \t Loss:  0.55574775\n",
      "Epoch:  3383 \t Loss:  0.555646\n",
      "Epoch:  3384 \t Loss:  0.55554396\n",
      "Epoch:  3385 \t Loss:  0.5554425\n",
      "Epoch:  3386 \t Loss:  0.5553409\n",
      "Epoch:  3387 \t Loss:  0.55523854\n",
      "Epoch:  3388 \t Loss:  0.5551373\n",
      "Epoch:  3389 \t Loss:  0.55503505\n",
      "Epoch:  3390 \t Loss:  0.5549332\n",
      "Epoch:  3391 \t Loss:  0.55483216\n",
      "Epoch:  3392 \t Loss:  0.5547305\n",
      "Epoch:  3393 \t Loss:  0.5546291\n",
      "Epoch:  3394 \t Loss:  0.5545273\n",
      "Epoch:  3395 \t Loss:  0.5544261\n",
      "Epoch:  3396 \t Loss:  0.55432445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3397 \t Loss:  0.5542224\n",
      "Epoch:  3398 \t Loss:  0.5541209\n",
      "Epoch:  3399 \t Loss:  0.5540198\n",
      "Epoch:  3400 \t Loss:  0.55391866\n",
      "Epoch:  3401 \t Loss:  0.55381674\n",
      "Epoch:  3402 \t Loss:  0.55371636\n",
      "Epoch:  3403 \t Loss:  0.5536147\n",
      "Epoch:  3404 \t Loss:  0.5535141\n",
      "Epoch:  3405 \t Loss:  0.553413\n",
      "Epoch:  3406 \t Loss:  0.5533116\n",
      "Epoch:  3407 \t Loss:  0.55321074\n",
      "Epoch:  3408 \t Loss:  0.5531098\n",
      "Epoch:  3409 \t Loss:  0.5530085\n",
      "Epoch:  3410 \t Loss:  0.5529082\n",
      "Epoch:  3411 \t Loss:  0.55280685\n",
      "Epoch:  3412 \t Loss:  0.5527062\n",
      "Epoch:  3413 \t Loss:  0.5526051\n",
      "Epoch:  3414 \t Loss:  0.55250436\n",
      "Epoch:  3415 \t Loss:  0.55240333\n",
      "Epoch:  3416 \t Loss:  0.55230296\n",
      "Epoch:  3417 \t Loss:  0.55220276\n",
      "Epoch:  3418 \t Loss:  0.5521026\n",
      "Epoch:  3419 \t Loss:  0.5520012\n",
      "Epoch:  3420 \t Loss:  0.55190116\n",
      "Epoch:  3421 \t Loss:  0.5518012\n",
      "Epoch:  3422 \t Loss:  0.5517005\n",
      "Epoch:  3423 \t Loss:  0.5516002\n",
      "Epoch:  3424 \t Loss:  0.5515001\n",
      "Epoch:  3425 \t Loss:  0.55139935\n",
      "Epoch:  3426 \t Loss:  0.55129904\n",
      "Epoch:  3427 \t Loss:  0.55119896\n",
      "Epoch:  3428 \t Loss:  0.55109864\n",
      "Epoch:  3429 \t Loss:  0.55099875\n",
      "Epoch:  3430 \t Loss:  0.5508986\n",
      "Epoch:  3431 \t Loss:  0.550798\n",
      "Epoch:  3432 \t Loss:  0.5506982\n",
      "Epoch:  3433 \t Loss:  0.5505982\n",
      "Epoch:  3434 \t Loss:  0.5504986\n",
      "Epoch:  3435 \t Loss:  0.5503985\n",
      "Epoch:  3436 \t Loss:  0.5502989\n",
      "Epoch:  3437 \t Loss:  0.5501988\n",
      "Epoch:  3438 \t Loss:  0.55009854\n",
      "Epoch:  3439 \t Loss:  0.5499988\n",
      "Epoch:  3440 \t Loss:  0.5498987\n",
      "Epoch:  3441 \t Loss:  0.5497997\n",
      "Epoch:  3442 \t Loss:  0.5497001\n",
      "Epoch:  3443 \t Loss:  0.5496003\n",
      "Epoch:  3444 \t Loss:  0.54950064\n",
      "Epoch:  3445 \t Loss:  0.5494009\n",
      "Epoch:  3446 \t Loss:  0.54930097\n",
      "Epoch:  3447 \t Loss:  0.54920185\n",
      "Epoch:  3448 \t Loss:  0.54910237\n",
      "Epoch:  3449 \t Loss:  0.54900384\n",
      "Epoch:  3450 \t Loss:  0.5489045\n",
      "Epoch:  3451 \t Loss:  0.5488047\n",
      "Epoch:  3452 \t Loss:  0.5487055\n",
      "Epoch:  3453 \t Loss:  0.54860663\n",
      "Epoch:  3454 \t Loss:  0.54850686\n",
      "Epoch:  3455 \t Loss:  0.5484078\n",
      "Epoch:  3456 \t Loss:  0.5483091\n",
      "Epoch:  3457 \t Loss:  0.5482094\n",
      "Epoch:  3458 \t Loss:  0.54811096\n",
      "Epoch:  3459 \t Loss:  0.54801154\n",
      "Epoch:  3460 \t Loss:  0.5479132\n",
      "Epoch:  3461 \t Loss:  0.54781395\n",
      "Epoch:  3462 \t Loss:  0.5477154\n",
      "Epoch:  3463 \t Loss:  0.5476161\n",
      "Epoch:  3464 \t Loss:  0.54751736\n",
      "Epoch:  3465 \t Loss:  0.5474182\n",
      "Epoch:  3466 \t Loss:  0.54731923\n",
      "Epoch:  3467 \t Loss:  0.5472207\n",
      "Epoch:  3468 \t Loss:  0.54712266\n",
      "Epoch:  3469 \t Loss:  0.5470241\n",
      "Epoch:  3470 \t Loss:  0.5469247\n",
      "Epoch:  3471 \t Loss:  0.5468261\n",
      "Epoch:  3472 \t Loss:  0.5467282\n",
      "Epoch:  3473 \t Loss:  0.5466301\n",
      "Epoch:  3474 \t Loss:  0.54653096\n",
      "Epoch:  3475 \t Loss:  0.546433\n",
      "Epoch:  3476 \t Loss:  0.54633445\n",
      "Epoch:  3477 \t Loss:  0.54623634\n",
      "Epoch:  3478 \t Loss:  0.5461382\n",
      "Epoch:  3479 \t Loss:  0.54603976\n",
      "Epoch:  3480 \t Loss:  0.5459414\n",
      "Epoch:  3481 \t Loss:  0.54584384\n",
      "Epoch:  3482 \t Loss:  0.54574513\n",
      "Epoch:  3483 \t Loss:  0.54564667\n",
      "Epoch:  3484 \t Loss:  0.54554826\n",
      "Epoch:  3485 \t Loss:  0.5454504\n",
      "Epoch:  3486 \t Loss:  0.54535264\n",
      "Epoch:  3487 \t Loss:  0.5452548\n",
      "Epoch:  3488 \t Loss:  0.5451572\n",
      "Epoch:  3489 \t Loss:  0.5450596\n",
      "Epoch:  3490 \t Loss:  0.54496163\n",
      "Epoch:  3491 \t Loss:  0.5448641\n",
      "Epoch:  3492 \t Loss:  0.5447662\n",
      "Epoch:  3493 \t Loss:  0.5446684\n",
      "Epoch:  3494 \t Loss:  0.54457015\n",
      "Epoch:  3495 \t Loss:  0.5444733\n",
      "Epoch:  3496 \t Loss:  0.5443754\n",
      "Epoch:  3497 \t Loss:  0.544278\n",
      "Epoch:  3498 \t Loss:  0.54418033\n",
      "Epoch:  3499 \t Loss:  0.54408234\n",
      "Epoch:  3500 \t Loss:  0.5439855\n",
      "Epoch:  3501 \t Loss:  0.5438888\n",
      "Epoch:  3502 \t Loss:  0.5437905\n",
      "Epoch:  3503 \t Loss:  0.543693\n",
      "Epoch:  3504 \t Loss:  0.54359543\n",
      "Epoch:  3505 \t Loss:  0.54349875\n",
      "Epoch:  3506 \t Loss:  0.5434015\n",
      "Epoch:  3507 \t Loss:  0.5433043\n",
      "Epoch:  3508 \t Loss:  0.54320693\n",
      "Epoch:  3509 \t Loss:  0.5431104\n",
      "Epoch:  3510 \t Loss:  0.54301316\n",
      "Epoch:  3511 \t Loss:  0.542916\n",
      "Epoch:  3512 \t Loss:  0.5428193\n",
      "Epoch:  3513 \t Loss:  0.5427216\n",
      "Epoch:  3514 \t Loss:  0.542625\n",
      "Epoch:  3515 \t Loss:  0.5425279\n",
      "Epoch:  3516 \t Loss:  0.5424311\n",
      "Epoch:  3517 \t Loss:  0.5423348\n",
      "Epoch:  3518 \t Loss:  0.5422381\n",
      "Epoch:  3519 \t Loss:  0.5421411\n",
      "Epoch:  3520 \t Loss:  0.54204476\n",
      "Epoch:  3521 \t Loss:  0.5419479\n",
      "Epoch:  3522 \t Loss:  0.5418516\n",
      "Epoch:  3523 \t Loss:  0.5417548\n",
      "Epoch:  3524 \t Loss:  0.5416577\n",
      "Epoch:  3525 \t Loss:  0.5415612\n",
      "Epoch:  3526 \t Loss:  0.54146487\n",
      "Epoch:  3527 \t Loss:  0.54136807\n",
      "Epoch:  3528 \t Loss:  0.54127127\n",
      "Epoch:  3529 \t Loss:  0.5411753\n",
      "Epoch:  3530 \t Loss:  0.5410794\n",
      "Epoch:  3531 \t Loss:  0.54098266\n",
      "Epoch:  3532 \t Loss:  0.54088664\n",
      "Epoch:  3533 \t Loss:  0.54079\n",
      "Epoch:  3534 \t Loss:  0.5406938\n",
      "Epoch:  3535 \t Loss:  0.54059815\n",
      "Epoch:  3536 \t Loss:  0.540502\n",
      "Epoch:  3537 \t Loss:  0.54040647\n",
      "Epoch:  3538 \t Loss:  0.5403102\n",
      "Epoch:  3539 \t Loss:  0.540214\n",
      "Epoch:  3540 \t Loss:  0.54011756\n",
      "Epoch:  3541 \t Loss:  0.54002166\n",
      "Epoch:  3542 \t Loss:  0.5399257\n",
      "Epoch:  3543 \t Loss:  0.5398297\n",
      "Epoch:  3544 \t Loss:  0.5397344\n",
      "Epoch:  3545 \t Loss:  0.53963834\n",
      "Epoch:  3546 \t Loss:  0.5395422\n",
      "Epoch:  3547 \t Loss:  0.53944635\n",
      "Epoch:  3548 \t Loss:  0.539351\n",
      "Epoch:  3549 \t Loss:  0.53925526\n",
      "Epoch:  3550 \t Loss:  0.5391596\n",
      "Epoch:  3551 \t Loss:  0.53906447\n",
      "Epoch:  3552 \t Loss:  0.53896916\n",
      "Epoch:  3553 \t Loss:  0.53887326\n",
      "Epoch:  3554 \t Loss:  0.5387778\n",
      "Epoch:  3555 \t Loss:  0.5386829\n",
      "Epoch:  3556 \t Loss:  0.53858733\n",
      "Epoch:  3557 \t Loss:  0.53849167\n",
      "Epoch:  3558 \t Loss:  0.5383964\n",
      "Epoch:  3559 \t Loss:  0.538301\n",
      "Epoch:  3560 \t Loss:  0.5382059\n",
      "Epoch:  3561 \t Loss:  0.5381109\n",
      "Epoch:  3562 \t Loss:  0.53801596\n",
      "Epoch:  3563 \t Loss:  0.5379204\n",
      "Epoch:  3564 \t Loss:  0.5378252\n",
      "Epoch:  3565 \t Loss:  0.53773\n",
      "Epoch:  3566 \t Loss:  0.5376358\n",
      "Epoch:  3567 \t Loss:  0.5375405\n",
      "Epoch:  3568 \t Loss:  0.5374455\n",
      "Epoch:  3569 \t Loss:  0.53735083\n",
      "Epoch:  3570 \t Loss:  0.5372557\n",
      "Epoch:  3571 \t Loss:  0.53716093\n",
      "Epoch:  3572 \t Loss:  0.5370658\n",
      "Epoch:  3573 \t Loss:  0.53697115\n",
      "Epoch:  3574 \t Loss:  0.53687626\n",
      "Epoch:  3575 \t Loss:  0.5367813\n",
      "Epoch:  3576 \t Loss:  0.53668696\n",
      "Epoch:  3577 \t Loss:  0.5365922\n",
      "Epoch:  3578 \t Loss:  0.5364984\n",
      "Epoch:  3579 \t Loss:  0.53640294\n",
      "Epoch:  3580 \t Loss:  0.53630877\n",
      "Epoch:  3581 \t Loss:  0.53621376\n",
      "Epoch:  3582 \t Loss:  0.53611994\n",
      "Epoch:  3583 \t Loss:  0.5360251\n",
      "Epoch:  3584 \t Loss:  0.53593117\n",
      "Epoch:  3585 \t Loss:  0.53583634\n",
      "Epoch:  3586 \t Loss:  0.53574175\n",
      "Epoch:  3587 \t Loss:  0.53564763\n",
      "Epoch:  3588 \t Loss:  0.5355529\n",
      "Epoch:  3589 \t Loss:  0.5354589\n",
      "Epoch:  3590 \t Loss:  0.535365\n",
      "Epoch:  3591 \t Loss:  0.53527045\n",
      "Epoch:  3592 \t Loss:  0.5351774\n",
      "Epoch:  3593 \t Loss:  0.53508306\n",
      "Epoch:  3594 \t Loss:  0.5349886\n",
      "Epoch:  3595 \t Loss:  0.53489476\n",
      "Epoch:  3596 \t Loss:  0.53480095\n",
      "Epoch:  3597 \t Loss:  0.5347069\n",
      "Epoch:  3598 \t Loss:  0.5346135\n",
      "Epoch:  3599 \t Loss:  0.5345192\n",
      "Epoch:  3600 \t Loss:  0.5344248\n",
      "Epoch:  3601 \t Loss:  0.5343309\n",
      "Epoch:  3602 \t Loss:  0.5342376\n",
      "Epoch:  3603 \t Loss:  0.53414375\n",
      "Epoch:  3604 \t Loss:  0.5340501\n",
      "Epoch:  3605 \t Loss:  0.5339567\n",
      "Epoch:  3606 \t Loss:  0.53386295\n",
      "Epoch:  3607 \t Loss:  0.5337692\n",
      "Epoch:  3608 \t Loss:  0.53367573\n",
      "Epoch:  3609 \t Loss:  0.5335824\n",
      "Epoch:  3610 \t Loss:  0.5334888\n",
      "Epoch:  3611 \t Loss:  0.53339577\n",
      "Epoch:  3612 \t Loss:  0.53330195\n",
      "Epoch:  3613 \t Loss:  0.53320855\n",
      "Epoch:  3614 \t Loss:  0.53311527\n",
      "Epoch:  3615 \t Loss:  0.5330218\n",
      "Epoch:  3616 \t Loss:  0.5329291\n",
      "Epoch:  3617 \t Loss:  0.5328353\n",
      "Epoch:  3618 \t Loss:  0.5327418\n",
      "Epoch:  3619 \t Loss:  0.53264904\n",
      "Epoch:  3620 \t Loss:  0.532556\n",
      "Epoch:  3621 \t Loss:  0.5324631\n",
      "Epoch:  3622 \t Loss:  0.5323703\n",
      "Epoch:  3623 \t Loss:  0.53227663\n",
      "Epoch:  3624 \t Loss:  0.5321831\n",
      "Epoch:  3625 \t Loss:  0.5320907\n",
      "Epoch:  3626 \t Loss:  0.5319979\n",
      "Epoch:  3627 \t Loss:  0.5319047\n",
      "Epoch:  3628 \t Loss:  0.53181225\n",
      "Epoch:  3629 \t Loss:  0.5317194\n",
      "Epoch:  3630 \t Loss:  0.5316259\n",
      "Epoch:  3631 \t Loss:  0.531533\n",
      "Epoch:  3632 \t Loss:  0.5314409\n",
      "Epoch:  3633 \t Loss:  0.5313482\n",
      "Epoch:  3634 \t Loss:  0.5312557\n",
      "Epoch:  3635 \t Loss:  0.5311625\n",
      "Epoch:  3636 \t Loss:  0.5310701\n",
      "Epoch:  3637 \t Loss:  0.53097785\n",
      "Epoch:  3638 \t Loss:  0.53088546\n",
      "Epoch:  3639 \t Loss:  0.5307924\n",
      "Epoch:  3640 \t Loss:  0.5307001\n",
      "Epoch:  3641 \t Loss:  0.53060794\n",
      "Epoch:  3642 \t Loss:  0.5305155\n",
      "Epoch:  3643 \t Loss:  0.53042316\n",
      "Epoch:  3644 \t Loss:  0.5303306\n",
      "Epoch:  3645 \t Loss:  0.53023905\n",
      "Epoch:  3646 \t Loss:  0.5301462\n",
      "Epoch:  3647 \t Loss:  0.5300539\n",
      "Epoch:  3648 \t Loss:  0.5299618\n",
      "Epoch:  3649 \t Loss:  0.52986985\n",
      "Epoch:  3650 \t Loss:  0.52977794\n",
      "Epoch:  3651 \t Loss:  0.5296858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3652 \t Loss:  0.52959365\n",
      "Epoch:  3653 \t Loss:  0.529501\n",
      "Epoch:  3654 \t Loss:  0.52940935\n",
      "Epoch:  3655 \t Loss:  0.52931786\n",
      "Epoch:  3656 \t Loss:  0.52922565\n",
      "Epoch:  3657 \t Loss:  0.5291337\n",
      "Epoch:  3658 \t Loss:  0.52904195\n",
      "Epoch:  3659 \t Loss:  0.5289501\n",
      "Epoch:  3660 \t Loss:  0.5288579\n",
      "Epoch:  3661 \t Loss:  0.5287667\n",
      "Epoch:  3662 \t Loss:  0.5286751\n",
      "Epoch:  3663 \t Loss:  0.5285832\n",
      "Epoch:  3664 \t Loss:  0.5284912\n",
      "Epoch:  3665 \t Loss:  0.5283999\n",
      "Epoch:  3666 \t Loss:  0.5283079\n",
      "Epoch:  3667 \t Loss:  0.5282169\n",
      "Epoch:  3668 \t Loss:  0.5281255\n",
      "Epoch:  3669 \t Loss:  0.5280336\n",
      "Epoch:  3670 \t Loss:  0.5279426\n",
      "Epoch:  3671 \t Loss:  0.5278508\n",
      "Epoch:  3672 \t Loss:  0.5277601\n",
      "Epoch:  3673 \t Loss:  0.527669\n",
      "Epoch:  3674 \t Loss:  0.5275775\n",
      "Epoch:  3675 \t Loss:  0.5274863\n",
      "Epoch:  3676 \t Loss:  0.5273951\n",
      "Epoch:  3677 \t Loss:  0.5273038\n",
      "Epoch:  3678 \t Loss:  0.52721226\n",
      "Epoch:  3679 \t Loss:  0.52712125\n",
      "Epoch:  3680 \t Loss:  0.5270304\n",
      "Epoch:  3681 \t Loss:  0.52693886\n",
      "Epoch:  3682 \t Loss:  0.52684814\n",
      "Epoch:  3683 \t Loss:  0.526757\n",
      "Epoch:  3684 \t Loss:  0.5266657\n",
      "Epoch:  3685 \t Loss:  0.5265751\n",
      "Epoch:  3686 \t Loss:  0.5264842\n",
      "Epoch:  3687 \t Loss:  0.52639353\n",
      "Epoch:  3688 \t Loss:  0.5263025\n",
      "Epoch:  3689 \t Loss:  0.52621156\n",
      "Epoch:  3690 \t Loss:  0.52612084\n",
      "Epoch:  3691 \t Loss:  0.52603\n",
      "Epoch:  3692 \t Loss:  0.5259397\n",
      "Epoch:  3693 \t Loss:  0.5258492\n",
      "Epoch:  3694 \t Loss:  0.5257585\n",
      "Epoch:  3695 \t Loss:  0.525668\n",
      "Epoch:  3696 \t Loss:  0.52557707\n",
      "Epoch:  3697 \t Loss:  0.52548665\n",
      "Epoch:  3698 \t Loss:  0.52539665\n",
      "Epoch:  3699 \t Loss:  0.52530587\n",
      "Epoch:  3700 \t Loss:  0.52521515\n",
      "Epoch:  3701 \t Loss:  0.5251248\n",
      "Epoch:  3702 \t Loss:  0.52503467\n",
      "Epoch:  3703 \t Loss:  0.52494425\n",
      "Epoch:  3704 \t Loss:  0.52485365\n",
      "Epoch:  3705 \t Loss:  0.5247634\n",
      "Epoch:  3706 \t Loss:  0.5246731\n",
      "Epoch:  3707 \t Loss:  0.524583\n",
      "Epoch:  3708 \t Loss:  0.52449316\n",
      "Epoch:  3709 \t Loss:  0.5244028\n",
      "Epoch:  3710 \t Loss:  0.52431196\n",
      "Epoch:  3711 \t Loss:  0.52422255\n",
      "Epoch:  3712 \t Loss:  0.5241325\n",
      "Epoch:  3713 \t Loss:  0.52404267\n",
      "Epoch:  3714 \t Loss:  0.5239524\n",
      "Epoch:  3715 \t Loss:  0.5238625\n",
      "Epoch:  3716 \t Loss:  0.52377254\n",
      "Epoch:  3717 \t Loss:  0.5236825\n",
      "Epoch:  3718 \t Loss:  0.5235933\n",
      "Epoch:  3719 \t Loss:  0.5235031\n",
      "Epoch:  3720 \t Loss:  0.52341336\n",
      "Epoch:  3721 \t Loss:  0.5233236\n",
      "Epoch:  3722 \t Loss:  0.52323407\n",
      "Epoch:  3723 \t Loss:  0.5231443\n",
      "Epoch:  3724 \t Loss:  0.52305424\n",
      "Epoch:  3725 \t Loss:  0.5229649\n",
      "Epoch:  3726 \t Loss:  0.52287525\n",
      "Epoch:  3727 \t Loss:  0.5227856\n",
      "Epoch:  3728 \t Loss:  0.52269566\n",
      "Epoch:  3729 \t Loss:  0.52260625\n",
      "Epoch:  3730 \t Loss:  0.5225171\n",
      "Epoch:  3731 \t Loss:  0.5224278\n",
      "Epoch:  3732 \t Loss:  0.5223382\n",
      "Epoch:  3733 \t Loss:  0.52224857\n",
      "Epoch:  3734 \t Loss:  0.5221594\n",
      "Epoch:  3735 \t Loss:  0.5220707\n",
      "Epoch:  3736 \t Loss:  0.52198166\n",
      "Epoch:  3737 \t Loss:  0.5218914\n",
      "Epoch:  3738 \t Loss:  0.5218028\n",
      "Epoch:  3739 \t Loss:  0.52171326\n",
      "Epoch:  3740 \t Loss:  0.5216246\n",
      "Epoch:  3741 \t Loss:  0.52153516\n",
      "Epoch:  3742 \t Loss:  0.5214462\n",
      "Epoch:  3743 \t Loss:  0.5213571\n",
      "Epoch:  3744 \t Loss:  0.5212679\n",
      "Epoch:  3745 \t Loss:  0.52117944\n",
      "Epoch:  3746 \t Loss:  0.52109057\n",
      "Epoch:  3747 \t Loss:  0.52100134\n",
      "Epoch:  3748 \t Loss:  0.52091235\n",
      "Epoch:  3749 \t Loss:  0.5208234\n",
      "Epoch:  3750 \t Loss:  0.52073544\n",
      "Epoch:  3751 \t Loss:  0.5206461\n",
      "Epoch:  3752 \t Loss:  0.5205575\n",
      "Epoch:  3753 \t Loss:  0.5204685\n",
      "Epoch:  3754 \t Loss:  0.52037984\n",
      "Epoch:  3755 \t Loss:  0.52029127\n",
      "Epoch:  3756 \t Loss:  0.5202026\n",
      "Epoch:  3757 \t Loss:  0.520114\n",
      "Epoch:  3758 \t Loss:  0.52002573\n",
      "Epoch:  3759 \t Loss:  0.5199372\n",
      "Epoch:  3760 \t Loss:  0.51984876\n",
      "Epoch:  3761 \t Loss:  0.51976013\n",
      "Epoch:  3762 \t Loss:  0.5196714\n",
      "Epoch:  3763 \t Loss:  0.519583\n",
      "Epoch:  3764 \t Loss:  0.51949525\n",
      "Epoch:  3765 \t Loss:  0.5194064\n",
      "Epoch:  3766 \t Loss:  0.51931816\n",
      "Epoch:  3767 \t Loss:  0.5192298\n",
      "Epoch:  3768 \t Loss:  0.5191423\n",
      "Epoch:  3769 \t Loss:  0.5190531\n",
      "Epoch:  3770 \t Loss:  0.51896536\n",
      "Epoch:  3771 \t Loss:  0.5188771\n",
      "Epoch:  3772 \t Loss:  0.51878935\n",
      "Epoch:  3773 \t Loss:  0.5187011\n",
      "Epoch:  3774 \t Loss:  0.5186131\n",
      "Epoch:  3775 \t Loss:  0.5185255\n",
      "Epoch:  3776 \t Loss:  0.5184371\n",
      "Epoch:  3777 \t Loss:  0.51834935\n",
      "Epoch:  3778 \t Loss:  0.5182607\n",
      "Epoch:  3779 \t Loss:  0.51817274\n",
      "Epoch:  3780 \t Loss:  0.5180848\n",
      "Epoch:  3781 \t Loss:  0.51799655\n",
      "Epoch:  3782 \t Loss:  0.5179097\n",
      "Epoch:  3783 \t Loss:  0.51782197\n",
      "Epoch:  3784 \t Loss:  0.5177343\n",
      "Epoch:  3785 \t Loss:  0.5176463\n",
      "Epoch:  3786 \t Loss:  0.51755905\n",
      "Epoch:  3787 \t Loss:  0.517471\n",
      "Epoch:  3788 \t Loss:  0.5173835\n",
      "Epoch:  3789 \t Loss:  0.5172961\n",
      "Epoch:  3790 \t Loss:  0.51720804\n",
      "Epoch:  3791 \t Loss:  0.51712143\n",
      "Epoch:  3792 \t Loss:  0.5170333\n",
      "Epoch:  3793 \t Loss:  0.5169455\n",
      "Epoch:  3794 \t Loss:  0.516859\n",
      "Epoch:  3795 \t Loss:  0.51677144\n",
      "Epoch:  3796 \t Loss:  0.5166839\n",
      "Epoch:  3797 \t Loss:  0.5165963\n",
      "Epoch:  3798 \t Loss:  0.5165093\n",
      "Epoch:  3799 \t Loss:  0.5164218\n",
      "Epoch:  3800 \t Loss:  0.5163346\n",
      "Epoch:  3801 \t Loss:  0.51624775\n",
      "Epoch:  3802 \t Loss:  0.5161605\n",
      "Epoch:  3803 \t Loss:  0.51607305\n",
      "Epoch:  3804 \t Loss:  0.51598614\n",
      "Epoch:  3805 \t Loss:  0.5158991\n",
      "Epoch:  3806 \t Loss:  0.51581174\n",
      "Epoch:  3807 \t Loss:  0.515725\n",
      "Epoch:  3808 \t Loss:  0.51563764\n",
      "Epoch:  3809 \t Loss:  0.51555014\n",
      "Epoch:  3810 \t Loss:  0.51546395\n",
      "Epoch:  3811 \t Loss:  0.5153773\n",
      "Epoch:  3812 \t Loss:  0.5152903\n",
      "Epoch:  3813 \t Loss:  0.515204\n",
      "Epoch:  3814 \t Loss:  0.5151166\n",
      "Epoch:  3815 \t Loss:  0.5150292\n",
      "Epoch:  3816 \t Loss:  0.5149431\n",
      "Epoch:  3817 \t Loss:  0.5148563\n",
      "Epoch:  3818 \t Loss:  0.5147694\n",
      "Epoch:  3819 \t Loss:  0.5146827\n",
      "Epoch:  3820 \t Loss:  0.5145965\n",
      "Epoch:  3821 \t Loss:  0.5145098\n",
      "Epoch:  3822 \t Loss:  0.5144231\n",
      "Epoch:  3823 \t Loss:  0.5143372\n",
      "Epoch:  3824 \t Loss:  0.51425\n",
      "Epoch:  3825 \t Loss:  0.51416385\n",
      "Epoch:  3826 \t Loss:  0.51407737\n",
      "Epoch:  3827 \t Loss:  0.5139909\n",
      "Epoch:  3828 \t Loss:  0.5139045\n",
      "Epoch:  3829 \t Loss:  0.51381826\n",
      "Epoch:  3830 \t Loss:  0.51373196\n",
      "Epoch:  3831 \t Loss:  0.5136457\n",
      "Epoch:  3832 \t Loss:  0.5135595\n",
      "Epoch:  3833 \t Loss:  0.5134737\n",
      "Epoch:  3834 \t Loss:  0.5133872\n",
      "Epoch:  3835 \t Loss:  0.5133004\n",
      "Epoch:  3836 \t Loss:  0.513215\n",
      "Epoch:  3837 \t Loss:  0.5131288\n",
      "Epoch:  3838 \t Loss:  0.5130428\n",
      "Epoch:  3839 \t Loss:  0.5129564\n",
      "Epoch:  3840 \t Loss:  0.5128699\n",
      "Epoch:  3841 \t Loss:  0.51278484\n",
      "Epoch:  3842 \t Loss:  0.5126982\n",
      "Epoch:  3843 \t Loss:  0.5126123\n",
      "Epoch:  3844 \t Loss:  0.5125261\n",
      "Epoch:  3845 \t Loss:  0.51244086\n",
      "Epoch:  3846 \t Loss:  0.51235485\n",
      "Epoch:  3847 \t Loss:  0.5122694\n",
      "Epoch:  3848 \t Loss:  0.51218367\n",
      "Epoch:  3849 \t Loss:  0.5120975\n",
      "Epoch:  3850 \t Loss:  0.51201135\n",
      "Epoch:  3851 \t Loss:  0.5119266\n",
      "Epoch:  3852 \t Loss:  0.5118407\n",
      "Epoch:  3853 \t Loss:  0.5117548\n",
      "Epoch:  3854 \t Loss:  0.5116697\n",
      "Epoch:  3855 \t Loss:  0.5115838\n",
      "Epoch:  3856 \t Loss:  0.5114987\n",
      "Epoch:  3857 \t Loss:  0.5114128\n",
      "Epoch:  3858 \t Loss:  0.5113279\n",
      "Epoch:  3859 \t Loss:  0.5112419\n",
      "Epoch:  3860 \t Loss:  0.5111571\n",
      "Epoch:  3861 \t Loss:  0.5110721\n",
      "Epoch:  3862 \t Loss:  0.5109863\n",
      "Epoch:  3863 \t Loss:  0.51090133\n",
      "Epoch:  3864 \t Loss:  0.5108155\n",
      "Epoch:  3865 \t Loss:  0.5107304\n",
      "Epoch:  3866 \t Loss:  0.51064515\n",
      "Epoch:  3867 \t Loss:  0.51056015\n",
      "Epoch:  3868 \t Loss:  0.5104749\n",
      "Epoch:  3869 \t Loss:  0.51038986\n",
      "Epoch:  3870 \t Loss:  0.5103042\n",
      "Epoch:  3871 \t Loss:  0.5102193\n",
      "Epoch:  3872 \t Loss:  0.5101341\n",
      "Epoch:  3873 \t Loss:  0.5100491\n",
      "Epoch:  3874 \t Loss:  0.5099639\n",
      "Epoch:  3875 \t Loss:  0.50987947\n",
      "Epoch:  3876 \t Loss:  0.50979435\n",
      "Epoch:  3877 \t Loss:  0.5097101\n",
      "Epoch:  3878 \t Loss:  0.50962484\n",
      "Epoch:  3879 \t Loss:  0.50953996\n",
      "Epoch:  3880 \t Loss:  0.5094546\n",
      "Epoch:  3881 \t Loss:  0.5093699\n",
      "Epoch:  3882 \t Loss:  0.5092858\n",
      "Epoch:  3883 \t Loss:  0.50920147\n",
      "Epoch:  3884 \t Loss:  0.50911635\n",
      "Epoch:  3885 \t Loss:  0.50903153\n",
      "Epoch:  3886 \t Loss:  0.5089469\n",
      "Epoch:  3887 \t Loss:  0.50886184\n",
      "Epoch:  3888 \t Loss:  0.5087779\n",
      "Epoch:  3889 \t Loss:  0.50869346\n",
      "Epoch:  3890 \t Loss:  0.5086088\n",
      "Epoch:  3891 \t Loss:  0.5085241\n",
      "Epoch:  3892 \t Loss:  0.5084401\n",
      "Epoch:  3893 \t Loss:  0.5083558\n",
      "Epoch:  3894 \t Loss:  0.50827116\n",
      "Epoch:  3895 \t Loss:  0.5081868\n",
      "Epoch:  3896 \t Loss:  0.5081028\n",
      "Epoch:  3897 \t Loss:  0.508018\n",
      "Epoch:  3898 \t Loss:  0.50793374\n",
      "Epoch:  3899 \t Loss:  0.50784963\n",
      "Epoch:  3900 \t Loss:  0.5077649\n",
      "Epoch:  3901 \t Loss:  0.50768155\n",
      "Epoch:  3902 \t Loss:  0.50759655\n",
      "Epoch:  3903 \t Loss:  0.50751245\n",
      "Epoch:  3904 \t Loss:  0.5074285\n",
      "Epoch:  3905 \t Loss:  0.5073448\n",
      "Epoch:  3906 \t Loss:  0.50726104\n",
      "Epoch:  3907 \t Loss:  0.5071764\n",
      "Epoch:  3908 \t Loss:  0.5070926\n",
      "Epoch:  3909 \t Loss:  0.50700885\n",
      "Epoch:  3910 \t Loss:  0.5069251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3911 \t Loss:  0.50684077\n",
      "Epoch:  3912 \t Loss:  0.50675756\n",
      "Epoch:  3913 \t Loss:  0.50667334\n",
      "Epoch:  3914 \t Loss:  0.5065895\n",
      "Epoch:  3915 \t Loss:  0.5065056\n",
      "Epoch:  3916 \t Loss:  0.5064218\n",
      "Epoch:  3917 \t Loss:  0.5063383\n",
      "Epoch:  3918 \t Loss:  0.50625414\n",
      "Epoch:  3919 \t Loss:  0.50617045\n",
      "Epoch:  3920 \t Loss:  0.5060868\n",
      "Epoch:  3921 \t Loss:  0.5060036\n",
      "Epoch:  3922 \t Loss:  0.50592\n",
      "Epoch:  3923 \t Loss:  0.5058363\n",
      "Epoch:  3924 \t Loss:  0.5057528\n",
      "Epoch:  3925 \t Loss:  0.5056696\n",
      "Epoch:  3926 \t Loss:  0.5055861\n",
      "Epoch:  3927 \t Loss:  0.5055028\n",
      "Epoch:  3928 \t Loss:  0.5054192\n",
      "Epoch:  3929 \t Loss:  0.5053361\n",
      "Epoch:  3930 \t Loss:  0.50525266\n",
      "Epoch:  3931 \t Loss:  0.5051692\n",
      "Epoch:  3932 \t Loss:  0.50508577\n",
      "Epoch:  3933 \t Loss:  0.50500256\n",
      "Epoch:  3934 \t Loss:  0.50491947\n",
      "Epoch:  3935 \t Loss:  0.50483596\n",
      "Epoch:  3936 \t Loss:  0.5047531\n",
      "Epoch:  3937 \t Loss:  0.5046694\n",
      "Epoch:  3938 \t Loss:  0.5045872\n",
      "Epoch:  3939 \t Loss:  0.5045037\n",
      "Epoch:  3940 \t Loss:  0.50442064\n",
      "Epoch:  3941 \t Loss:  0.50433755\n",
      "Epoch:  3942 \t Loss:  0.5042544\n",
      "Epoch:  3943 \t Loss:  0.50417143\n",
      "Epoch:  3944 \t Loss:  0.504089\n",
      "Epoch:  3945 \t Loss:  0.50400573\n",
      "Epoch:  3946 \t Loss:  0.5039234\n",
      "Epoch:  3947 \t Loss:  0.5038404\n",
      "Epoch:  3948 \t Loss:  0.5037576\n",
      "Epoch:  3949 \t Loss:  0.50367475\n",
      "Epoch:  3950 \t Loss:  0.503592\n",
      "Epoch:  3951 \t Loss:  0.5035094\n",
      "Epoch:  3952 \t Loss:  0.50342655\n",
      "Epoch:  3953 \t Loss:  0.5033435\n",
      "Epoch:  3954 \t Loss:  0.50326115\n",
      "Epoch:  3955 \t Loss:  0.50317854\n",
      "Epoch:  3956 \t Loss:  0.50309616\n",
      "Epoch:  3957 \t Loss:  0.5030134\n",
      "Epoch:  3958 \t Loss:  0.50293046\n",
      "Epoch:  3959 \t Loss:  0.50284815\n",
      "Epoch:  3960 \t Loss:  0.50276595\n",
      "Epoch:  3961 \t Loss:  0.502683\n",
      "Epoch:  3962 \t Loss:  0.50260085\n",
      "Epoch:  3963 \t Loss:  0.5025185\n",
      "Epoch:  3964 \t Loss:  0.50243616\n",
      "Epoch:  3965 \t Loss:  0.5023536\n",
      "Epoch:  3966 \t Loss:  0.50227123\n",
      "Epoch:  3967 \t Loss:  0.5021889\n",
      "Epoch:  3968 \t Loss:  0.50210667\n",
      "Epoch:  3969 \t Loss:  0.50202405\n",
      "Epoch:  3970 \t Loss:  0.5019416\n",
      "Epoch:  3971 \t Loss:  0.50186026\n",
      "Epoch:  3972 \t Loss:  0.5017782\n",
      "Epoch:  3973 \t Loss:  0.501696\n",
      "Epoch:  3974 \t Loss:  0.5016141\n",
      "Epoch:  3975 \t Loss:  0.50153196\n",
      "Epoch:  3976 \t Loss:  0.50144976\n",
      "Epoch:  3977 \t Loss:  0.50136715\n",
      "Epoch:  3978 \t Loss:  0.50128573\n",
      "Epoch:  3979 \t Loss:  0.5012038\n",
      "Epoch:  3980 \t Loss:  0.5011222\n",
      "Epoch:  3981 \t Loss:  0.5010402\n",
      "Epoch:  3982 \t Loss:  0.5009585\n",
      "Epoch:  3983 \t Loss:  0.5008771\n",
      "Epoch:  3984 \t Loss:  0.5007948\n",
      "Epoch:  3985 \t Loss:  0.50071317\n",
      "Epoch:  3986 \t Loss:  0.5006313\n",
      "Epoch:  3987 \t Loss:  0.5005491\n",
      "Epoch:  3988 \t Loss:  0.50046736\n",
      "Epoch:  3989 \t Loss:  0.5003861\n",
      "Epoch:  3990 \t Loss:  0.50030404\n",
      "Epoch:  3991 \t Loss:  0.500223\n",
      "Epoch:  3992 \t Loss:  0.50014174\n",
      "Epoch:  3993 \t Loss:  0.5000598\n",
      "Epoch:  3994 \t Loss:  0.49997827\n",
      "Epoch:  3995 \t Loss:  0.49989676\n",
      "Epoch:  3996 \t Loss:  0.49981555\n",
      "Epoch:  3997 \t Loss:  0.49973384\n",
      "Epoch:  3998 \t Loss:  0.49965298\n",
      "Epoch:  3999 \t Loss:  0.49957156\n",
      "Epoch:  4000 \t Loss:  0.49948972\n",
      "Epoch:  4001 \t Loss:  0.49940825\n",
      "Epoch:  4002 \t Loss:  0.4993275\n",
      "Epoch:  4003 \t Loss:  0.49924636\n",
      "Epoch:  4004 \t Loss:  0.49916488\n",
      "Epoch:  4005 \t Loss:  0.4990834\n",
      "Epoch:  4006 \t Loss:  0.49900204\n",
      "Epoch:  4007 \t Loss:  0.498921\n",
      "Epoch:  4008 \t Loss:  0.49884006\n",
      "Epoch:  4009 \t Loss:  0.49875882\n",
      "Epoch:  4010 \t Loss:  0.49867767\n",
      "Epoch:  4011 \t Loss:  0.4985965\n",
      "Epoch:  4012 \t Loss:  0.49851516\n",
      "Epoch:  4013 \t Loss:  0.49843445\n",
      "Epoch:  4014 \t Loss:  0.49835363\n",
      "Epoch:  4015 \t Loss:  0.4982729\n",
      "Epoch:  4016 \t Loss:  0.49819234\n",
      "Epoch:  4017 \t Loss:  0.4981111\n",
      "Epoch:  4018 \t Loss:  0.49803016\n",
      "Epoch:  4019 \t Loss:  0.49794993\n",
      "Epoch:  4020 \t Loss:  0.49786898\n",
      "Epoch:  4021 \t Loss:  0.49778774\n",
      "Epoch:  4022 \t Loss:  0.4977073\n",
      "Epoch:  4023 \t Loss:  0.49762627\n",
      "Epoch:  4024 \t Loss:  0.49754563\n",
      "Epoch:  4025 \t Loss:  0.49746493\n",
      "Epoch:  4026 \t Loss:  0.49738437\n",
      "Epoch:  4027 \t Loss:  0.49730426\n",
      "Epoch:  4028 \t Loss:  0.4972232\n",
      "Epoch:  4029 \t Loss:  0.49714273\n",
      "Epoch:  4030 \t Loss:  0.4970618\n",
      "Epoch:  4031 \t Loss:  0.49698177\n",
      "Epoch:  4032 \t Loss:  0.49690118\n",
      "Epoch:  4033 \t Loss:  0.4968203\n",
      "Epoch:  4034 \t Loss:  0.49673977\n",
      "Epoch:  4035 \t Loss:  0.49665958\n",
      "Epoch:  4036 \t Loss:  0.4965795\n",
      "Epoch:  4037 \t Loss:  0.4964989\n",
      "Epoch:  4038 \t Loss:  0.49641868\n",
      "Epoch:  4039 \t Loss:  0.49633867\n",
      "Epoch:  4040 \t Loss:  0.49625757\n",
      "Epoch:  4041 \t Loss:  0.49617833\n",
      "Epoch:  4042 \t Loss:  0.49609786\n",
      "Epoch:  4043 \t Loss:  0.49601772\n",
      "Epoch:  4044 \t Loss:  0.49593723\n",
      "Epoch:  4045 \t Loss:  0.49585745\n",
      "Epoch:  4046 \t Loss:  0.49577734\n",
      "Epoch:  4047 \t Loss:  0.49569735\n",
      "Epoch:  4048 \t Loss:  0.4956175\n",
      "Epoch:  4049 \t Loss:  0.4955376\n",
      "Epoch:  4050 \t Loss:  0.49545765\n",
      "Epoch:  4051 \t Loss:  0.49537694\n",
      "Epoch:  4052 \t Loss:  0.49529734\n",
      "Epoch:  4053 \t Loss:  0.49521768\n",
      "Epoch:  4054 \t Loss:  0.4951377\n",
      "Epoch:  4055 \t Loss:  0.49505743\n",
      "Epoch:  4056 \t Loss:  0.49497813\n",
      "Epoch:  4057 \t Loss:  0.49489796\n",
      "Epoch:  4058 \t Loss:  0.49481788\n",
      "Epoch:  4059 \t Loss:  0.49473855\n",
      "Epoch:  4060 \t Loss:  0.49465883\n",
      "Epoch:  4061 \t Loss:  0.49457937\n",
      "Epoch:  4062 \t Loss:  0.49449953\n",
      "Epoch:  4063 \t Loss:  0.49442023\n",
      "Epoch:  4064 \t Loss:  0.49434015\n",
      "Epoch:  4065 \t Loss:  0.49426073\n",
      "Epoch:  4066 \t Loss:  0.494181\n",
      "Epoch:  4067 \t Loss:  0.4941014\n",
      "Epoch:  4068 \t Loss:  0.4940225\n",
      "Epoch:  4069 \t Loss:  0.4939426\n",
      "Epoch:  4070 \t Loss:  0.49386314\n",
      "Epoch:  4071 \t Loss:  0.49378383\n",
      "Epoch:  4072 \t Loss:  0.49370486\n",
      "Epoch:  4073 \t Loss:  0.49362493\n",
      "Epoch:  4074 \t Loss:  0.4935464\n",
      "Epoch:  4075 \t Loss:  0.4934667\n",
      "Epoch:  4076 \t Loss:  0.49338725\n",
      "Epoch:  4077 \t Loss:  0.4933078\n",
      "Epoch:  4078 \t Loss:  0.49322876\n",
      "Epoch:  4079 \t Loss:  0.49314985\n",
      "Epoch:  4080 \t Loss:  0.49307063\n",
      "Epoch:  4081 \t Loss:  0.4929911\n",
      "Epoch:  4082 \t Loss:  0.4929119\n",
      "Epoch:  4083 \t Loss:  0.49283296\n",
      "Epoch:  4084 \t Loss:  0.4927539\n",
      "Epoch:  4085 \t Loss:  0.49267462\n",
      "Epoch:  4086 \t Loss:  0.4925959\n",
      "Epoch:  4087 \t Loss:  0.49251658\n",
      "Epoch:  4088 \t Loss:  0.49243805\n",
      "Epoch:  4089 \t Loss:  0.49235895\n",
      "Epoch:  4090 \t Loss:  0.49228027\n",
      "Epoch:  4091 \t Loss:  0.49220195\n",
      "Epoch:  4092 \t Loss:  0.49212298\n",
      "Epoch:  4093 \t Loss:  0.49204376\n",
      "Epoch:  4094 \t Loss:  0.49196476\n",
      "Epoch:  4095 \t Loss:  0.49188608\n",
      "Epoch:  4096 \t Loss:  0.49180734\n",
      "Epoch:  4097 \t Loss:  0.4917291\n",
      "Epoch:  4098 \t Loss:  0.49165022\n",
      "Epoch:  4099 \t Loss:  0.4915714\n",
      "Epoch:  4100 \t Loss:  0.49149233\n",
      "Epoch:  4101 \t Loss:  0.49141422\n",
      "Epoch:  4102 \t Loss:  0.4913353\n",
      "Epoch:  4103 \t Loss:  0.4912566\n",
      "Epoch:  4104 \t Loss:  0.4911779\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        _, train_loss = sess.run([train_op, loss], feed_dict={X: mean_centered_ratings_matrix})\n",
    "        print(\"Epoch: \", epoch, \"\\t Loss: \", train_loss)\n",
    "        \n",
    "    u = sess.run(U)\n",
    "    v = sess.run(V)\n",
    "    print(np.matmul(u, np.transpose(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.38970588 -0.61029412  0.38970588 ...         nan         nan\n",
      "          nan]\n",
      " [ 0.29032258         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " [        nan         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " ...\n",
      " [ 0.95454545         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " [        nan         nan         nan ...         nan         nan\n",
      "          nan]\n",
      " [        nan  1.58928571         nan ...         nan         nan\n",
      "          nan]]\n"
     ]
    }
   ],
   "source": [
    "print(mean_centered_ratings_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May be overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Minimize\\ J = \\dfrac{1}{2} \\sum_{(i, j) \\in S} e_{ij}^2 + \\dfrac{\\lambda}{2} \\sum_{i = 1}^{m} \\sum_{s = 1}^{k} u_{is}^2 + \\dfrac{\\lambda}{2} \\sum_{j = 1}^{n} \\sum_{s = 1}^{k} v_{js}^2 = \\dfrac{1}{2} \\sum_{(i, j) \\in S} (r_{ij} - \\sum_{s=1}^k u_{is}v_{js})^2 + \\dfrac{\\lambda}{2} \\sum_{i = 1}^{m} \\sum_{s = 1}^{k} u_{is}^2 + \\dfrac{\\lambda}{2} \\sum_{j = 1}^{n} \\sum_{s = 1}^{k} v_{js}^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[n_users, n_items], name='X')\n",
    "# X = tf.Variable(initial_value=mean_centered_ratings_matrix, trainable=False)\n",
    "# indices = tf.where(tf.is_finite(X))\n",
    "\n",
    "U = tf.Variable(tf.random_uniform(shape=[n_users, k]))\n",
    "V = tf.Variable(tf.random_uniform(shape=[n_items, k]))\n",
    "\n",
    "outputs = tf.matmul(U, tf.transpose(V))\n",
    "\n",
    "loss = loss_on_observed_value(X, outputs)\n",
    "reg_losses = tf.nn.l2_loss(U) + tf.nn.l2_loss(V)\n",
    "\n",
    "loss = loss + 0.01*reg_losses\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=1).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \t Loss:  360.7362\n",
      "Epoch:  1 \t Loss:  333.51807\n",
      "Epoch:  2 \t Loss:  309.82025\n",
      "Epoch:  3 \t Loss:  289.0325\n",
      "Epoch:  4 \t Loss:  270.67242\n",
      "Epoch:  5 \t Loss:  254.35492\n",
      "Epoch:  6 \t Loss:  239.76944\n",
      "Epoch:  7 \t Loss:  226.66225\n",
      "Epoch:  8 \t Loss:  214.82568\n",
      "Epoch:  9 \t Loss:  204.08765\n",
      "Epoch:  10 \t Loss:  194.30458\n",
      "Epoch:  11 \t Loss:  185.35597\n",
      "Epoch:  12 \t Loss:  177.14047\n",
      "Epoch:  13 \t Loss:  169.57133\n",
      "Epoch:  14 \t Loss:  162.57494\n",
      "Epoch:  15 \t Loss:  156.08806\n",
      "Epoch:  16 \t Loss:  150.05609\n",
      "Epoch:  17 \t Loss:  144.43178\n",
      "Epoch:  18 \t Loss:  139.1742\n",
      "Epoch:  19 \t Loss:  134.24745\n",
      "Epoch:  20 \t Loss:  129.62001\n",
      "Epoch:  21 \t Loss:  125.2643\n",
      "Epoch:  22 \t Loss:  121.15607\n",
      "Epoch:  23 \t Loss:  117.27352\n",
      "Epoch:  24 \t Loss:  113.59768\n",
      "Epoch:  25 \t Loss:  110.111565\n",
      "Epoch:  26 \t Loss:  106.79974\n",
      "Epoch:  27 \t Loss:  103.648766\n",
      "Epoch:  28 \t Loss:  100.64629\n",
      "Epoch:  29 \t Loss:  97.781334\n",
      "Epoch:  30 \t Loss:  95.04402\n",
      "Epoch:  31 \t Loss:  92.425385\n",
      "Epoch:  32 \t Loss:  89.91728\n",
      "Epoch:  33 \t Loss:  87.51229\n",
      "Epoch:  34 \t Loss:  85.2037\n",
      "Epoch:  35 \t Loss:  82.98547\n",
      "Epoch:  36 \t Loss:  80.85201\n",
      "Epoch:  37 \t Loss:  78.798096\n",
      "Epoch:  38 \t Loss:  76.81918\n",
      "Epoch:  39 \t Loss:  74.91087\n",
      "Epoch:  40 \t Loss:  73.069305\n",
      "Epoch:  41 \t Loss:  71.290665\n",
      "Epoch:  42 \t Loss:  69.57177\n",
      "Epoch:  43 \t Loss:  67.90935\n",
      "Epoch:  44 \t Loss:  66.30064\n",
      "Epoch:  45 \t Loss:  64.74296\n",
      "Epoch:  46 \t Loss:  63.23374\n",
      "Epoch:  47 \t Loss:  61.770798\n",
      "Epoch:  48 \t Loss:  60.35183\n",
      "Epoch:  49 \t Loss:  58.974953\n",
      "Epoch:  50 \t Loss:  57.638313\n",
      "Epoch:  51 \t Loss:  56.34007\n",
      "Epoch:  52 \t Loss:  55.078568\n",
      "Epoch:  53 \t Loss:  53.85236\n",
      "Epoch:  54 \t Loss:  52.659977\n",
      "Epoch:  55 \t Loss:  51.500114\n",
      "Epoch:  56 \t Loss:  50.37137\n",
      "Epoch:  57 \t Loss:  49.27266\n",
      "Epoch:  58 \t Loss:  48.202705\n",
      "Epoch:  59 \t Loss:  47.1606\n",
      "Epoch:  60 \t Loss:  46.145275\n",
      "Epoch:  61 \t Loss:  45.155712\n",
      "Epoch:  62 \t Loss:  44.191063\n",
      "Epoch:  63 \t Loss:  43.250504\n",
      "Epoch:  64 \t Loss:  42.333107\n",
      "Epoch:  65 \t Loss:  41.43822\n",
      "Epoch:  66 \t Loss:  40.565002\n",
      "Epoch:  67 \t Loss:  39.712814\n",
      "Epoch:  68 \t Loss:  38.880966\n",
      "Epoch:  69 \t Loss:  38.068855\n",
      "Epoch:  70 \t Loss:  37.275814\n",
      "Epoch:  71 \t Loss:  36.50129\n",
      "Epoch:  72 \t Loss:  35.744736\n",
      "Epoch:  73 \t Loss:  35.005592\n",
      "Epoch:  74 \t Loss:  34.283386\n",
      "Epoch:  75 \t Loss:  33.57758\n",
      "Epoch:  76 \t Loss:  32.887768\n",
      "Epoch:  77 \t Loss:  32.213474\n",
      "Epoch:  78 \t Loss:  31.554255\n",
      "Epoch:  79 \t Loss:  30.909681\n",
      "Epoch:  80 \t Loss:  30.279406\n",
      "Epoch:  81 \t Loss:  29.663017\n",
      "Epoch:  82 \t Loss:  29.060148\n",
      "Epoch:  83 \t Loss:  28.470434\n",
      "Epoch:  84 \t Loss:  27.893545\n",
      "Epoch:  85 \t Loss:  27.32914\n",
      "Epoch:  86 \t Loss:  26.776909\n",
      "Epoch:  87 \t Loss:  26.23655\n",
      "Epoch:  88 \t Loss:  25.707754\n",
      "Epoch:  89 \t Loss:  25.1902\n",
      "Epoch:  90 \t Loss:  24.683697\n",
      "Epoch:  91 \t Loss:  24.187914\n",
      "Epoch:  92 \t Loss:  23.702585\n",
      "Epoch:  93 \t Loss:  23.227497\n",
      "Epoch:  94 \t Loss:  22.762365\n",
      "Epoch:  95 \t Loss:  22.30697\n",
      "Epoch:  96 \t Loss:  21.861109\n",
      "Epoch:  97 \t Loss:  21.424524\n",
      "Epoch:  98 \t Loss:  20.996984\n",
      "Epoch:  99 \t Loss:  20.578327\n",
      "Epoch:  100 \t Loss:  20.168333\n",
      "Epoch:  101 \t Loss:  19.766787\n",
      "Epoch:  102 \t Loss:  19.373499\n",
      "Epoch:  103 \t Loss:  18.988325\n",
      "Epoch:  104 \t Loss:  18.611023\n",
      "Epoch:  105 \t Loss:  18.241451\n",
      "Epoch:  106 \t Loss:  17.879435\n",
      "Epoch:  107 \t Loss:  17.524788\n",
      "Epoch:  108 \t Loss:  17.177378\n",
      "Epoch:  109 \t Loss:  16.83703\n",
      "Epoch:  110 \t Loss:  16.503584\n",
      "Epoch:  111 \t Loss:  16.176899\n",
      "Epoch:  112 \t Loss:  15.856839\n",
      "Epoch:  113 \t Loss:  15.543223\n",
      "Epoch:  114 \t Loss:  15.235954\n",
      "Epoch:  115 \t Loss:  14.934884\n",
      "Epoch:  116 \t Loss:  14.639875\n",
      "Epoch:  117 \t Loss:  14.350792\n",
      "Epoch:  118 \t Loss:  14.067522\n",
      "Epoch:  119 \t Loss:  13.789941\n",
      "Epoch:  120 \t Loss:  13.51792\n",
      "Epoch:  121 \t Loss:  13.251355\n",
      "Epoch:  122 \t Loss:  12.990128\n",
      "Epoch:  123 \t Loss:  12.734124\n",
      "Epoch:  124 \t Loss:  12.483233\n",
      "Epoch:  125 \t Loss:  12.237338\n",
      "Epoch:  126 \t Loss:  11.996362\n",
      "Epoch:  127 \t Loss:  11.760188\n",
      "Epoch:  128 \t Loss:  11.528716\n",
      "Epoch:  129 \t Loss:  11.301857\n",
      "Epoch:  130 \t Loss:  11.079516\n",
      "Epoch:  131 \t Loss:  10.861584\n",
      "Epoch:  132 \t Loss:  10.647991\n",
      "Epoch:  133 \t Loss:  10.438634\n",
      "Epoch:  134 \t Loss:  10.233433\n",
      "Epoch:  135 \t Loss:  10.032304\n",
      "Epoch:  136 \t Loss:  9.835167\n",
      "Epoch:  137 \t Loss:  9.64194\n",
      "Epoch:  138 \t Loss:  9.452531\n",
      "Epoch:  139 \t Loss:  9.26688\n",
      "Epoch:  140 \t Loss:  9.084905\n",
      "Epoch:  141 \t Loss:  8.906528\n",
      "Epoch:  142 \t Loss:  8.731679\n",
      "Epoch:  143 \t Loss:  8.560283\n",
      "Epoch:  144 \t Loss:  8.392292\n",
      "Epoch:  145 \t Loss:  8.22761\n",
      "Epoch:  146 \t Loss:  8.06618\n",
      "Epoch:  147 \t Loss:  7.9079447\n",
      "Epoch:  148 \t Loss:  7.752836\n",
      "Epoch:  149 \t Loss:  7.6007895\n",
      "Epoch:  150 \t Loss:  7.4517436\n",
      "Epoch:  151 \t Loss:  7.305634\n",
      "Epoch:  152 \t Loss:  7.162412\n",
      "Epoch:  153 \t Loss:  7.022018\n",
      "Epoch:  154 \t Loss:  6.8843927\n",
      "Epoch:  155 \t Loss:  6.7494826\n",
      "Epoch:  156 \t Loss:  6.617234\n",
      "Epoch:  157 \t Loss:  6.4875937\n",
      "Epoch:  158 \t Loss:  6.360506\n",
      "Epoch:  159 \t Loss:  6.235926\n",
      "Epoch:  160 \t Loss:  6.113809\n",
      "Epoch:  161 \t Loss:  5.9940925\n",
      "Epoch:  162 \t Loss:  5.876735\n",
      "Epoch:  163 \t Loss:  5.7616963\n",
      "Epoch:  164 \t Loss:  5.6489267\n",
      "Epoch:  165 \t Loss:  5.538374\n",
      "Epoch:  166 \t Loss:  5.430005\n",
      "Epoch:  167 \t Loss:  5.323766\n",
      "Epoch:  168 \t Loss:  5.2196245\n",
      "Epoch:  169 \t Loss:  5.117537\n",
      "Epoch:  170 \t Loss:  5.017461\n",
      "Epoch:  171 \t Loss:  4.9193606\n",
      "Epoch:  172 \t Loss:  4.8231926\n",
      "Epoch:  173 \t Loss:  4.728915\n",
      "Epoch:  174 \t Loss:  4.6365056\n",
      "Epoch:  175 \t Loss:  4.545913\n",
      "Epoch:  176 \t Loss:  4.457102\n",
      "Epoch:  177 \t Loss:  4.37005\n",
      "Epoch:  178 \t Loss:  4.2847095\n",
      "Epoch:  179 \t Loss:  4.201053\n",
      "Epoch:  180 \t Loss:  4.1190486\n",
      "Epoch:  181 \t Loss:  4.0386577\n",
      "Epoch:  182 \t Loss:  3.9598546\n",
      "Epoch:  183 \t Loss:  3.882604\n",
      "Epoch:  184 \t Loss:  3.8068829\n",
      "Epoch:  185 \t Loss:  3.7326503\n",
      "Epoch:  186 \t Loss:  3.6598866\n",
      "Epoch:  187 \t Loss:  3.5885568\n",
      "Epoch:  188 \t Loss:  3.5186312\n",
      "Epoch:  189 \t Loss:  3.45009\n",
      "Epoch:  190 \t Loss:  3.3829021\n",
      "Epoch:  191 \t Loss:  3.3170383\n",
      "Epoch:  192 \t Loss:  3.2524753\n",
      "Epoch:  193 \t Loss:  3.1891887\n",
      "Epoch:  194 \t Loss:  3.1271482\n",
      "Epoch:  195 \t Loss:  3.066339\n",
      "Epoch:  196 \t Loss:  3.0067282\n",
      "Epoch:  197 \t Loss:  2.948292\n",
      "Epoch:  198 \t Loss:  2.8910139\n",
      "Epoch:  199 \t Loss:  2.8348668\n",
      "Epoch:  200 \t Loss:  2.77983\n",
      "Epoch:  201 \t Loss:  2.7258794\n",
      "Epoch:  202 \t Loss:  2.6729994\n",
      "Epoch:  203 \t Loss:  2.6211622\n",
      "Epoch:  204 \t Loss:  2.5703492\n",
      "Epoch:  205 \t Loss:  2.520541\n",
      "Epoch:  206 \t Loss:  2.471719\n",
      "Epoch:  207 \t Loss:  2.4238653\n",
      "Epoch:  208 \t Loss:  2.3769543\n",
      "Epoch:  209 \t Loss:  2.330973\n",
      "Epoch:  210 \t Loss:  2.285903\n",
      "Epoch:  211 \t Loss:  2.2417233\n",
      "Epoch:  212 \t Loss:  2.1984205\n",
      "Epoch:  213 \t Loss:  2.1559732\n",
      "Epoch:  214 \t Loss:  2.1143682\n",
      "Epoch:  215 \t Loss:  2.0735865\n",
      "Epoch:  216 \t Loss:  2.0336115\n",
      "Epoch:  217 \t Loss:  1.9944284\n",
      "Epoch:  218 \t Loss:  1.9560238\n",
      "Epoch:  219 \t Loss:  1.9183781\n",
      "Epoch:  220 \t Loss:  1.8814803\n",
      "Epoch:  221 \t Loss:  1.8453131\n",
      "Epoch:  222 \t Loss:  1.8098632\n",
      "Epoch:  223 \t Loss:  1.775116\n",
      "Epoch:  224 \t Loss:  1.741057\n",
      "Epoch:  225 \t Loss:  1.7076757\n",
      "Epoch:  226 \t Loss:  1.6749562\n",
      "Epoch:  227 \t Loss:  1.6428854\n",
      "Epoch:  228 \t Loss:  1.6114516\n",
      "Epoch:  229 \t Loss:  1.5806392\n",
      "Epoch:  230 \t Loss:  1.5504403\n",
      "Epoch:  231 \t Loss:  1.5208422\n",
      "Epoch:  232 \t Loss:  1.4918301\n",
      "Epoch:  233 \t Loss:  1.463394\n",
      "Epoch:  234 \t Loss:  1.435524\n",
      "Epoch:  235 \t Loss:  1.4082075\n",
      "Epoch:  236 \t Loss:  1.3814318\n",
      "Epoch:  237 \t Loss:  1.3551893\n",
      "Epoch:  238 \t Loss:  1.3294687\n",
      "Epoch:  239 \t Loss:  1.3042585\n",
      "Epoch:  240 \t Loss:  1.2795515\n",
      "Epoch:  241 \t Loss:  1.2553338\n",
      "Epoch:  242 \t Loss:  1.2315974\n",
      "Epoch:  243 \t Loss:  1.208334\n",
      "Epoch:  244 \t Loss:  1.1855327\n",
      "Epoch:  245 \t Loss:  1.1631858\n",
      "Epoch:  246 \t Loss:  1.1412827\n",
      "Epoch:  247 \t Loss:  1.1198156\n",
      "Epoch:  248 \t Loss:  1.0987757\n",
      "Epoch:  249 \t Loss:  1.078155\n",
      "Epoch:  250 \t Loss:  1.0579439\n",
      "Epoch:  251 \t Loss:  1.038135\n",
      "Epoch:  252 \t Loss:  1.018722\n",
      "Epoch:  253 \t Loss:  0.9996942\n",
      "Epoch:  254 \t Loss:  0.98104656\n",
      "Epoch:  255 \t Loss:  0.96276987\n",
      "Epoch:  256 \t Loss:  0.9448572\n",
      "Epoch:  257 \t Loss:  0.92730075\n",
      "Epoch:  258 \t Loss:  0.9100951\n",
      "Epoch:  259 \t Loss:  0.8932316\n",
      "Epoch:  260 \t Loss:  0.87670434\n",
      "Epoch:  261 \t Loss:  0.86050713\n",
      "Epoch:  262 \t Loss:  0.84463185\n",
      "Epoch:  263 \t Loss:  0.8290734\n",
      "Epoch:  264 \t Loss:  0.8138254\n",
      "Epoch:  265 \t Loss:  0.7988817\n",
      "Epoch:  266 \t Loss:  0.7842362\n",
      "Epoch:  267 \t Loss:  0.76988226\n",
      "Epoch:  268 \t Loss:  0.7558144\n",
      "Epoch:  269 \t Loss:  0.7420277\n",
      "Epoch:  270 \t Loss:  0.7285157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  271 \t Loss:  0.715274\n",
      "Epoch:  272 \t Loss:  0.7022963\n",
      "Epoch:  273 \t Loss:  0.6895773\n",
      "Epoch:  274 \t Loss:  0.67711246\n",
      "Epoch:  275 \t Loss:  0.6648968\n",
      "Epoch:  276 \t Loss:  0.6529245\n",
      "Epoch:  277 \t Loss:  0.64119124\n",
      "Epoch:  278 \t Loss:  0.6296925\n",
      "Epoch:  279 \t Loss:  0.61842316\n",
      "Epoch:  280 \t Loss:  0.6073792\n",
      "Epoch:  281 \t Loss:  0.5965558\n",
      "Epoch:  282 \t Loss:  0.5859483\n",
      "Epoch:  283 \t Loss:  0.5755534\n",
      "Epoch:  284 \t Loss:  0.5653658\n",
      "Epoch:  285 \t Loss:  0.55538136\n",
      "Epoch:  286 \t Loss:  0.54559726\n",
      "Epoch:  287 \t Loss:  0.5360085\n",
      "Epoch:  288 \t Loss:  0.5266109\n",
      "Epoch:  289 \t Loss:  0.5174015\n",
      "Epoch:  290 \t Loss:  0.5083759\n",
      "Epoch:  291 \t Loss:  0.49953154\n",
      "Epoch:  292 \t Loss:  0.4908628\n",
      "Epoch:  293 \t Loss:  0.48236838\n",
      "Epoch:  294 \t Loss:  0.47404355\n",
      "Epoch:  295 \t Loss:  0.46588504\n",
      "Epoch:  296 \t Loss:  0.45788985\n",
      "Epoch:  297 \t Loss:  0.45005462\n",
      "Epoch:  298 \t Loss:  0.44237605\n",
      "Epoch:  299 \t Loss:  0.43485096\n",
      "Epoch:  300 \t Loss:  0.4274766\n",
      "Epoch:  301 \t Loss:  0.42024976\n",
      "Epoch:  302 \t Loss:  0.41316772\n",
      "Epoch:  303 \t Loss:  0.40622714\n",
      "Epoch:  304 \t Loss:  0.39942557\n",
      "Epoch:  305 \t Loss:  0.39275974\n",
      "Epoch:  306 \t Loss:  0.386228\n",
      "Epoch:  307 \t Loss:  0.37982678\n",
      "Epoch:  308 \t Loss:  0.37355322\n",
      "Epoch:  309 \t Loss:  0.36740565\n",
      "Epoch:  310 \t Loss:  0.36138093\n",
      "Epoch:  311 \t Loss:  0.35547715\n",
      "Epoch:  312 \t Loss:  0.3496911\n",
      "Epoch:  313 \t Loss:  0.34402144\n",
      "Epoch:  314 \t Loss:  0.33846515\n",
      "Epoch:  315 \t Loss:  0.3330196\n",
      "Epoch:  316 \t Loss:  0.32768402\n",
      "Epoch:  317 \t Loss:  0.32245475\n",
      "Epoch:  318 \t Loss:  0.31733018\n",
      "Epoch:  319 \t Loss:  0.31230825\n",
      "Epoch:  320 \t Loss:  0.30738705\n",
      "Epoch:  321 \t Loss:  0.3025644\n",
      "Epoch:  322 \t Loss:  0.29783833\n",
      "Epoch:  323 \t Loss:  0.29320705\n",
      "Epoch:  324 \t Loss:  0.28866842\n",
      "Epoch:  325 \t Loss:  0.28422076\n",
      "Epoch:  326 \t Loss:  0.27986228\n",
      "Epoch:  327 \t Loss:  0.2755909\n",
      "Epoch:  328 \t Loss:  0.27140528\n",
      "Epoch:  329 \t Loss:  0.26730347\n",
      "Epoch:  330 \t Loss:  0.26328397\n",
      "Epoch:  331 \t Loss:  0.25934485\n",
      "Epoch:  332 \t Loss:  0.25548467\n",
      "Epoch:  333 \t Loss:  0.25170198\n",
      "Epoch:  334 \t Loss:  0.24799508\n",
      "Epoch:  335 \t Loss:  0.24436241\n",
      "Epoch:  336 \t Loss:  0.24080274\n",
      "Epoch:  337 \t Loss:  0.2373142\n",
      "Epoch:  338 \t Loss:  0.23389572\n",
      "Epoch:  339 \t Loss:  0.23054564\n",
      "Epoch:  340 \t Loss:  0.22726275\n",
      "Epoch:  341 \t Loss:  0.22404575\n",
      "Epoch:  342 \t Loss:  0.22089319\n",
      "Epoch:  343 \t Loss:  0.21780373\n",
      "Epoch:  344 \t Loss:  0.2147764\n",
      "Epoch:  345 \t Loss:  0.21180972\n",
      "Epoch:  346 \t Loss:  0.2089023\n",
      "Epoch:  347 \t Loss:  0.2060533\n",
      "Epoch:  348 \t Loss:  0.20326138\n",
      "Epoch:  349 \t Loss:  0.20052561\n",
      "Epoch:  350 \t Loss:  0.19784456\n",
      "Epoch:  351 \t Loss:  0.19521731\n",
      "Epoch:  352 \t Loss:  0.19264266\n",
      "Epoch:  353 \t Loss:  0.19011979\n",
      "Epoch:  354 \t Loss:  0.1876474\n",
      "Epoch:  355 \t Loss:  0.18522456\n",
      "Epoch:  356 \t Loss:  0.18285032\n",
      "Epoch:  357 \t Loss:  0.18052377\n",
      "Epoch:  358 \t Loss:  0.17824382\n",
      "Epoch:  359 \t Loss:  0.17600965\n",
      "Epoch:  360 \t Loss:  0.1738202\n",
      "Epoch:  361 \t Loss:  0.17167477\n",
      "Epoch:  362 \t Loss:  0.16957226\n",
      "Epoch:  363 \t Loss:  0.16751188\n",
      "Epoch:  364 \t Loss:  0.16549289\n",
      "Epoch:  365 \t Loss:  0.16351439\n",
      "Epoch:  366 \t Loss:  0.16157553\n",
      "Epoch:  367 \t Loss:  0.15967554\n",
      "Epoch:  368 \t Loss:  0.15781364\n",
      "Epoch:  369 \t Loss:  0.15598929\n",
      "Epoch:  370 \t Loss:  0.15420136\n",
      "Epoch:  371 \t Loss:  0.15244932\n",
      "Epoch:  372 \t Loss:  0.15073253\n",
      "Epoch:  373 \t Loss:  0.14904998\n",
      "Epoch:  374 \t Loss:  0.14740118\n",
      "Epoch:  375 \t Loss:  0.14578559\n",
      "Epoch:  376 \t Loss:  0.14420235\n",
      "Epoch:  377 \t Loss:  0.14265086\n",
      "Epoch:  378 \t Loss:  0.14113045\n",
      "Epoch:  379 \t Loss:  0.13964051\n",
      "Epoch:  380 \t Loss:  0.13818055\n",
      "Epoch:  381 \t Loss:  0.1367498\n",
      "Epoch:  382 \t Loss:  0.13534784\n",
      "Epoch:  383 \t Loss:  0.13397396\n",
      "Epoch:  384 \t Loss:  0.13262759\n",
      "Epoch:  385 \t Loss:  0.1313082\n",
      "Epoch:  386 \t Loss:  0.13001531\n",
      "Epoch:  387 \t Loss:  0.12874833\n",
      "Epoch:  388 \t Loss:  0.12750673\n",
      "Epoch:  389 \t Loss:  0.12629014\n",
      "Epoch:  390 \t Loss:  0.12509789\n",
      "Epoch:  391 \t Loss:  0.123929545\n",
      "Epoch:  392 \t Loss:  0.12278469\n",
      "Epoch:  393 \t Loss:  0.12166272\n",
      "Epoch:  394 \t Loss:  0.12056328\n",
      "Epoch:  395 \t Loss:  0.11948585\n",
      "Epoch:  396 \t Loss:  0.11843005\n",
      "Epoch:  397 \t Loss:  0.11739539\n",
      "Epoch:  398 \t Loss:  0.116381556\n",
      "Epoch:  399 \t Loss:  0.115388006\n",
      "Epoch:  400 \t Loss:  0.11441435\n",
      "Epoch:  401 \t Loss:  0.11346035\n",
      "Epoch:  402 \t Loss:  0.112525344\n",
      "Epoch:  403 \t Loss:  0.11160918\n",
      "Epoch:  404 \t Loss:  0.11071131\n",
      "Epoch:  405 \t Loss:  0.1098315\n",
      "Epoch:  406 \t Loss:  0.108969346\n",
      "Epoch:  407 \t Loss:  0.1081245\n",
      "Epoch:  408 \t Loss:  0.10729656\n",
      "Epoch:  409 \t Loss:  0.106485225\n",
      "Epoch:  410 \t Loss:  0.10569017\n",
      "Epoch:  411 \t Loss:  0.10491099\n",
      "Epoch:  412 \t Loss:  0.10414752\n",
      "Epoch:  413 \t Loss:  0.10339932\n",
      "Epoch:  414 \t Loss:  0.10266609\n",
      "Epoch:  415 \t Loss:  0.101947546\n",
      "Epoch:  416 \t Loss:  0.10124345\n",
      "Epoch:  417 \t Loss:  0.10055345\n",
      "Epoch:  418 \t Loss:  0.0998773\n",
      "Epoch:  419 \t Loss:  0.09921464\n",
      "Epoch:  420 \t Loss:  0.09856533\n",
      "Epoch:  421 \t Loss:  0.097929\n",
      "Epoch:  422 \t Loss:  0.09730537\n",
      "Epoch:  423 \t Loss:  0.09669436\n",
      "Epoch:  424 \t Loss:  0.096095495\n",
      "Epoch:  425 \t Loss:  0.09550861\n",
      "Epoch:  426 \t Loss:  0.0949335\n",
      "Epoch:  427 \t Loss:  0.094369926\n",
      "Epoch:  428 \t Loss:  0.09381763\n",
      "Epoch:  429 \t Loss:  0.093276456\n",
      "Epoch:  430 \t Loss:  0.09274601\n",
      "Epoch:  431 \t Loss:  0.09222622\n",
      "Epoch:  432 \t Loss:  0.09171687\n",
      "Epoch:  433 \t Loss:  0.09121767\n",
      "Epoch:  434 \t Loss:  0.09072851\n",
      "Epoch:  435 \t Loss:  0.09024908\n",
      "Epoch:  436 \t Loss:  0.08977922\n",
      "Epoch:  437 \t Loss:  0.0893188\n",
      "Epoch:  438 \t Loss:  0.088867545\n",
      "Epoch:  439 \t Loss:  0.08842537\n",
      "Epoch:  440 \t Loss:  0.08799188\n",
      "Epoch:  441 \t Loss:  0.087567106\n",
      "Epoch:  442 \t Loss:  0.08715084\n",
      "Epoch:  443 \t Loss:  0.08674293\n",
      "Epoch:  444 \t Loss:  0.08634314\n",
      "Epoch:  445 \t Loss:  0.0859513\n",
      "Epoch:  446 \t Loss:  0.08556733\n",
      "Epoch:  447 \t Loss:  0.08519106\n",
      "Epoch:  448 \t Loss:  0.084822275\n",
      "Epoch:  449 \t Loss:  0.0844609\n",
      "Epoch:  450 \t Loss:  0.08410668\n",
      "Epoch:  451 \t Loss:  0.08375961\n",
      "Epoch:  452 \t Loss:  0.08341952\n",
      "Epoch:  453 \t Loss:  0.0830862\n",
      "Epoch:  454 \t Loss:  0.08275954\n",
      "Epoch:  455 \t Loss:  0.08243946\n",
      "Epoch:  456 \t Loss:  0.082125776\n",
      "Epoch:  457 \t Loss:  0.08181842\n",
      "Epoch:  458 \t Loss:  0.08151719\n",
      "Epoch:  459 \t Loss:  0.081222005\n",
      "Epoch:  460 \t Loss:  0.08093274\n",
      "Epoch:  461 \t Loss:  0.080649264\n",
      "Epoch:  462 \t Loss:  0.08037147\n",
      "Epoch:  463 \t Loss:  0.08009927\n",
      "Epoch:  464 \t Loss:  0.079832554\n",
      "Epoch:  465 \t Loss:  0.07957118\n",
      "Epoch:  466 \t Loss:  0.07931504\n",
      "Epoch:  467 \t Loss:  0.07906404\n",
      "Epoch:  468 \t Loss:  0.07881811\n",
      "Epoch:  469 \t Loss:  0.07857711\n",
      "Epoch:  470 \t Loss:  0.07834093\n",
      "Epoch:  471 \t Loss:  0.07810951\n",
      "Epoch:  472 \t Loss:  0.077882774\n",
      "Epoch:  473 \t Loss:  0.07766054\n",
      "Epoch:  474 \t Loss:  0.077442825\n",
      "Epoch:  475 \t Loss:  0.07722947\n",
      "Epoch:  476 \t Loss:  0.07702041\n",
      "Epoch:  477 \t Loss:  0.07681552\n",
      "Epoch:  478 \t Loss:  0.076614805\n",
      "Epoch:  479 \t Loss:  0.07641804\n",
      "Epoch:  480 \t Loss:  0.07622527\n",
      "Epoch:  481 \t Loss:  0.07603636\n",
      "Epoch:  482 \t Loss:  0.07585127\n",
      "Epoch:  483 \t Loss:  0.07566986\n",
      "Epoch:  484 \t Loss:  0.07549205\n",
      "Epoch:  485 \t Loss:  0.075317845\n",
      "Epoch:  486 \t Loss:  0.07514714\n",
      "Epoch:  487 \t Loss:  0.07497985\n",
      "Epoch:  488 \t Loss:  0.07481591\n",
      "Epoch:  489 \t Loss:  0.07465525\n",
      "Epoch:  490 \t Loss:  0.07449783\n",
      "Epoch:  491 \t Loss:  0.07434357\n",
      "Epoch:  492 \t Loss:  0.074192435\n",
      "Epoch:  493 \t Loss:  0.07404427\n",
      "Epoch:  494 \t Loss:  0.07389911\n",
      "Epoch:  495 \t Loss:  0.07375685\n",
      "Epoch:  496 \t Loss:  0.07361748\n",
      "Epoch:  497 \t Loss:  0.07348087\n",
      "Epoch:  498 \t Loss:  0.07334704\n",
      "Epoch:  499 \t Loss:  0.073215865\n",
      "[[0.00035205 0.00027918 0.0001994  ... 0.00024526 0.00027515 0.000245  ]\n",
      " [0.00028757 0.00037074 0.00027025 ... 0.00025224 0.00032714 0.00028312]\n",
      " [0.00030145 0.00031533 0.0002316  ... 0.00025182 0.00031159 0.00026471]\n",
      " ...\n",
      " [0.00026549 0.00032036 0.00024034 ... 0.00019216 0.00027728 0.0002361 ]\n",
      " [0.00029435 0.00029116 0.00029719 ... 0.00023608 0.00030598 0.00025767]\n",
      " [0.00028262 0.00029161 0.0002597  ... 0.00024197 0.00024923 0.00022406]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        _, train_loss = sess.run([train_op, loss], feed_dict={X: mean_centered_ratings_matrix})\n",
    "        print(\"Epoch: \", epoch, \"\\t Loss: \", train_loss)\n",
    "        \n",
    "    u = sess.run(U)\n",
    "    v = sess.run(V)\n",
    "    print(np.matmul(u, np.transpose(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.matmul(u, np.transpose(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00026250078\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental Latent Component Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first perform the update for $u_{iq}$ and $v_{jq}$ only for $q = 1$. The approach repeatedly cycles through all the observed entries in S while performing the update for $q = 1$ until covergence is reached. Therefore, we can learn the first pair of columns $\\bar{U_1}$ and $\\bar{V_1}$. Then the outer product matrix $\\bar{U_1} \\bar{V_1}^T$ is subtracted from $R$ (for observed entries). Continue for $q = 2$ to $k$: \n",
    "$$ R \\approx UV^T = \\sum_{q = 1}^k \\bar{U_q} \\bar{V_q}^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternating Least Squares and Coordinate Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatiing Least Squares: <br>\n",
    "Iterative approach:\n",
    "1. Keeping U fixed, optimize V\n",
    "2. Keeping V fixed, optimize U \n",
    "\n",
    "The drawback of ALS is that it is not quite as efficient as SGD in large-scale settings with explicit ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coordinate Descent:<br>\n",
    "Fixing a subset of variable. All entries in $U$ and $V$ are fixed except for a single entry (or coordinate) in one of two matrices, which will be optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporating User and Item Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User biases: $o_i$ <br>\n",
    "Item biases: $p_j$ <br>\n",
    "The model learn this two variables. The predicted value if the rating entry $(i, j)$ given by:\n",
    "$$ \\hat{r_{ij}} = o_i + p_j + \\sum_{s=1}^k u_{is}.v_{js} $$ <br>\n",
    "Then the error $e_{ij}$ is given by: \n",
    "$$ e_{ij} = r_{ij} - \\hat{r_{ij}} = r_{ij} - o_i - p_j - \\sum_{s=1}^k u_{is}.v_{js}  $$ <br>\n",
    "And the loss funtion of this type is given by:\n",
    "$$ J = \\dfrac{1}{2} \\sum_{(i, j) \\in S} e_{ij}^2 + \\dfrac{\\lambda}{2} \\sum_{i = 1}^{m} \\sum_{s = 1}^{k} u_{is}^2 + \\dfrac{\\lambda}{2} \\sum_{j = 1}^{n} \\sum_{s = 1}^{k} v_{js}^2 + \\dfrac{\\lambda}{2} \\sum_{i = 1}^{m} o_i^2 + \\dfrac{\\lambda}{2} \\sum_{j = 1}^{n} p_j^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_with_bias(original_matrix, predicted_matrix, user_biases, item_biases):\n",
    "    indices = tf_specified_rating_indices(original_matrix)\n",
    "    \n",
    "    predicted_matrix = tf.transpose(predicted_matrix) - user_biases\n",
    "    predicted_matrix = tf.transpose(predicted_matrix)\n",
    "    predicted_matrix = predicted_matrix - item_biases\n",
    "    \n",
    "    observed_values = tf.gather_nd(original_matrix, indices)\n",
    "    predicted_values = tf.gather_nd(predicted_matrix, indices)\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(observed_values, predicted_values)\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[n_users, n_items], name='X')\n",
    "# X = tf.Variable(initial_value=mean_centered_ratings_matrix, trainable=False)\n",
    "# indices = tf.where(tf.is_finite(X))\n",
    "\n",
    "U = tf.Variable(tf.random_uniform(shape=[n_users, k]))\n",
    "V = tf.Variable(tf.random_uniform(shape=[n_items, k]))\n",
    "\n",
    "user_biases = tf.Variable(tf.random_uniform(shape=[n_users]))\n",
    "item_biases = tf.Variable(tf.random_uniform(shape=[n_items]))\n",
    "\n",
    "outputs = tf.matmul(U, tf.transpose(V))\n",
    "\n",
    "# fix here\n",
    "loss = get_loss_with_bias(X, outputs, user_biases, item_biases)\n",
    "reg_losses = tf.nn.l2_loss(U) + tf.nn.l2_loss(V) + tf.nn.l2_loss(user_biases) + tf.nn.l2_loss(item_biases)\n",
    "\n",
    "loss = loss + 0.01*reg_losses\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=1).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \t Loss:  357.73584\n",
      "Epoch:  1 \t Loss:  330.9766\n",
      "Epoch:  2 \t Loss:  307.6568\n",
      "Epoch:  3 \t Loss:  287.1829\n",
      "Epoch:  4 \t Loss:  269.08588\n",
      "Epoch:  5 \t Loss:  252.99023\n",
      "Epoch:  6 \t Loss:  238.59293\n",
      "Epoch:  7 \t Loss:  225.64665\n",
      "Epoch:  8 \t Loss:  213.94823\n",
      "Epoch:  9 \t Loss:  203.32939\n",
      "Epoch:  10 \t Loss:  193.64995\n",
      "Epoch:  11 \t Loss:  184.7915\n",
      "Epoch:  12 \t Loss:  176.65427\n",
      "Epoch:  13 \t Loss:  169.1539\n",
      "Epoch:  14 \t Loss:  162.21811\n",
      "Epoch:  15 \t Loss:  155.78458\n",
      "Epoch:  16 \t Loss:  149.79977\n",
      "Epoch:  17 \t Loss:  144.21715\n",
      "Epoch:  18 \t Loss:  138.99657\n",
      "Epoch:  19 \t Loss:  134.10265\n",
      "Epoch:  20 \t Loss:  129.50444\n",
      "Epoch:  21 \t Loss:  125.17487\n",
      "Epoch:  22 \t Loss:  121.08976\n",
      "Epoch:  23 \t Loss:  117.22796\n",
      "Epoch:  24 \t Loss:  113.570816\n",
      "Epoch:  25 \t Loss:  110.10115\n",
      "Epoch:  26 \t Loss:  106.80412\n",
      "Epoch:  27 \t Loss:  103.666336\n",
      "Epoch:  28 \t Loss:  100.675705\n",
      "Epoch:  29 \t Loss:  97.82139\n",
      "Epoch:  30 \t Loss:  95.09349\n",
      "Epoch:  31 \t Loss:  92.483315\n",
      "Epoch:  32 \t Loss:  89.982574\n",
      "Epoch:  33 \t Loss:  87.584335\n",
      "Epoch:  34 \t Loss:  85.28163\n",
      "Epoch:  35 \t Loss:  83.068634\n",
      "Epoch:  36 \t Loss:  80.93981\n",
      "Epoch:  37 \t Loss:  78.89002\n",
      "Epoch:  38 \t Loss:  76.91461\n",
      "Epoch:  39 \t Loss:  75.00937\n",
      "Epoch:  40 \t Loss:  73.17044\n",
      "Epoch:  41 \t Loss:  71.39413\n",
      "Epoch:  42 \t Loss:  69.677185\n",
      "Epoch:  43 \t Loss:  68.01646\n",
      "Epoch:  44 \t Loss:  66.409096\n",
      "Epoch:  45 \t Loss:  64.85251\n",
      "Epoch:  46 \t Loss:  63.344166\n",
      "Epoch:  47 \t Loss:  61.88188\n",
      "Epoch:  48 \t Loss:  60.46342\n",
      "Epoch:  49 \t Loss:  59.08684\n",
      "Epoch:  50 \t Loss:  57.750282\n",
      "Epoch:  51 \t Loss:  56.45203\n",
      "Epoch:  52 \t Loss:  55.190395\n",
      "Epoch:  53 \t Loss:  53.96396\n",
      "Epoch:  54 \t Loss:  52.77116\n",
      "Epoch:  55 \t Loss:  51.610752\n",
      "Epoch:  56 \t Loss:  50.481472\n",
      "Epoch:  57 \t Loss:  49.382046\n",
      "Epoch:  58 \t Loss:  48.31141\n",
      "Epoch:  59 \t Loss:  47.268497\n",
      "Epoch:  60 \t Loss:  46.2523\n",
      "Epoch:  61 \t Loss:  45.26182\n",
      "Epoch:  62 \t Loss:  44.296227\n",
      "Epoch:  63 \t Loss:  43.354618\n",
      "Epoch:  64 \t Loss:  42.436226\n",
      "Epoch:  65 \t Loss:  41.540195\n",
      "Epoch:  66 \t Loss:  40.6659\n",
      "Epoch:  67 \t Loss:  39.812588\n",
      "Epoch:  68 \t Loss:  38.97957\n",
      "Epoch:  69 \t Loss:  38.16627\n",
      "Epoch:  70 \t Loss:  37.37201\n",
      "Epoch:  71 \t Loss:  36.596264\n",
      "Epoch:  72 \t Loss:  35.83849\n",
      "Epoch:  73 \t Loss:  35.098118\n",
      "Epoch:  74 \t Loss:  34.374664\n",
      "Epoch:  75 \t Loss:  33.667645\n",
      "Epoch:  76 \t Loss:  32.97656\n",
      "Epoch:  77 \t Loss:  32.30101\n",
      "Epoch:  78 \t Loss:  31.640512\n",
      "Epoch:  79 \t Loss:  30.994722\n",
      "Epoch:  80 \t Loss:  30.363178\n",
      "Epoch:  81 \t Loss:  29.74554\n",
      "Epoch:  82 \t Loss:  29.141428\n",
      "Epoch:  83 \t Loss:  28.550503\n",
      "Epoch:  84 \t Loss:  27.97237\n",
      "Epoch:  85 \t Loss:  27.406746\n",
      "Epoch:  86 \t Loss:  26.853306\n",
      "Epoch:  87 \t Loss:  26.311722\n",
      "Epoch:  88 \t Loss:  25.781734\n",
      "Epoch:  89 \t Loss:  25.263035\n",
      "Epoch:  90 \t Loss:  24.755327\n",
      "Epoch:  91 \t Loss:  24.258377\n",
      "Epoch:  92 \t Loss:  23.771915\n",
      "Epoch:  93 \t Loss:  23.295662\n",
      "Epoch:  94 \t Loss:  22.829403\n",
      "Epoch:  95 \t Loss:  22.372889\n",
      "Epoch:  96 \t Loss:  21.925905\n",
      "Epoch:  97 \t Loss:  21.488216\n",
      "Epoch:  98 \t Loss:  21.059622\n",
      "Epoch:  99 \t Loss:  20.63988\n",
      "Epoch:  100 \t Loss:  20.228846\n",
      "Epoch:  101 \t Loss:  19.826267\n",
      "Epoch:  102 \t Loss:  19.431942\n",
      "Epoch:  103 \t Loss:  19.045727\n",
      "Epoch:  104 \t Loss:  18.667437\n",
      "Epoch:  105 \t Loss:  18.296902\n",
      "Epoch:  106 \t Loss:  17.933887\n",
      "Epoch:  107 \t Loss:  17.57829\n",
      "Epoch:  108 \t Loss:  17.229927\n",
      "Epoch:  109 \t Loss:  16.888638\n",
      "Epoch:  110 \t Loss:  16.554274\n",
      "Epoch:  111 \t Loss:  16.22669\n",
      "Epoch:  112 \t Loss:  15.905725\n",
      "Epoch:  113 \t Loss:  15.591246\n",
      "Epoch:  114 \t Loss:  15.283096\n",
      "Epoch:  115 \t Loss:  14.98118\n",
      "Epoch:  116 \t Loss:  14.6853285\n",
      "Epoch:  117 \t Loss:  14.395419\n",
      "Epoch:  118 \t Loss:  14.111338\n",
      "Epoch:  119 \t Loss:  13.832962\n",
      "Epoch:  120 \t Loss:  13.560149\n",
      "Epoch:  121 \t Loss:  13.292813\n",
      "Epoch:  122 \t Loss:  13.030812\n",
      "Epoch:  123 \t Loss:  12.774054\n",
      "Epoch:  124 \t Loss:  12.522424\n",
      "Epoch:  125 \t Loss:  12.275815\n",
      "Epoch:  126 \t Loss:  12.03412\n",
      "Epoch:  127 \t Loss:  11.797237\n",
      "Epoch:  128 \t Loss:  11.565073\n",
      "Epoch:  129 \t Loss:  11.337541\n",
      "Epoch:  130 \t Loss:  11.11452\n",
      "Epoch:  131 \t Loss:  10.895942\n",
      "Epoch:  132 \t Loss:  10.68169\n",
      "Epoch:  133 \t Loss:  10.471703\n",
      "Epoch:  134 \t Loss:  10.265877\n",
      "Epoch:  135 \t Loss:  10.06414\n",
      "Epoch:  136 \t Loss:  9.866393\n",
      "Epoch:  137 \t Loss:  9.67257\n",
      "Epoch:  138 \t Loss:  9.482585\n",
      "Epoch:  139 \t Loss:  9.29636\n",
      "Epoch:  140 \t Loss:  9.113819\n",
      "Epoch:  141 \t Loss:  8.934889\n",
      "Epoch:  142 \t Loss:  8.759502\n",
      "Epoch:  143 \t Loss:  8.58758\n",
      "Epoch:  144 \t Loss:  8.419051\n",
      "Epoch:  145 \t Loss:  8.253862\n",
      "Epoch:  146 \t Loss:  8.091929\n",
      "Epoch:  147 \t Loss:  7.9332\n",
      "Epoch:  148 \t Loss:  7.7775955\n",
      "Epoch:  149 \t Loss:  7.6250725\n",
      "Epoch:  150 \t Loss:  7.4755583\n",
      "Epoch:  151 \t Loss:  7.32899\n",
      "Epoch:  152 \t Loss:  7.1853137\n",
      "Epoch:  153 \t Loss:  7.0444756\n",
      "Epoch:  154 \t Loss:  6.906416\n",
      "Epoch:  155 \t Loss:  6.7710767\n",
      "Epoch:  156 \t Loss:  6.638405\n",
      "Epoch:  157 \t Loss:  6.5083456\n",
      "Epoch:  158 \t Loss:  6.380864\n",
      "Epoch:  159 \t Loss:  6.255882\n",
      "Epoch:  160 \t Loss:  6.133373\n",
      "Epoch:  161 \t Loss:  6.0132732\n",
      "Epoch:  162 \t Loss:  5.895545\n",
      "Epoch:  163 \t Loss:  5.780132\n",
      "Epoch:  164 \t Loss:  5.6669993\n",
      "Epoch:  165 \t Loss:  5.556091\n",
      "Epoch:  166 \t Loss:  5.4473724\n",
      "Epoch:  167 \t Loss:  5.3407965\n",
      "Epoch:  168 \t Loss:  5.236316\n",
      "Epoch:  169 \t Loss:  5.1339016\n",
      "Epoch:  170 \t Loss:  5.033505\n",
      "Epoch:  171 \t Loss:  4.9350867\n",
      "Epoch:  172 \t Loss:  4.838603\n",
      "Epoch:  173 \t Loss:  4.7440248\n",
      "Epoch:  174 \t Loss:  4.65131\n",
      "Epoch:  175 \t Loss:  4.5604253\n",
      "Epoch:  176 \t Loss:  4.471332\n",
      "Epoch:  177 \t Loss:  4.383995\n",
      "Epoch:  178 \t Loss:  4.2983775\n",
      "Epoch:  179 \t Loss:  4.2144504\n",
      "Epoch:  180 \t Loss:  4.1321754\n",
      "Epoch:  181 \t Loss:  4.0515256\n",
      "Epoch:  182 \t Loss:  3.972467\n",
      "Epoch:  183 \t Loss:  3.8949654\n",
      "Epoch:  184 \t Loss:  3.8189938\n",
      "Epoch:  185 \t Loss:  3.744521\n",
      "Epoch:  186 \t Loss:  3.67152\n",
      "Epoch:  187 \t Loss:  3.5999548\n",
      "Epoch:  188 \t Loss:  3.5298038\n",
      "Epoch:  189 \t Loss:  3.461038\n",
      "Epoch:  190 \t Loss:  3.393629\n",
      "Epoch:  191 \t Loss:  3.3275518\n",
      "Epoch:  192 \t Loss:  3.2627804\n",
      "Epoch:  193 \t Loss:  3.1992838\n",
      "Epoch:  194 \t Loss:  3.137044\n",
      "Epoch:  195 \t Loss:  3.0760305\n",
      "Epoch:  196 \t Loss:  3.016226\n",
      "Epoch:  197 \t Loss:  2.9576013\n",
      "Epoch:  198 \t Loss:  2.900135\n",
      "Epoch:  199 \t Loss:  2.8438017\n",
      "Epoch:  200 \t Loss:  2.7885854\n",
      "Epoch:  201 \t Loss:  2.73446\n",
      "Epoch:  202 \t Loss:  2.681403\n",
      "Epoch:  203 \t Loss:  2.6293976\n",
      "Epoch:  204 \t Loss:  2.578418\n",
      "Epoch:  205 \t Loss:  2.5284474\n",
      "Epoch:  206 \t Loss:  2.4794652\n",
      "Epoch:  207 \t Loss:  2.4314525\n",
      "Epoch:  208 \t Loss:  2.384389\n",
      "Epoch:  209 \t Loss:  2.3382587\n",
      "Epoch:  210 \t Loss:  2.2930412\n",
      "Epoch:  211 \t Loss:  2.2487156\n",
      "Epoch:  212 \t Loss:  2.2052703\n",
      "Epoch:  213 \t Loss:  2.1626847\n",
      "Epoch:  214 \t Loss:  2.120942\n",
      "Epoch:  215 \t Loss:  2.0800276\n",
      "Epoch:  216 \t Loss:  2.0399208\n",
      "Epoch:  217 \t Loss:  2.000611\n",
      "Epoch:  218 \t Loss:  1.9620798\n",
      "Epoch:  219 \t Loss:  1.9243113\n",
      "Epoch:  220 \t Loss:  1.8872914\n",
      "Epoch:  221 \t Loss:  1.8510064\n",
      "Epoch:  222 \t Loss:  1.8154407\n",
      "Epoch:  223 \t Loss:  1.7805803\n",
      "Epoch:  224 \t Loss:  1.7464108\n",
      "Epoch:  225 \t Loss:  1.712919\n",
      "Epoch:  226 \t Loss:  1.6800925\n",
      "Epoch:  227 \t Loss:  1.6479169\n",
      "Epoch:  228 \t Loss:  1.6163783\n",
      "Epoch:  229 \t Loss:  1.5854671\n",
      "Epoch:  230 \t Loss:  1.5551692\n",
      "Epoch:  231 \t Loss:  1.5254744\n",
      "Epoch:  232 \t Loss:  1.4963666\n",
      "Epoch:  233 \t Loss:  1.4678382\n",
      "Epoch:  234 \t Loss:  1.4398777\n",
      "Epoch:  235 \t Loss:  1.4124702\n",
      "Epoch:  236 \t Loss:  1.3856077\n",
      "Epoch:  237 \t Loss:  1.3592811\n",
      "Epoch:  238 \t Loss:  1.3334761\n",
      "Epoch:  239 \t Loss:  1.3081844\n",
      "Epoch:  240 \t Loss:  1.2833959\n",
      "Epoch:  241 \t Loss:  1.2590992\n",
      "Epoch:  242 \t Loss:  1.2352865\n",
      "Epoch:  243 \t Loss:  1.2119461\n",
      "Epoch:  244 \t Loss:  1.1890719\n",
      "Epoch:  245 \t Loss:  1.1666512\n",
      "Epoch:  246 \t Loss:  1.1446773\n",
      "Epoch:  247 \t Loss:  1.1231402\n",
      "Epoch:  248 \t Loss:  1.1020322\n",
      "Epoch:  249 \t Loss:  1.0813444\n",
      "Epoch:  250 \t Loss:  1.0610684\n",
      "Epoch:  251 \t Loss:  1.0411961\n",
      "Epoch:  252 \t Loss:  1.0217185\n",
      "Epoch:  253 \t Loss:  1.0026298\n",
      "Epoch:  254 \t Loss:  0.9839212\n",
      "Epoch:  255 \t Loss:  0.9655851\n",
      "Epoch:  256 \t Loss:  0.947615\n",
      "Epoch:  257 \t Loss:  0.9300016\n",
      "Epoch:  258 \t Loss:  0.91273993\n",
      "Epoch:  259 \t Loss:  0.8958219\n",
      "Epoch:  260 \t Loss:  0.87924176\n",
      "Epoch:  261 \t Loss:  0.86299133\n",
      "Epoch:  262 \t Loss:  0.8470656\n",
      "Epoch:  263 \t Loss:  0.8314572\n",
      "Epoch:  264 \t Loss:  0.81615984\n",
      "Epoch:  265 \t Loss:  0.8011681\n",
      "Epoch:  266 \t Loss:  0.7864749\n",
      "Epoch:  267 \t Loss:  0.77207446\n",
      "Epoch:  268 \t Loss:  0.75796205\n",
      "Epoch:  269 \t Loss:  0.7441309\n",
      "Epoch:  270 \t Loss:  0.7305757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  271 \t Loss:  0.71729106\n",
      "Epoch:  272 \t Loss:  0.70427173\n",
      "Epoch:  273 \t Loss:  0.69151247\n",
      "Epoch:  274 \t Loss:  0.6790069\n",
      "Epoch:  275 \t Loss:  0.6667525\n",
      "Epoch:  276 \t Loss:  0.6547418\n",
      "Epoch:  277 \t Loss:  0.6429709\n",
      "Epoch:  278 \t Loss:  0.6314356\n",
      "Epoch:  279 \t Loss:  0.62013066\n",
      "Epoch:  280 \t Loss:  0.6090504\n",
      "Epoch:  281 \t Loss:  0.5981932\n",
      "Epoch:  282 \t Loss:  0.58755195\n",
      "Epoch:  283 \t Loss:  0.57712334\n",
      "Epoch:  284 \t Loss:  0.56690323\n",
      "Epoch:  285 \t Loss:  0.5568879\n",
      "Epoch:  286 \t Loss:  0.54707193\n",
      "Epoch:  287 \t Loss:  0.5374526\n",
      "Epoch:  288 \t Loss:  0.5280252\n",
      "Epoch:  289 \t Loss:  0.5187862\n",
      "Epoch:  290 \t Loss:  0.5097325\n",
      "Epoch:  291 \t Loss:  0.5008596\n",
      "Epoch:  292 \t Loss:  0.4921638\n",
      "Epoch:  293 \t Loss:  0.48364225\n",
      "Epoch:  294 \t Loss:  0.47529116\n",
      "Epoch:  295 \t Loss:  0.4671068\n",
      "Epoch:  296 \t Loss:  0.45908645\n",
      "Epoch:  297 \t Loss:  0.45122606\n",
      "Epoch:  298 \t Loss:  0.4435234\n",
      "Epoch:  299 \t Loss:  0.43597487\n",
      "Epoch:  300 \t Loss:  0.42857707\n",
      "Epoch:  301 \t Loss:  0.42132747\n",
      "Epoch:  302 \t Loss:  0.41422272\n",
      "Epoch:  303 \t Loss:  0.4072604\n",
      "Epoch:  304 \t Loss:  0.40043753\n",
      "Epoch:  305 \t Loss:  0.3937507\n",
      "Epoch:  306 \t Loss:  0.3871983\n",
      "Epoch:  307 \t Loss:  0.3807766\n",
      "Epoch:  308 \t Loss:  0.3744839\n",
      "Epoch:  309 \t Loss:  0.368317\n",
      "Epoch:  310 \t Loss:  0.36227334\n",
      "Epoch:  311 \t Loss:  0.3563509\n",
      "Epoch:  312 \t Loss:  0.35054725\n",
      "Epoch:  313 \t Loss:  0.34485954\n",
      "Epoch:  314 \t Loss:  0.33928597\n",
      "Epoch:  315 \t Loss:  0.33382368\n",
      "Epoch:  316 \t Loss:  0.32847077\n",
      "Epoch:  317 \t Loss:  0.32322532\n",
      "Epoch:  318 \t Loss:  0.3180849\n",
      "Epoch:  319 \t Loss:  0.31304735\n",
      "Epoch:  320 \t Loss:  0.30811098\n",
      "Epoch:  321 \t Loss:  0.30327326\n",
      "Epoch:  322 \t Loss:  0.29853255\n",
      "Epoch:  323 \t Loss:  0.29388666\n",
      "Epoch:  324 \t Loss:  0.28933412\n",
      "Epoch:  325 \t Loss:  0.2848725\n",
      "Epoch:  326 \t Loss:  0.2805004\n",
      "Epoch:  327 \t Loss:  0.27621603\n",
      "Epoch:  328 \t Loss:  0.27201742\n",
      "Epoch:  329 \t Loss:  0.26790297\n",
      "Epoch:  330 \t Loss:  0.2638709\n",
      "Epoch:  331 \t Loss:  0.25991973\n",
      "Epoch:  332 \t Loss:  0.25604782\n",
      "Epoch:  333 \t Loss:  0.2522534\n",
      "Epoch:  334 \t Loss:  0.24853492\n",
      "Epoch:  335 \t Loss:  0.24489108\n",
      "Epoch:  336 \t Loss:  0.24132025\n",
      "Epoch:  337 \t Loss:  0.2378211\n",
      "Epoch:  338 \t Loss:  0.23439214\n",
      "Epoch:  339 \t Loss:  0.23103178\n",
      "Epoch:  340 \t Loss:  0.22773877\n",
      "Epoch:  341 \t Loss:  0.22451195\n",
      "Epoch:  342 \t Loss:  0.22134976\n",
      "Epoch:  343 \t Loss:  0.21825083\n",
      "Epoch:  344 \t Loss:  0.21521431\n",
      "Epoch:  345 \t Loss:  0.21223837\n",
      "Epoch:  346 \t Loss:  0.2093223\n",
      "Epoch:  347 \t Loss:  0.20646441\n",
      "Epoch:  348 \t Loss:  0.20366411\n",
      "Epoch:  349 \t Loss:  0.20092005\n",
      "Epoch:  350 \t Loss:  0.1982308\n",
      "Epoch:  351 \t Loss:  0.1955954\n",
      "Epoch:  352 \t Loss:  0.19301304\n",
      "Epoch:  353 \t Loss:  0.19048247\n",
      "Epoch:  354 \t Loss:  0.18800253\n",
      "Epoch:  355 \t Loss:  0.18557242\n",
      "Epoch:  356 \t Loss:  0.18319094\n",
      "Epoch:  357 \t Loss:  0.18085727\n",
      "Epoch:  358 \t Loss:  0.17857045\n",
      "Epoch:  359 \t Loss:  0.17632951\n",
      "Epoch:  360 \t Loss:  0.17413348\n",
      "Epoch:  361 \t Loss:  0.17198142\n",
      "Epoch:  362 \t Loss:  0.16987252\n",
      "Epoch:  363 \t Loss:  0.16780615\n",
      "Epoch:  364 \t Loss:  0.16578105\n",
      "Epoch:  365 \t Loss:  0.16379657\n",
      "Epoch:  366 \t Loss:  0.16185185\n",
      "Epoch:  367 \t Loss:  0.15994614\n",
      "Epoch:  368 \t Loss:  0.15807869\n",
      "Epoch:  369 \t Loss:  0.15624878\n",
      "Epoch:  370 \t Loss:  0.15445554\n",
      "Epoch:  371 \t Loss:  0.15269823\n",
      "Epoch:  372 \t Loss:  0.15097606\n",
      "Epoch:  373 \t Loss:  0.14928864\n",
      "Epoch:  374 \t Loss:  0.14763497\n",
      "Epoch:  375 \t Loss:  0.14601447\n",
      "Epoch:  376 \t Loss:  0.14442652\n",
      "Epoch:  377 \t Loss:  0.14287046\n",
      "Epoch:  378 \t Loss:  0.1413455\n",
      "Epoch:  379 \t Loss:  0.13985106\n",
      "Epoch:  380 \t Loss:  0.1383867\n",
      "Epoch:  381 \t Loss:  0.13695174\n",
      "Epoch:  382 \t Loss:  0.1355455\n",
      "Epoch:  383 \t Loss:  0.13416746\n",
      "Epoch:  384 \t Loss:  0.13281718\n",
      "Epoch:  385 \t Loss:  0.13149403\n",
      "Epoch:  386 \t Loss:  0.13019729\n",
      "Epoch:  387 \t Loss:  0.12892653\n",
      "Epoch:  388 \t Loss:  0.1276813\n",
      "Epoch:  389 \t Loss:  0.12646109\n",
      "Epoch:  390 \t Loss:  0.12526527\n",
      "Epoch:  391 \t Loss:  0.12409349\n",
      "Epoch:  392 \t Loss:  0.122945175\n",
      "Epoch:  393 \t Loss:  0.121819876\n",
      "Epoch:  394 \t Loss:  0.12071724\n",
      "Epoch:  395 \t Loss:  0.1196367\n",
      "Epoch:  396 \t Loss:  0.11857784\n",
      "Epoch:  397 \t Loss:  0.117540196\n",
      "Epoch:  398 \t Loss:  0.11652334\n",
      "Epoch:  399 \t Loss:  0.1155269\n",
      "Epoch:  400 \t Loss:  0.11455054\n",
      "Epoch:  401 \t Loss:  0.11359363\n",
      "Epoch:  402 \t Loss:  0.11265587\n",
      "Epoch:  403 \t Loss:  0.11173701\n",
      "Epoch:  404 \t Loss:  0.11083659\n",
      "Epoch:  405 \t Loss:  0.10995415\n",
      "Epoch:  406 \t Loss:  0.10908948\n",
      "Epoch:  407 \t Loss:  0.108242154\n",
      "Epoch:  408 \t Loss:  0.10741183\n",
      "Epoch:  409 \t Loss:  0.10659814\n",
      "Epoch:  410 \t Loss:  0.10580078\n",
      "Epoch:  411 \t Loss:  0.10501926\n",
      "Epoch:  412 \t Loss:  0.10425354\n",
      "Epoch:  413 \t Loss:  0.10350314\n",
      "Epoch:  414 \t Loss:  0.102767706\n",
      "Epoch:  415 \t Loss:  0.10204712\n",
      "Epoch:  416 \t Loss:  0.10134091\n",
      "Epoch:  417 \t Loss:  0.100648865\n",
      "Epoch:  418 \t Loss:  0.09997068\n",
      "Epoch:  419 \t Loss:  0.09930614\n",
      "Epoch:  420 \t Loss:  0.098654814\n",
      "Epoch:  421 \t Loss:  0.09801662\n",
      "Epoch:  422 \t Loss:  0.0973912\n",
      "Epoch:  423 \t Loss:  0.09677826\n",
      "Epoch:  424 \t Loss:  0.09617768\n",
      "Epoch:  425 \t Loss:  0.09558913\n",
      "Epoch:  426 \t Loss:  0.095012344\n",
      "Epoch:  427 \t Loss:  0.09444716\n",
      "Epoch:  428 \t Loss:  0.093893275\n",
      "Epoch:  429 \t Loss:  0.0933505\n",
      "Epoch:  430 \t Loss:  0.0928186\n",
      "Epoch:  431 \t Loss:  0.092297316\n",
      "Epoch:  432 \t Loss:  0.09178646\n",
      "Epoch:  433 \t Loss:  0.09128589\n",
      "Epoch:  434 \t Loss:  0.09079529\n",
      "Epoch:  435 \t Loss:  0.09031454\n",
      "Epoch:  436 \t Loss:  0.089843385\n",
      "Epoch:  437 \t Loss:  0.08938168\n",
      "Epoch:  438 \t Loss:  0.08892915\n",
      "Epoch:  439 \t Loss:  0.08848568\n",
      "Epoch:  440 \t Loss:  0.08805113\n",
      "Epoch:  441 \t Loss:  0.0876252\n",
      "Epoch:  442 \t Loss:  0.08720777\n",
      "Epoch:  443 \t Loss:  0.08679863\n",
      "Epoch:  444 \t Loss:  0.0863978\n",
      "Epoch:  445 \t Loss:  0.08600486\n",
      "Epoch:  446 \t Loss:  0.0856198\n",
      "Epoch:  447 \t Loss:  0.085242406\n",
      "Epoch:  448 \t Loss:  0.08487257\n",
      "Epoch:  449 \t Loss:  0.08451013\n",
      "Epoch:  450 \t Loss:  0.08415497\n",
      "Epoch:  451 \t Loss:  0.08380692\n",
      "Epoch:  452 \t Loss:  0.083465785\n",
      "Epoch:  453 \t Loss:  0.08313154\n",
      "Epoch:  454 \t Loss:  0.082803935\n",
      "Epoch:  455 \t Loss:  0.08248291\n",
      "Epoch:  456 \t Loss:  0.08216834\n",
      "Epoch:  457 \t Loss:  0.08186007\n",
      "Epoch:  458 \t Loss:  0.08155795\n",
      "Epoch:  459 \t Loss:  0.08126191\n",
      "Epoch:  460 \t Loss:  0.0809718\n",
      "Epoch:  461 \t Loss:  0.08068751\n",
      "Epoch:  462 \t Loss:  0.080408975\n",
      "Epoch:  463 \t Loss:  0.080135964\n",
      "Epoch:  464 \t Loss:  0.079868495\n",
      "Epoch:  465 \t Loss:  0.07960635\n",
      "Epoch:  466 \t Loss:  0.07934949\n",
      "Epoch:  467 \t Loss:  0.07909775\n",
      "Epoch:  468 \t Loss:  0.07885109\n",
      "Epoch:  469 \t Loss:  0.078609444\n",
      "Epoch:  470 \t Loss:  0.07837261\n",
      "Epoch:  471 \t Loss:  0.07814059\n",
      "Epoch:  472 \t Loss:  0.07791319\n",
      "Epoch:  473 \t Loss:  0.077690385\n",
      "Epoch:  474 \t Loss:  0.07747206\n",
      "Epoch:  475 \t Loss:  0.07725812\n",
      "Epoch:  476 \t Loss:  0.0770485\n",
      "Epoch:  477 \t Loss:  0.07684309\n",
      "Epoch:  478 \t Loss:  0.07664177\n",
      "Epoch:  479 \t Loss:  0.07644451\n",
      "Epoch:  480 \t Loss:  0.07625123\n",
      "Epoch:  481 \t Loss:  0.0760618\n",
      "Epoch:  482 \t Loss:  0.07587615\n",
      "Epoch:  483 \t Loss:  0.07569422\n",
      "Epoch:  484 \t Loss:  0.07551594\n",
      "Epoch:  485 \t Loss:  0.07534127\n",
      "Epoch:  486 \t Loss:  0.075170085\n",
      "Epoch:  487 \t Loss:  0.075002365\n",
      "Epoch:  488 \t Loss:  0.07483797\n",
      "Epoch:  489 \t Loss:  0.07467691\n",
      "Epoch:  490 \t Loss:  0.07451907\n",
      "Epoch:  491 \t Loss:  0.07436434\n",
      "Epoch:  492 \t Loss:  0.074212745\n",
      "Epoch:  493 \t Loss:  0.07406418\n",
      "Epoch:  494 \t Loss:  0.07391863\n",
      "Epoch:  495 \t Loss:  0.07377601\n",
      "Epoch:  496 \t Loss:  0.07363623\n",
      "Epoch:  497 \t Loss:  0.073499255\n",
      "Epoch:  498 \t Loss:  0.07336505\n",
      "Epoch:  499 \t Loss:  0.07323355\n",
      "[[0.0002516  0.00028572 0.00028553 ... 0.00030018 0.00032815 0.00031657]\n",
      " [0.00030362 0.00021509 0.00030458 ... 0.00023628 0.0003446  0.00027462]\n",
      " [0.00027168 0.00026629 0.00027373 ... 0.00028568 0.00032478 0.00028089]\n",
      " ...\n",
      " [0.00036302 0.00028313 0.00029223 ... 0.00025944 0.00034338 0.00026521]\n",
      " [0.00033859 0.00031657 0.00026462 ... 0.00029511 0.00029496 0.00028119]\n",
      " [0.00023228 0.00018696 0.00019267 ... 0.00026264 0.00024304 0.00019177]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        _, train_loss = sess.run([train_op, loss], feed_dict={X: mean_centered_ratings_matrix})\n",
    "        print(\"Epoch: \", epoch, \"\\t Loss: \", train_loss)\n",
    "        \n",
    "    u = sess.run(U)\n",
    "    v = sess.run(V)\n",
    "    \n",
    "    u_biases, i_biases = sess.run([user_biases, item_biases])\n",
    "    print(np.matmul(u, np.transpose(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having separate variables $o_i$ and $p_j$ for users and items, we can increase the size of the factor matrices to incorporate these bias variables. We need to add two additional columns to each factor matrix $U$ and $V$, to create a larger factor matrices of size $m \\times (k+2)$ and $n \\times (k + 2)$. The last two columns of each factor matrix are special, because they correspond to the bias components. We have:\n",
    "$$u_{i, k+1} = o_i$$\n",
    "$$u_{i, k+2} = 1$$\n",
    "$$v_{j, k+1} = 1$$\n",
    "$$u_{j, k+2} = p_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the loss funtion is given as follows:\n",
    "$$ Minimize\\ J = \\dfrac{1}{2} \\sum_{(i, j) \\in S} (r_{ij} - \\sum_{s=1}^{k+2} u_{is}v_{js})^2 + \\dfrac{\\lambda}{2} \\sum_{s=1}^{k+2} (\\sum_{i=1}^m u_{is}^2 + \\sum_{j=1}^n v_{js}^2 )  $$\n",
    "$$ subject\\ to: $$\n",
    "$$ (k+2)th\\ column\\ of\\ U\\ contains\\ only\\ 1s $$\n",
    "$$ (k+1)th\\ column\\ of\\ V\\ contains\\ only\\ 1s $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
