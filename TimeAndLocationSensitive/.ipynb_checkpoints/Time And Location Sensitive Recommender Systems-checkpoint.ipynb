{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many real scenarios, the buying and rating behaviors of customers are associated with temporal information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time can be viewed from a recency and forecasting perspective, or from a contextual (e.g., seasonal) perspective. From a recency perspective, the basic idea is that recent ratings are more important than older ratings. In the contextual perspective, various periodic aspects, such as season or month, may be used.\n",
    "\n",
    "When time is viewed as a continuous variable, the recommendations are often created as functions of time. The temporal context can be viewed from a periodic, recency, or modeling point of view.\n",
    "\n",
    "Time can be treated as a modeling variable by explicitly expressing the predicted ratings as a function of time. The parameters of this function can be learned in a data-driven manner by minimizing the squared error of the predicted ratings with respect to the observed ratings. An example of such a model is time-SVD++, which expresses the predicted ratings as a function of temporally parameterized biases and factor matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal information can be used in one of two ways in order to improve the effectiveness of prediction:\n",
    "1. Recency-based models: Some models consider recent ratings more important than older ratings. In these cases, window-based and decay-based models are used for more accurate prediction.\n",
    "2. Periodic context-based models: In periodic context-based models, the specific property of a period, such as the time at the level of specificity of the hour, day, week, month, or sea- son, is used to perform the recommendation. \n",
    "3. Models that explicitly use time as an independent variable: A recent approach, referred to as time-SVD++, uses time as an independent variable within the modeling process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recency-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decay-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In decay-based methods, a time-stamp $t_{uj}$ is associated with each observed rating of user $u$ and item $j$ in the $m \\times n$ ratings matrix $R$. It is assumed that all recommendations should be made at a future time $t_f$ . This future time is also referred to as the target time. Then, the weight $w_{uj}(t_f)$ of the rating $r_{uj}$ at target time $t_f$ is defined with the use of a decay function, that penalizes larger distances between $t_{uj}$ and $t_f$ . A decay function is the exponential function: \n",
    "$$ w_{uj}(t_f) = exp[-\\lambda (t_f - t_{uj})]$$\n",
    "\n",
    "The decay-rate $\\lambda$ is a user-defined parameter that regulates the importance of time. Larger values of $\\lambda$ de-emphasize older ratings to a greater degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then:\n",
    "$$ \\hat{r}_{uj} (t_f) = \\mu_u + \\dfrac{\\sum_{v \\in P_u(j)} w_{vj} (t_f) .Sim(u, v).(r_{vj} - \\mu_v)}{\\sum_{v \\in P_u(j)} w_{vj} (t_f) .|Sim(u, v)|} $$\n",
    "\n",
    "Here, $P_u(j)$ represents the $k$ closest users to user $u$ that have specified ratings for item $j$. The optimal value of λ can be learned using cross-validation methods,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example in MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_path = os.path.abspath(os.path.join('', os.pardir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv(os.path.join(dir_path, 'data/ml-100k/u.data'), names=names, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan = np.nan\n",
    "\n",
    "time_matrix = np.zeros((n_users, n_items)) * nan\n",
    "ratings_matrix = np.zeros((n_users, n_items)) * nan\n",
    "\n",
    "for line in df.itertuples():\n",
    "    ratings_matrix[line[1]-1, line[2]-1] = line[3]\n",
    "    time_matrix[line[1]-1, line[2]-1] = line[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get test data\n",
    "We need to get the test data with the newest time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_time_matrix = np.reshape(time_matrix, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_time_indices = np.argsort(flatten_time_matrix) # decreasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_test_indices = sorted_time_indices[:5000]\n",
    "flatten_train_indices = sorted_time_indices[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(flatten_indices):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for indices in flatten_indices:\n",
    "        x = indices // n_items\n",
    "        y = indices % n_items\n",
    "        \n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        \n",
    "    return [tuple(X), tuple(Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = get_indices(flatten_test_indices)\n",
    "train_indices = get_indices(flatten_train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratings_matrix(original_matrix, indices):\n",
    "    shape = np.shape(original_matrix)\n",
    "    matrix = np.ones(shape) * nan\n",
    "    \n",
    "    for i in range(len(indices[0])):\n",
    "        x = indices[0][i]\n",
    "        y = indices[1][i]\n",
    "        \n",
    "        matrix[x][y] = original_matrix[x][y]\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings_matrix = get_ratings_matrix(ratings_matrix, train_indices)\n",
    "test_ratings_matrix = get_ratings_matrix(ratings_matrix, test_indices)\n",
    "\n",
    "train_time_matrix = get_ratings_matrix(time_matrix, train_indices)\n",
    "test_time_matrix = get_ratings_matrix(time_matrix, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices for vector\n",
    "def specified_rating_indices(u):\n",
    "    return np.where(np.isfinite(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean rating for each user i using his specified rating\n",
    "def mean(u):\n",
    "    # may use specified_rating_indices but use more time\n",
    "    specified_ratings = u[specified_rating_indices(u)]#u[np.isfinite(u)]\n",
    "    if np.shape(specified_ratings)[0] == 0: return nan\n",
    "    m = sum(specified_ratings)/np.shape(specified_ratings)[0]\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_user_mean_ratings(ratings_matrix):\n",
    "    return np.array([mean(ratings_matrix[u, :]) for u in range(ratings_matrix.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_centered_ratings_matrix(ratings_matrix):\n",
    "    users_mean_rating = all_user_mean_ratings(ratings_matrix)\n",
    "    mean_centered_ratings_matrix = ratings_matrix - np.reshape(users_mean_rating, [-1, 1])\n",
    "    return mean_centered_ratings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_centered_ratings_matrix = get_mean_centered_ratings_matrix(train_ratings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson(u, v):\n",
    "    mean_u = mean(u)\n",
    "    mean_v = mean(v)\n",
    "    \n",
    "    specified_rating_indices_u = set(specified_rating_indices(u)[0])\n",
    "    specified_rating_indices_v = set(specified_rating_indices(v)[0])\n",
    "    \n",
    "    mutually_specified_ratings_indices = specified_rating_indices_u.intersection(specified_rating_indices_v)\n",
    "    mutually_specified_ratings_indices = list(mutually_specified_ratings_indices)\n",
    "    \n",
    "    u_mutually = u[mutually_specified_ratings_indices]\n",
    "    v_mutually = v[mutually_specified_ratings_indices]\n",
    "      \n",
    "    centralized_mutually_u = u_mutually - mean_u\n",
    "    centralized_mutually_v = v_mutually - mean_v\n",
    "#     print(np.sqrt(np.sum(np.square(centralized_mutually_u))))\n",
    "\n",
    "    result = np.sum(np.multiply(centralized_mutually_u, centralized_mutually_v)) \n",
    "    result = result / (np.sqrt(np.sum(np.square(centralized_mutually_u))) * np.sqrt(np.sum(np.square(centralized_mutually_v))))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from surprise import similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_centered(u):\n",
    "    return u - mean(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_similarity_value_for(u_index, ratings_matrix, func):\n",
    "    user_ratings = ratings_matrix[u_index, :]\n",
    "    similarity_value = np.array([func(ratings_matrix[i, :], user_ratings) for i in range(ratings_matrix.shape[0])])\n",
    "    return similarity_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def get_user_similarity_matrix(ratings_matrix, func):\n",
    "    similarity_matrix = []\n",
    "    for u_index in tqdm(range(ratings_matrix.shape[0])):\n",
    "        similarity_value = get_user_similarity_value_for(u_index, ratings_matrix, func)\n",
    "        similarity_matrix.append(similarity_value)\n",
    "    return np.array(similarity_matrix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/943 [00:00<?, ?it/s]/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n",
      "100%|██████████| 943/943 [01:49<00:00,  8.59it/s]\n"
     ]
    }
   ],
   "source": [
    "user_similarity_matrix = get_user_similarity_matrix(train_ratings_matrix, pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_mean_rating = all_user_mean_ratings(train_ratings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_month(d1, d2):\n",
    "    d1 = datetime.fromtimestamp(d1)\n",
    "    d2 = datetime.fromtimestamp(d2)\n",
    "    return (d1.year - d2.year) * 12 + d1.month - d2.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weight by diff of months\n",
    "\n",
    "def weight_for_rating(lambda_param, rating_time, current_time):\n",
    "    months = diff_month(rating_time, current_time)\n",
    "    return np.exp(-lambda_param*months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(u_index, i_index, k, current_time=None):    \n",
    "    similarity_value = user_similarity_matrix[u_index]\n",
    "    sorted_users_similar = np.argsort(similarity_value)\n",
    "    sorted_users_similar = np.flip(sorted_users_similar, axis=0)\n",
    "        \n",
    "    # only for this item\n",
    "    users_rated_item = specified_rating_indices(train_ratings_matrix[:, i_index])[0]\n",
    "\n",
    "    set_2 = frozenset(users_rated_item)\n",
    "    ranked_similar_user_rated_item = [u for u in sorted_users_similar if u in set_2] \n",
    "    \n",
    "    if k < len(ranked_similar_user_rated_item):\n",
    "        top_k_similar_user = ranked_similar_user_rated_item[0:k]   \n",
    "    else:\n",
    "        top_k_similar_user = np.array(ranked_similar_user_rated_item)\n",
    "            \n",
    "    # replace with mean_centered for user\n",
    "    \n",
    "    ratings_in_item = mean_centered_ratings_matrix[:, i_index]\n",
    "    try: \n",
    "        top_k_ratings = ratings_in_item[top_k_similar_user]\n",
    "        top_k_similarity_value = similarity_value[top_k_similar_user]\n",
    "    except:\n",
    "        return nan\n",
    "    \n",
    "    \n",
    "    \n",
    "    current_time = 875743787.0 #time_matrix[u_index, i_index]\n",
    "    weights = []\n",
    "    for i in range(len(top_k_similar_user)):\n",
    "        rating_time = time_matrix[top_k_similar_user[i], i_index]\n",
    "        weights.append(weight_for_rating(0.1, rating_time, current_time))\n",
    "    \n",
    "    weights = np.array(weights)\n",
    "    top_k_similarity_value = np.multiply(top_k_similarity_value, weights)\n",
    "\n",
    "    r_hat = users_mean_rating[u_index] + np.sum(top_k_ratings * top_k_similarity_value)/np.sum(np.abs(top_k_similarity_value))\n",
    "    return r_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4640883977900554"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(10, 1, 10)\n",
    "users_mean_rating[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for i in range(len(test_indices[0])):\n",
    "    pairs.append((test_indices[0][i], test_indices[1][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(pairs):\n",
    "    for u, i in pairs:\n",
    "        print(predict(u, i, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_ratings_matrix(current_time):\n",
    "    predicted_ratings = []\n",
    "    for u_index in tqdm(range(n_users)):\n",
    "        user_ratings = []\n",
    "        for i_index in range(n_items):\n",
    "#             rating = ratings_matrix[u_index][i_index]\n",
    "#             if np.isnan(rating):\n",
    "            rating = predict(u_index, i_index, 100, current_time)\n",
    "            user_ratings.append(rating)\n",
    "        predicted_ratings.append(user_ratings)\n",
    "    return predicted_ratings            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/943 [00:01<05:08,  3.04it/s]/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in double_scalars\n",
      "100%|██████████| 943/943 [05:18<00:00,  2.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# predicted_ratings = get_predicted_ratings_matrix()\n",
    "# predicted_ratings = np.array(predicted_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In window-based methods, ratings that are older than a particular time are pruned from consideration. This approach can be viewed as a special case of pre-filtering or post-filtering methods in context-based models. There are several ways in which windows can be modeled:\n",
    "1. If the difference between the target time $t_f$ and the rating time $t_{ij}$ is larger than a particular threshold, then the rating is dropped.\n",
    "2. In some cases, it is possible to obtain some insight into the active periods for various items depending on the underlying domain. In such cases, the windows can be set in a domain- and item-specific way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Periodic Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Periodic context is designed to handle cases in which the time dimension may refer to a specific period in time, such as hour of the day, day of the week, season, or the time intervals in the vicinity of specific periodic events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Filtering and Post-Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pre-filtering, a significant part of the ratings data are removed that are not relevant to the specific target time (i.e., context) within which the recommendation is being performed or executed. Within each context, a separate model is constructed for prediction. After filtering, any non-contextual method may be used to make predictions on the pruned data within each segment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In post-filtering, the recommendations are adjusted based on the context, after a non- contextual method has been used to generate the recommendation on all the data. Therefore, the basic approach of post-filtering uses the following two steps:\n",
    "1. Generate the recommendations using a conventional collaborative filtering approach on all data, while ignoring the temporal context.\n",
    "2. Adjust the generated recommendation list with the use of temporal context as a post- processing step. Either the ranks of the recommended list may be adjusted, or the list may be pruned of contextually irrelevant items.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Incorporation of Temporal Context\n",
    "It is also possible to directly modify existing models such as neighborhood methods in order to incorporate temporal context. In such cases, one works directly with the 3-dimensional representation corresponding to user, item, and context. One can also modify regression and latent-factor models to incorporate the temporal context directly. These methods apply generally to any context- based scenario (e.g., location), and not just the temporal context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Ratings as a Function of Time\n",
    "In these methods, the ratings are modeled as a function of time and the parameters of the model are learned in a data-dependent way. These methods can intelligently separate long-term trends from transient and noisy trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Time-SVD++ Model\n",
    "(This model is built based on both explicit and implicit feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factor model, which incorporates bias, expresses the ratings matrix $R = [r_{ij}]_{m \\times n}$ in terms of the user biases, the item biases, and the factor matrices. The predicted rating $\\hat{r_{ij}}$ is expressed in terms of these variables as follows:\n",
    "$$ \\hat{r}_{ij} = o_i + p_j + \\sum_{s=1}^k u_{is}.v_{js} $$\n",
    "\n",
    "Intuitively, the variable $o_i$ indicates the propensity of user $i$ to rate all items highly, whereas the variable $p_j$ denotes the propensity of item $j$ to be rated highly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basic bias-based model is further enhanced with the notion of implicit feedback variables $Y = [y_{ij}]_{n×k}$ for each user-item pair. These variables encode the propensity of each factor-item combination to contribute to implicit feedback.\n",
    "\n",
    "Let $I_i$ be the set of items rated by user $i$. Then, the predicted value of the rating, which includes implicit feedback, can be expressed as follows:\n",
    "$$ \\hat{r}_{ij} = o_i + p_j + \\sum_{s=1}^k (u_{is} + \\sum_{h \\in I_i} \\dfrac{y_{hs}}{\\sqrt{|I_i|}} ).v_{js} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, the time-SVD++ model assumes that the user biases $o_i$ , item biases $p_j$ , and the user factors $u_{is}$ are functions of time. Therefore, these terms will be expressed as $o_i(t)$, $p_j(t)$, and $u_{is}(t)$ to denote the fact that they are functions of time. By using these temporal variables, one now obtains the time-varying predicted value $\\hat{r}_{ij}(t)$ of the $(i, j)$th entry of the ratings matrix at time $t$ as follows:\n",
    "$$ \\hat{r}_{ij} = o_i(t) + p_j(t) + \\sum_{s=1}^k (u_{is}(t) + \\sum_{h \\in I_i} \\dfrac{y_{hs}}{\\sqrt{|I_i|}} ).v_{js} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to compute $o_i(t)$, $p_j(t)$ and $u_{is}(t)$\n",
    "\n",
    "1. The intuition for choosing the temporal form of the item bias $p_j(t)$ is that the popular- ity of an item can vary significantly with time, but it shows a high level of continuity and stability over shorter periods. Therefore, the time horizon can be split into bins of equal size, and the ratings belonging to a particular bin have the same bias. Smaller bin sizes lead to better granularity but it may also result in overfitting because enough ratings may not be present in each bin. The item bias $p_j(t)$ can now be split into a constant part and an offset parameter, which is bin-specific depending on the time $t$ at which item $j$ is rated:\n",
    "$$ p_j(t) = C_j + Offset_{j, Bin(t)} $$\n",
    "\n",
    "Note that both the constant part $C_j$ and offsets are parameters that need to be learned in a data-driven manner.\n",
    "2. A different approach is used to parameterize the user bias $o_i(t)$. Therefore, a functional form may be used to parameterize the user bias, which captures the concept drift of the user over time. Let the mean date of all ratings of user $i$ be denoted by $\\nu_i$. Then, the concept drift $dev_i(t)$ of user $i$ at time $t$ can be computed as a function of $t$ as follows:\n",
    "$$ dev_i(t) = sign(t − \\nu_i) . |t − \\nu_i|^\\beta $$\n",
    "The parameter $\\beta$ is selected using cross-validation. A typical value of $\\beta$ is around 0.4. In addition, the transient noise at each time t is captured with the parameters $\\epsilon_{it}$. Then, the user bias $o_i(t)$ is split into a constant part, a time-dependent part, and transient noise, as follows:\n",
    "$$ o_i(t) = K_i + \\alpha_i · dev_i(t) + \\epsilon_{it} $$\n",
    "\n",
    "3. The user factors $u_{is}(t)$ correspond to the affinity of users towards various concepts. As in the case of user biases, the amount of elapsed time is a crucial factor in deciding the amount of drift. Therefore, a similar approach to user biases is used for modeling the temporal change in the user factors:\n",
    "$$ u_{is}(t) = K' + \\alpha' . dev(t) + \\epsilon'_{ist} $$\n",
    "As in the case of user biases, the constant effects, long-term effects, and transient effects are modeled by the three terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Problem\n",
    "If $S$ contains the set of user-item pairs for which ratings are specified in the matrix $R = [r_{ij}]_{m \\times n}$, then one must solve the following optimization problem:\n",
    "$$ Minimize J = \\dfrac{1}{2} \\sum_{(i, j) \\in S}[r_{ij} − \\hat{r}_{ij} (t_{ij})] + \\lambda . (Regularization Term) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Temporal Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete temporal models are relevant to the case where the underlying data is received as discrete sequences. Such data can be encountered in a variety of application scenarios, most of which are associated with implicit user feedback rather than explicit ratings. Some examples of such application scenarios are as follows:\n",
    "1. Web logs and clickstreams\n",
    "2. Supermarket transactions\n",
    "3. Query recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markovian Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location-Aware Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location can influence the recommendation process in a wide variety of ways, of which the following two ways are particularly common:\n",
    "\n",
    "1. The global geographical location of a user can have a significant influence on her pref- erences in terms of taste, culture, clothing, eating habits, and so on. This property is referred to as preference locality. In this case, the locality is inherently associated with the user, but not with the item. Therefore, in this case, the users are spatial, whereas the items are not.\n",
    "2. Mobile users often want to discover restaurants or leisure places in the vicinity of their current location. In this case, the recommended items are inherently spatial. This prop- erty is referred to as travel locality.\n",
    "3. It is possible to imagine scenarios in which both users and items are spatial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
